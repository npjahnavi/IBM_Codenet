{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efaf1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, SimpleRNN, Embedding\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tokenize, glob\n",
    "import os, re, requests, tarfile, shutil, json, glob # For file manipualation only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4afc79ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version X.X\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebb88b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=1 (NAME), string='while', start=(1, 0), end=(1, 5), line='while True:\\n')\n",
      "TokenInfo(type=1 (NAME), string='True', start=(1, 6), end=(1, 10), line='while True:\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(1, 10), end=(1, 11), line='while True:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 11), end=(1, 12), line='while True:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='    ', start=(2, 0), end=(2, 4), line='    try:\\n')\n",
      "TokenInfo(type=1 (NAME), string='try', start=(2, 4), end=(2, 7), line='    try:\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(2, 7), end=(2, 8), line='    try:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(2, 8), end=(2, 9), line='    try:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(3, 0), end=(3, 8), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='a', start=(3, 8), end=(3, 9), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=',', start=(3, 9), end=(3, 10), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='b', start=(3, 11), end=(3, 12), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='=', start=(3, 13), end=(3, 14), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='map', start=(3, 15), end=(3, 18), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='(', start=(3, 18), end=(3, 19), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='int', start=(3, 19), end=(3, 22), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=',', start=(3, 22), end=(3, 23), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='input', start=(3, 24), end=(3, 29), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='(', start=(3, 29), end=(3, 30), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=')', start=(3, 30), end=(3, 31), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='.', start=(3, 31), end=(3, 32), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='split', start=(3, 32), end=(3, 37), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='(', start=(3, 37), end=(3, 38), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=')', start=(3, 38), end=(3, 39), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=')', start=(3, 39), end=(3, 40), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(3, 40), end=(3, 41), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(4, 4), end=(4, 4), line='    except EOFError:\\n')\n",
      "TokenInfo(type=1 (NAME), string='except', start=(4, 4), end=(4, 10), line='    except EOFError:\\n')\n",
      "TokenInfo(type=1 (NAME), string='EOFError', start=(4, 11), end=(4, 19), line='    except EOFError:\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(4, 19), end=(4, 20), line='    except EOFError:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(4, 20), end=(4, 21), line='    except EOFError:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(5, 0), end=(5, 8), line='        break\\n')\n",
      "TokenInfo(type=1 (NAME), string='break', start=(5, 8), end=(5, 13), line='        break\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(5, 13), end=(5, 14), line='        break\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(6, 4), end=(6, 4), line='    count=1\\n')\n",
      "TokenInfo(type=1 (NAME), string='count', start=(6, 4), end=(6, 9), line='    count=1\\n')\n",
      "TokenInfo(type=54 (OP), string='=', start=(6, 9), end=(6, 10), line='    count=1\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1', start=(6, 10), end=(6, 11), line='    count=1\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(6, 11), end=(6, 12), line='    count=1\\n')\n",
      "TokenInfo(type=1 (NAME), string='k', start=(7, 4), end=(7, 5), line='    k=a+b\\n')\n",
      "TokenInfo(type=54 (OP), string='=', start=(7, 5), end=(7, 6), line='    k=a+b\\n')\n",
      "TokenInfo(type=1 (NAME), string='a', start=(7, 6), end=(7, 7), line='    k=a+b\\n')\n",
      "TokenInfo(type=54 (OP), string='+', start=(7, 7), end=(7, 8), line='    k=a+b\\n')\n",
      "TokenInfo(type=1 (NAME), string='b', start=(7, 8), end=(7, 9), line='    k=a+b\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(7, 9), end=(7, 10), line='    k=a+b\\n')\n",
      "TokenInfo(type=1 (NAME), string='while', start=(8, 4), end=(8, 9), line='    while k>=10:\\n')\n",
      "TokenInfo(type=1 (NAME), string='k', start=(8, 10), end=(8, 11), line='    while k>=10:\\n')\n",
      "TokenInfo(type=54 (OP), string='>=', start=(8, 11), end=(8, 13), line='    while k>=10:\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='10', start=(8, 13), end=(8, 15), line='    while k>=10:\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(8, 15), end=(8, 16), line='    while k>=10:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(8, 16), end=(8, 17), line='    while k>=10:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(9, 0), end=(9, 8), line='        k//=10\\n')\n",
      "TokenInfo(type=1 (NAME), string='k', start=(9, 8), end=(9, 9), line='        k//=10\\n')\n",
      "TokenInfo(type=54 (OP), string='//=', start=(9, 9), end=(9, 12), line='        k//=10\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='10', start=(9, 12), end=(9, 14), line='        k//=10\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(9, 14), end=(9, 15), line='        k//=10\\n')\n",
      "TokenInfo(type=1 (NAME), string='count', start=(10, 8), end=(10, 13), line='        count+=1\\n')\n",
      "TokenInfo(type=54 (OP), string='+=', start=(10, 13), end=(10, 15), line='        count+=1\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1', start=(10, 15), end=(10, 16), line='        count+=1\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(10, 16), end=(10, 17), line='        count+=1\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(11, 4), end=(11, 4), line='    print(count)')\n",
      "TokenInfo(type=1 (NAME), string='print', start=(11, 4), end=(11, 9), line='    print(count)')\n",
      "TokenInfo(type=54 (OP), string='(', start=(11, 9), end=(11, 10), line='    print(count)')\n",
      "TokenInfo(type=1 (NAME), string='count', start=(11, 10), end=(11, 15), line='    print(count)')\n",
      "TokenInfo(type=54 (OP), string=')', start=(11, 15), end=(11, 16), line='    print(count)')\n",
      "TokenInfo(type=4 (NEWLINE), string='', start=(11, 16), end=(11, 17), line='')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(12, 0), end=(12, 0), line='')\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(12, 0), end=(12, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "# Experiment with tokenize library to see its use\n",
    "with tokenize.open('Data/p00002/Python/s005515634.py') as f:\n",
    "    tokens = tokenize.generate_tokens(f.readline)\n",
    "    for token in tokens:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39e44e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob('data/*/Python/*.py')\n",
    "print(len(all_files))\n",
    "np.savetxt('all_py_files.txt', X = all_files, delimiter=',', comments='', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e57e87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_mini = glob.glob('data/*/Python/*.py')\n",
    "np.savetxt('all_py_files.txt', X = all_files_mini, delimiter=',', comments='', fmt='%s')\n",
    "len(all_files_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d752a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokenizer vocabulary 2091\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_save(dir_name):\n",
    "  tokenizerD = Tokenizer(\n",
    "    num_words=10000,\n",
    "    filters='',\n",
    "    lower=False, split=' ', char_level=False, oov_token=\"<UNK>\",\n",
    "    document_count=0\n",
    "  )\n",
    "  \n",
    "  # Assumes a file with all_py_files.txt name\n",
    "  file_index = 1\n",
    "  # with open(dir_name + 'all_py_files.txt') as dir_f:\n",
    "  #   for py_file in dir_f.readlines():\n",
    "  for py_file in all_files:\n",
    "    file_index = file_index + 1\n",
    "    with tokenize.open(py_file.strip()) as f:\n",
    "      try:\n",
    "        tokens = [tok.string for tok in tokenize.generate_tokens(f.readline)]\n",
    "        tokenizerD.fit_on_texts(tokens)\n",
    "      except:\n",
    "        pass \n",
    "\n",
    "      if file_index % 2000 == 0:\n",
    "        print(str(file_index) + \" python files tokenized with #tokens: \" + str(len(tokenizerD.word_index)))\n",
    "        tokenizer_json = tokenizerD.to_json()\n",
    "        with open(dir_name + 'tokenizer.json', 'w') as f:\n",
    "          json.dump(tokenizer_json, f)\n",
    "\n",
    "  # Save final version\n",
    "  tokenizer_json = tokenizerD.to_json()\n",
    "  with open(dir_name + 'tokenizer.json', 'w') as f:\n",
    "    json.dump(tokenizer_json, f)\n",
    "\n",
    "  print('Total tokenizer vocabulary {}'.format(len(tokenizerD.word_index)))\n",
    "\n",
    "\n",
    "tokenize_and_save('IBM project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34cb8b86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load tokens into 'tokenizer' file. Total tokens created is 213461\n",
    "\n",
    "path_full = \"tokenizer.json\"\n",
    "# path_full = \"drive/MyDrive/code_completion_data/samples/tokenizer.json\"\n",
    "\n",
    "with open(path_full) as f:\n",
    "  json_string = json.load(f)\n",
    "\n",
    "tokenizer = keras.preprocessing.text.tokenizer_from_json(\n",
    "    json_string\n",
    ")\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e80037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tokenizer.json') as f:\n",
    "  json_string = json.load(f)\n",
    "\n",
    "tokenizer_mini = keras.preprocessing.text.tokenizer_from_json(\n",
    "    json_string\n",
    ")\n",
    "\n",
    "len(tokenizer_mini.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c782e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files is a list of python files to generate vector on\n",
    "def vectorize_and_save(files):\n",
    "  # Load tokenizer\n",
    "  with open(\"tokenizer.json\") as f:\n",
    "    json_string = json.load(f)\n",
    "\n",
    "  tokenizer = keras.preprocessing.text.tokenizer_from_json(\n",
    "      json_string\n",
    "  )\n",
    "  \n",
    "  seq_length = 10\n",
    "  header_str = \"X1,X2,X3,X4,X5,X6,X7,X8,X9,X10,Y\"\n",
    "  file_index = 1\n",
    "  py_file_index = 1\n",
    "  dataX = []\n",
    "  n_patterns = 0\n",
    "  max_rows_in_each_file = 10000000\n",
    "  for py_file in files:\n",
    "    with tokenize.open(py_file.strip()) as f:\n",
    "      try:\n",
    "        py_file_index = py_file_index + 1\n",
    "        py_tokens = [tok.string for tok in tokenize.generate_tokens(f.readline)]\n",
    "        # Now convert the python tokens into an array of tokens from our Corpus tokenizer\n",
    "\n",
    "        words_tokens = [0]*seq_length + [tokenizer.word_index[w] if w in tokenizer.word_index else tokenizer.word_index['<UNK>'] for w in py_tokens] # Get token id if exists else use <UNK>\n",
    "\n",
    "        for j in range(len(words_tokens) - seq_length):\n",
    "          dataX.append(words_tokens[j:j+seq_length+1])\n",
    "\n",
    "        # Save to file if data reaches max count or if its the last row.\n",
    "        if len(dataX) >= max_rows_in_each_file:\n",
    "          file_name = 'dataxy_10_1_{}.csv'.format(file_index)\n",
    "          np.savetxt(fname = file_name, \n",
    "                    X = dataX, \n",
    "                    delimiter=',', \n",
    "                    comments='',\n",
    "                    fmt='%d',\n",
    "                    header=header_str)\n",
    "          n_patterns = n_patterns + len(dataX)\n",
    "          dataX = []\n",
    "          file_index = file_index + 1\n",
    "          print(\"Saved to file \" + file_name)\n",
    "\n",
    "        if py_file_index % 2000 == 0:\n",
    "          print(\"Processed files {} with dataset size {}\".format(py_file_index, n_patterns + len(dataX)))\n",
    "      except:\n",
    "        pass \n",
    "\n",
    "  # Save once more for left items unsaved\n",
    "  file_name = 'dataxy_10_1_{}.csv'.format(file_index)\n",
    "  np.savetxt(fname = file_name, \n",
    "            X = dataX, \n",
    "            delimiter=',', \n",
    "            comments='',\n",
    "            fmt='%d',\n",
    "            header=header_str)\n",
    "  n_patterns = n_patterns + len(dataX)\n",
    "  \n",
    "  print(\"Saved to file \" + file_name)\n",
    "\n",
    "  print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fae5b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to file dataxy_10_1_1.csv\n",
      "Total Patterns:  130369\n"
     ]
    }
   ],
   "source": [
    "py_files = glob.iglob('data/*/Python/*.py')\n",
    "vectorize_and_save(py_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ce3174f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to file dataxy_10_1_1.csv\n",
      "Total Patterns:  130369\n"
     ]
    }
   ],
   "source": [
    "# Now generate on entire dataset full\n",
    "py_files = glob.iglob('data/*/Python/*.py')\n",
    "vectorize_and_save(py_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eacf35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81e317c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130364</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130365</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130366</th>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130367</th>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130368</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130369 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1   X2   X3   X4  X5  X6   X7   X8   X9  X10    Y\n",
       "0         0    0    0    0   0   0    0    0    0    0   27\n",
       "1         0    0    0    0   0   0    0    0    0   27    5\n",
       "2         0    0    0    0   0   0    0    0   27    5   45\n",
       "3         0    0    0    0   0   0    0   27    5   45    3\n",
       "4         0    0    0    0   0   0   27    5   45    3    1\n",
       "...     ...  ...  ...  ...  ..  ..  ...  ...  ...  ...  ...\n",
       "130364    5   52    3  105   4   2   18    3   50    3  105\n",
       "130365   52    3  105    4   2  18    3   50    3  105    4\n",
       "130366    3  105    4    2  18   3   50    3  105    4    4\n",
       "130367  105    4    2   18   3  50    3  105    4    4    1\n",
       "130368    4    2   18    3  50   3  105    4    4    1    1\n",
       "\n",
       "[130369 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0  27]\n",
      " [  0   0   0 ...   0  27   5]\n",
      " ...\n",
      " [  3 105   4 ...   3 105   4]\n",
      " [105   4   2 ... 105   4   4]\n",
      " [  4   2  18 ...   4   4   1]]\n",
      "[27  5 45 ...  4  1  1]\n"
     ]
    }
   ],
   "source": [
    "dataxy = pd.read_csv('dataxy_10_1_1.csv', header = 0)\n",
    "display(dataxy)\n",
    "dataX = dataxy.iloc[:, 0:10].values\n",
    "dataY = dataxy.iloc[:, 10].values\n",
    "\n",
    "print(dataX)\n",
    "print(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37d5f61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1,\n",
       " '\\n': 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '=': 5,\n",
       " ':': 6,\n",
       " ',': 7,\n",
       " '.': 8,\n",
       " '1': 9,\n",
       " '_': 10,\n",
       " 'a': 11,\n",
       " '\"': 12,\n",
       " '[': 13,\n",
       " ']': 14,\n",
       " 'x': 15,\n",
       " '0': 16,\n",
       " 'i': 17,\n",
       " 'print': 18,\n",
       " 'input': 19,\n",
       " '*': 20,\n",
       " 'y': 21,\n",
       " 'n': 22,\n",
       " 'int': 23,\n",
       " '2': 24,\n",
       " \"'\": 25,\n",
       " 'if': 26,\n",
       " 's': 27,\n",
       " 'in': 28,\n",
       " 'for': 29,\n",
       " '-': 30,\n",
       " 'b': 31,\n",
       " 'w': 32,\n",
       " 'h': 33,\n",
       " 'ans': 34,\n",
       " '+': 35,\n",
       " 'split': 36,\n",
       " '/': 37,\n",
       " 'r': 38,\n",
       " 'else': 39,\n",
       " 'range': 40,\n",
       " 'max': 41,\n",
       " 'map': 42,\n",
       " '#': 43,\n",
       " '9': 44,\n",
       " 'list': 45,\n",
       " '%': 46,\n",
       " '\\t': 47,\n",
       " 'import': 48,\n",
       " 'math': 49,\n",
       " 'len': 50,\n",
       " 'l': 51,\n",
       " 'str': 52,\n",
       " 'pi': 53,\n",
       " 'def': 54,\n",
       " 'append': 55,\n",
       " 'and': 56,\n",
       " 'return': 57,\n",
       " 'elif': 58,\n",
       " 'sys': 59,\n",
       " '>': 60,\n",
       " 'c': 61,\n",
       " '3': 62,\n",
       " '5': 63,\n",
       " '{': 64,\n",
       " '}': 65,\n",
       " 'f': 66,\n",
       " '!': 67,\n",
       " 'm': 68,\n",
       " 'gcd': 69,\n",
       " 'while': 70,\n",
       " '<': 71,\n",
       " '6': 72,\n",
       " 'min': 73,\n",
       " 'float': 74,\n",
       " 'num': 75,\n",
       " 'main': 76,\n",
       " 'join': 77,\n",
       " \"''\": 78,\n",
       " '8': 79,\n",
       " 'stdin': 80,\n",
       " 'd': 81,\n",
       " 'format': 82,\n",
       " 't': 83,\n",
       " 'true': 84,\n",
       " '?': 85,\n",
       " 'or': 86,\n",
       " 'from': 87,\n",
       " 'replace': 88,\n",
       " '4': 89,\n",
       " 'reverse': 90,\n",
       " 'self': 91,\n",
       " 'raw': 92,\n",
       " 'j': 93,\n",
       " 'tmp': 94,\n",
       " 'break': 95,\n",
       " 'line': 96,\n",
       " 'area': 97,\n",
       " 'k': 98,\n",
       " \"'b'\": 99,\n",
       " 'end': 100,\n",
       " 'sorted': 101,\n",
       " 'pop': 102,\n",
       " 'readline': 103,\n",
       " 'v': 104,\n",
       " 'sum': 105,\n",
       " 'except': 106,\n",
       " 'try': 107,\n",
       " 'result': 108,\n",
       " 'name': 109,\n",
       " 'p': 110,\n",
       " 'utf': 111,\n",
       " 'coding': 112,\n",
       " 'lst': 113,\n",
       " 'data': 114,\n",
       " 'count': 115,\n",
       " 'copy': 116,\n",
       " 'z': 117,\n",
       " 'res': 118,\n",
       " '7': 119,\n",
       " 'li': 120,\n",
       " 'lambda': 121,\n",
       " 'second': 122,\n",
       " 'nums': 123,\n",
       " 'sort': 124,\n",
       " 'mod': 125,\n",
       " 'index': 126,\n",
       " 'log': 127,\n",
       " ';': 128,\n",
       " 'flag': 129,\n",
       " 'temp': 130,\n",
       " 'collections': 131,\n",
       " 'new': 132,\n",
       " 'eoferror': 133,\n",
       " 'divide': 134,\n",
       " 'rstrip': 135,\n",
       " 'string': 136,\n",
       " 'out': 137,\n",
       " 'key': 138,\n",
       " 'divisor': 139,\n",
       " 'np': 140,\n",
       " 'reversed': 141,\n",
       " 'ma': 142,\n",
       " \"f'\": 143,\n",
       " 'other': 144,\n",
       " 'idx': 145,\n",
       " 'ret': 146,\n",
       " 'circle': 147,\n",
       " 'length': 148,\n",
       " 'not': 149,\n",
       " 'answer': 150,\n",
       " 'continue': 151,\n",
       " 'set': 152,\n",
       " 'false': 153,\n",
       " '\\\\': 154,\n",
       " 'val': 155,\n",
       " 'odd': 156,\n",
       " 'even': 157,\n",
       " 'deque': 158,\n",
       " 'as': 159,\n",
       " 'del': 160,\n",
       " 'cnt': 161,\n",
       " 'strip': 162,\n",
       " 'round': 163,\n",
       " 'is': 164,\n",
       " 'q': 165,\n",
       " 'digit': 166,\n",
       " 'u': 167,\n",
       " 'text': 168,\n",
       " 'word': 169,\n",
       " 'aa': 170,\n",
       " 'bin': 171,\n",
       " 'counter': 172,\n",
       " 'bisect': 173,\n",
       " 'python': 174,\n",
       " 'first': 175,\n",
       " 'menseki': 176,\n",
       " 'arr': 177,\n",
       " 'usr': 178,\n",
       " 'value': 179,\n",
       " 'pass': 180,\n",
       " 'class': 181,\n",
       " 'e': 182,\n",
       " 'exit': 183,\n",
       " 'alist': 184,\n",
       " 'amax': 185,\n",
       " 'common': 186,\n",
       " 'distance': 187,\n",
       " 'abs': 188,\n",
       " 'env': 189,\n",
       " 'dy': 190,\n",
       " 'sep': 191,\n",
       " 'ns': 192,\n",
       " 'ss': 193,\n",
       " 'itertools': 194,\n",
       " 'seq': 195,\n",
       " 'tate': 196,\n",
       " 'vlist': 197,\n",
       " 'remove': 198,\n",
       " 'find': 199,\n",
       " 'inp': 200,\n",
       " 'number': 201,\n",
       " 'big': 202,\n",
       " 'check': 203,\n",
       " 'table': 204,\n",
       " '&': 205,\n",
       " 'solve': 206,\n",
       " 'yoko': 207,\n",
       " 'o': 208,\n",
       " 'type': 209,\n",
       " 'inputs': 210,\n",
       " 'enumerate': 211,\n",
       " 'zzz': 212,\n",
       " 're': 213,\n",
       " 'display': 214,\n",
       " 'fractions': 215,\n",
       " 'nl': 216,\n",
       " 'array': 217,\n",
       " 'numbers': 218,\n",
       " 'nmax': 219,\n",
       " 'dx': 220,\n",
       " 'xy': 221,\n",
       " 'setrecursionlimit': 222,\n",
       " 'heapq': 223,\n",
       " 'none': 224,\n",
       " 'stack': 225,\n",
       " 'tuple': 226,\n",
       " 'all': 227,\n",
       " 'multi': 228,\n",
       " 'flg': 229,\n",
       " 'inf': 230,\n",
       " 'deepcopy': 231,\n",
       " 'ind': 232,\n",
       " 'mi': 233,\n",
       " 'linklist': 234,\n",
       " 'numpy': 235,\n",
       " 'size': 236,\n",
       " 'add': 237,\n",
       " 'dic': 238,\n",
       " 'test': 239,\n",
       " 'moji': 240,\n",
       " 'insert': 241,\n",
       " 'maxa': 242,\n",
       " 'hoge': 243,\n",
       " 'buff': 244,\n",
       " 'cent': 245,\n",
       " 'cog': 246,\n",
       " 'greatest': 247,\n",
       " 'circumference': 248,\n",
       " 'parents': 249,\n",
       " 'sub': 250,\n",
       " 'functools': 251,\n",
       " 'yes': 252,\n",
       " 'no': 253,\n",
       " 'floor': 254,\n",
       " 'ceil': 255,\n",
       " 'modint': 256,\n",
       " 'sec': 257,\n",
       " 'calc': 258,\n",
       " 'item': 259,\n",
       " 'pat': 260,\n",
       " 'men': 261,\n",
       " 'ev': 262,\n",
       " 'od': 263,\n",
       " 'readlines': 264,\n",
       " 'innums': 265,\n",
       " 'na': 266,\n",
       " \"b'\": 267,\n",
       " 'defaultdict': 268,\n",
       " 'time': 269,\n",
       " 'get': 270,\n",
       " 'exception': 271,\n",
       " 'on': 272,\n",
       " 'al': 273,\n",
       " \"n'\": 274,\n",
       " \"'''\": 275,\n",
       " 'your': 276,\n",
       " 'code': 277,\n",
       " 'here': 278,\n",
       " 'ls': 279,\n",
       " 'pow': 280,\n",
       " 'ascii': 281,\n",
       " 'left': 282,\n",
       " 'maxindex': 283,\n",
       " 'apple': 284,\n",
       " 'sq': 285,\n",
       " 'double': 286,\n",
       " 'whxy': 287,\n",
       " 'sm': 288,\n",
       " 'output': 289,\n",
       " 'full': 290,\n",
       " 'small': 291,\n",
       " 'radius': 292,\n",
       " 'stdout': 293,\n",
       " 'write': 294,\n",
       " 'sc': 295,\n",
       " 'heappop': 296,\n",
       " 'next': 297,\n",
       " 'total': 298,\n",
       " 'sqrt': 299,\n",
       " 'combinations': 300,\n",
       " 'itemgetter': 301,\n",
       " 'to': 302,\n",
       " 'raise': 303,\n",
       " 'st': 304,\n",
       " 'sa': 305,\n",
       " 'lists': 306,\n",
       " 'two': 307,\n",
       " 'forward': 308,\n",
       " 'backward': 309,\n",
       " 'fst': 310,\n",
       " 'huga': 311,\n",
       " 'ruiseki': 312,\n",
       " 'ai': 313,\n",
       " 'tatemax': 314,\n",
       " 'yokomax': 315,\n",
       " 'abc': 316,\n",
       " 'mem': 317,\n",
       " 'center': 318,\n",
       " 'bb': 319,\n",
       " 'translate': 320,\n",
       " \"'x'\": 321,\n",
       " 'inl': 322,\n",
       " 'sentence': 323,\n",
       " 'bool': 324,\n",
       " 'lru': 325,\n",
       " 'cache': 326,\n",
       " 'ch': 327,\n",
       " 'reduce': 328,\n",
       " 'plist': 329,\n",
       " 'row': 330,\n",
       " 'accumulate': 331,\n",
       " 'permutations': 332,\n",
       " 'operator': 333,\n",
       " 'mul': 334,\n",
       " 'digits': 335,\n",
       " 'lcm': 336,\n",
       " 'lis': 337,\n",
       " 'info': 338,\n",
       " 'snd': 339,\n",
       " 'rtol': 340,\n",
       " 'rectangle': 341,\n",
       " 'values': 342,\n",
       " 'lines': 343,\n",
       " 'div': 344,\n",
       " 'scanner': 345,\n",
       " 'divisors': 346,\n",
       " 'ln': 347,\n",
       " 'heappush': 348,\n",
       " 'os': 349,\n",
       " 'queue': 350,\n",
       " '@': 351,\n",
       " 'rank': 352,\n",
       " 'cin': 353,\n",
       " 'popleft': 354,\n",
       " 'buffer': 355,\n",
       " \"'inf'\": 356,\n",
       " '^': 357,\n",
       " 'factorial': 358,\n",
       " 'product': 359,\n",
       " 'stk': 360,\n",
       " 'of': 361,\n",
       " 'read': 362,\n",
       " 'ltor': 363,\n",
       " 'maxvalue': 364,\n",
       " 'ansx': 365,\n",
       " 'right': 366,\n",
       " 'buf': 367,\n",
       " 'candidate': 368,\n",
       " 'wk': 369,\n",
       " 'sv': 370,\n",
       " 'cd': 371,\n",
       " 'converter': 372,\n",
       " 'lg': 373,\n",
       " 'yy': 374,\n",
       " 'maketrans': 375,\n",
       " 'sam': 376,\n",
       " 'counts': 377,\n",
       " 'part': 378,\n",
       " 'fileinput': 379,\n",
       " 'dn': 380,\n",
       " 'up': 381,\n",
       " '|': 382,\n",
       " 'init': 383,\n",
       " 'inpl': 384,\n",
       " 'ri': 385,\n",
       " 'reslut': 386,\n",
       " 'keys': 387,\n",
       " 'lowercase': 388,\n",
       " 'func': 389,\n",
       " 'bs': 390,\n",
       " 'll': 391,\n",
       " 'vmax': 392,\n",
       " 'maxx': 393,\n",
       " \"'a'\": 394,\n",
       " 'areas': 395,\n",
       " 'decimal': 396,\n",
       " 'hs': 397,\n",
       " 'only': 398,\n",
       " 'ver': 399,\n",
       " 'harf': 400,\n",
       " 'half': 401,\n",
       " 'most': 402,\n",
       " 'wy': 403,\n",
       " 'xh': 404,\n",
       " 'g': 405,\n",
       " 'と': 406,\n",
       " 'an': 407,\n",
       " 'correct': 408,\n",
       " 'sumnum': 409,\n",
       " 'sz': 410,\n",
       " 'order': 411,\n",
       " \"'yes'\": 412,\n",
       " \"'no'\": 413,\n",
       " 'yn': 414,\n",
       " 'now': 415,\n",
       " 'fac': 416,\n",
       " 'dict': 417,\n",
       " 'lx': 418,\n",
       " 'resolve': 419,\n",
       " 'rl': 420,\n",
       " 'uppercase': 421,\n",
       " 'firsta': 422,\n",
       " 'indexed': 423,\n",
       " 'argmax': 424,\n",
       " 'kou': 425,\n",
       " 'lmax': 426,\n",
       " 'rmax': 427,\n",
       " 'id': 428,\n",
       " 'pr': 429,\n",
       " 'heapify': 430,\n",
       " 'newlist': 431,\n",
       " 'mx': 432,\n",
       " 'maxim': 433,\n",
       " 'maxcnt': 434,\n",
       " 'maxidx': 435,\n",
       " 'pos': 436,\n",
       " 'kk': 437,\n",
       " 'xx': 438,\n",
       " 'ap': 439,\n",
       " 'ac': 440,\n",
       " 'slove': 441,\n",
       " 'anoa': 442,\n",
       " 'anslist': 443,\n",
       " 'ii': 444,\n",
       " 'arg': 445,\n",
       " 'ansy': 446,\n",
       " 'dup': 447,\n",
       " 'cutting': 448,\n",
       " 'atcoder': 449,\n",
       " 'hori': 450,\n",
       " 'co': 451,\n",
       " 'has': 452,\n",
       " 'seen': 453,\n",
       " 'inds': 454,\n",
       " 'cap': 455,\n",
       " 'ord': 456,\n",
       " 'extend': 457,\n",
       " 'rectanglecutting': 458,\n",
       " 'sh': 459,\n",
       " 'dbg': 460,\n",
       " 'yset': 461,\n",
       " 'matched': 462,\n",
       " 'intersection': 463,\n",
       " 'herf': 464,\n",
       " 'smaller': 465,\n",
       " 'gcm': 466,\n",
       " 'long': 467,\n",
       " 'gc': 468,\n",
       " 'maximum': 469,\n",
       " 'natural': 470,\n",
       " 'slist': 471,\n",
       " '¨': 472,\n",
       " 'sen': 473,\n",
       " 'py': 474,\n",
       " 'around': 475,\n",
       " 'cn': 476,\n",
       " 'elem': 477,\n",
       " 'ilinei': 478,\n",
       " 'members': 479,\n",
       " 'roots': 480,\n",
       " 'retlist': 481,\n",
       " 'cf': 482,\n",
       " 'inpsum': 483,\n",
       " 'plus': 484,\n",
       " 'absum': 485,\n",
       " 'chr': 486,\n",
       " 'ni': 487,\n",
       " 'union': 488,\n",
       " 'comb': 489,\n",
       " 'si': 490,\n",
       " 'hypot': 491,\n",
       " 'sin': 492,\n",
       " 'cos': 493,\n",
       " 'radians': 494,\n",
       " 'datas': 495,\n",
       " 'modinv': 496,\n",
       " 'jun': 497,\n",
       " 'author': 498,\n",
       " 'seconda': 499,\n",
       " 'bk': 500,\n",
       " 'cp': 501,\n",
       " 'fi': 502,\n",
       " 'itr': 503,\n",
       " 'open': 504,\n",
       " 'instas': 505,\n",
       " 'saidai': 506,\n",
       " 'seca': 507,\n",
       " 'delete': 508,\n",
       " 'la': 509,\n",
       " 'orange': 510,\n",
       " 'idxs': 511,\n",
       " 'argsort': 512,\n",
       " 'compare': 513,\n",
       " 'tempmax': 514,\n",
       " 'nlist': 515,\n",
       " 'fastinput': 516,\n",
       " 'others': 517,\n",
       " 'target': 518,\n",
       " 'dou': 519,\n",
       " 'nearx': 520,\n",
       " 'neary': 521,\n",
       " 'pprint': 522,\n",
       " 'cond': 523,\n",
       " 'width': 524,\n",
       " 'ws': 525,\n",
       " 'bunkatsu': 526,\n",
       " 'beginner': 527,\n",
       " 'contest': 528,\n",
       " 'dif': 529,\n",
       " \"d'\": 530,\n",
       " 'horizon': 531,\n",
       " 'vertical': 532,\n",
       " 'another': 533,\n",
       " 'args': 534,\n",
       " 'nr': 535,\n",
       " 'point': 536,\n",
       " 'way': 537,\n",
       " 'case': 538,\n",
       " 'hantei': 539,\n",
       " 'ld': 540,\n",
       " 'lu': 541,\n",
       " 'rd': 542,\n",
       " 'ru': 543,\n",
       " 'points': 544,\n",
       " 'sp': 545,\n",
       " 'fraction': 546,\n",
       " 'kb': 547,\n",
       " 'bytes': 548,\n",
       " 'ext': 549,\n",
       " 'include': 550,\n",
       " 'larger': 551,\n",
       " 'euclidean': 552,\n",
       " 'mygcd': 553,\n",
       " 'public': 554,\n",
       " 'helper': 555,\n",
       " 'swap': 556,\n",
       " 'ar': 557,\n",
       " 'euclideanalg': 558,\n",
       " 'my': 559,\n",
       " 'π': 560,\n",
       " 'en': 561,\n",
       " 'pai': 562,\n",
       " 'numlist': 563,\n",
       " 'date': 564,\n",
       " 'iline': 565,\n",
       " 'it': 566,\n",
       " 'sr': 567,\n",
       " 'longest': 568,\n",
       " 'previous': 569,\n",
       " 'char': 570,\n",
       " 'stock': 571,\n",
       " 'nstr': 572,\n",
       " 'ij': 573,\n",
       " 'seeds': 574,\n",
       " 'valueerror': 575,\n",
       " 'ds': 576,\n",
       " 'countlist': 577,\n",
       " 'inputnum': 578,\n",
       " 'splited': 579,\n",
       " 'remaining': 580,\n",
       " 'questions': 581,\n",
       " 'numl': 582,\n",
       " 'tokens': 583,\n",
       " 'returnlist': 584,\n",
       " '文字列の入力': 585,\n",
       " 'typing': 586,\n",
       " 'indexerror': 587,\n",
       " 'orders': 588,\n",
       " 'yesno': 589,\n",
       " 'random': 590,\n",
       " 'maxsize': 591,\n",
       " 'same': 592,\n",
       " 'link': 593,\n",
       " 'power': 594,\n",
       " 'cino': 595,\n",
       " 'intl': 596,\n",
       " 'rr': 597,\n",
       " 'rs': 598,\n",
       " '入力': 599,\n",
       " 'cha': 600,\n",
       " 'honmono': 601,\n",
       " 'with': 602,\n",
       " 'groupby': 603,\n",
       " 'zip': 604,\n",
       " 'judge': 605,\n",
       " 'intinput': 606,\n",
       " 'empty': 607,\n",
       " 'convert': 608,\n",
       " 'the': 609,\n",
       " 'created': 610,\n",
       " 'mapint': 611,\n",
       " 'intcolslist': 612,\n",
       " 'roundup': 613,\n",
       " 'cont': 614,\n",
       " 'submit': 615,\n",
       " 'dash': 616,\n",
       " 'se': 617,\n",
       " 'nd': 618,\n",
       " 'seta': 619,\n",
       " 'tempa': 620,\n",
       " 'nf': 621,\n",
       " '`': 622,\n",
       " 'argfst': 623,\n",
       " 'one': 624,\n",
       " 'nibanme': 625,\n",
       " 'filter': 626,\n",
       " 'mm': 627,\n",
       " 'escape': 628,\n",
       " 'rema': 629,\n",
       " 'nextvalue': 630,\n",
       " 'largest': 631,\n",
       " 'lf': 632,\n",
       " 'halfw': 633,\n",
       " 'halfh': 634,\n",
       " 'height': 635,\n",
       " 'makey': 636,\n",
       " 'xeven': 637,\n",
       " 'yeven': 638,\n",
       " 'square': 639,\n",
       " 'debug': 640,\n",
       " 'kwargs': 641,\n",
       " 'cenx': 642,\n",
       " 'ceny': 643,\n",
       " 'axis': 644,\n",
       " 'lsq': 645,\n",
       " 'rsq': 646,\n",
       " 'usq': 647,\n",
       " 'dsq': 648,\n",
       " 'items': 649,\n",
       " 'alp': 650,\n",
       " 'mins': 651,\n",
       " 'jp': 652,\n",
       " '三角形': 653,\n",
       " 'の面積': 654,\n",
       " 'separate': 655,\n",
       " 'straight': 656,\n",
       " 'の傾き': 657,\n",
       " 'について': 658,\n",
       " 'siki': 659,\n",
       " 'void': 660,\n",
       " 'scanf': 661,\n",
       " 'stdinput': 662,\n",
       " 'xset': 663,\n",
       " 'getgcm': 664,\n",
       " 'divider': 665,\n",
       " 'euclid': 666,\n",
       " 'ex': 667,\n",
       " 'getresult': 668,\n",
       " 'greatestdivisor': 669,\n",
       " 'twoset': 670,\n",
       " 'static': 671,\n",
       " 'system': 672,\n",
       " 'nextint': 673,\n",
       " 'inputarrline': 674,\n",
       " 'xcd': 675,\n",
       " 'ycd': 676,\n",
       " '÷': 677,\n",
       " 'の約数': 678,\n",
       " 'shu': 679,\n",
       " 'nag': 680,\n",
       " 'cir': 681,\n",
       " '¢': 682,\n",
       " 'pie': 683,\n",
       " 'circum': 684,\n",
       " 'perimeter': 685,\n",
       " 'ensyu': 686,\n",
       " 'mennseki': 687,\n",
       " 'ennsyuu': 688,\n",
       " 'squre': 689,\n",
       " 'are': 690,\n",
       " 'calculate': 691,\n",
       " '‘': 692,\n",
       " 'rx': 693,\n",
       " 'gyaku': 694,\n",
       " 'dst': 695,\n",
       " 'ir': 696,\n",
       " 'root': 697,\n",
       " 'group': 698,\n",
       " 'make': 699,\n",
       " 'inums': 700,\n",
       " 'intin': 701,\n",
       " 'global': 702,\n",
       " 'xylist': 703,\n",
       " 'setdefault': 704,\n",
       " 'nodecount': 705,\n",
       " 'repl': 706,\n",
       " \"'o'\": 707,\n",
       " 'bi': 708,\n",
       " 'mojiretu': 709,\n",
       " 'rev': 710,\n",
       " 'nine': 711,\n",
       " \"'q'\": 712,\n",
       " 'do': 713,\n",
       " 'head': 714,\n",
       " 'foll': 715,\n",
       " 'nint': 716,\n",
       " 'digitnumber': 717,\n",
       " 'eval': 718,\n",
       " 'po': 719,\n",
       " 'xp': 720,\n",
       " 'yp': 721,\n",
       " 'strs': 722,\n",
       " 'jk': 723,\n",
       " 'digitstr': 724,\n",
       " 'wa': 725,\n",
       " 'keta': 726,\n",
       " 'numplusnum': 727,\n",
       " 'bが存在する間実行': 728,\n",
       " '最初のbのインデックスを取得': 729,\n",
       " 'bが先頭でないとき': 730,\n",
       " 'bの': 731,\n",
       " '個前とbを消す': 732,\n",
       " 'bが先頭の時': 733,\n",
       " 'この区切り文字でリストを連結': 734,\n",
       " '答え用意': 735,\n",
       " '末尾に要素追加': 736,\n",
       " '長さ': 737,\n",
       " '指定した位置': 738,\n",
       " '今回は末尾': 739,\n",
       " 'の要素を削除': 740,\n",
       " 'datetime': 741,\n",
       " 'alphabet': 742,\n",
       " 'abcdefghijklmnopqrstuvwxyz': 743,\n",
       " 'unite': 744,\n",
       " 'aはbの累乗数か': 745,\n",
       " 'facs': 746,\n",
       " '~': 747,\n",
       " 'primes': 748,\n",
       " 'prime': 749,\n",
       " 'ifelse': 750,\n",
       " 'factorize': 751,\n",
       " 'hani': 752,\n",
       " 'seifu': 753,\n",
       " 'cina': 754,\n",
       " 'inpln': 755,\n",
       " 'rm': 756,\n",
       " '空のリスト': 757,\n",
       " 'cが': 758,\n",
       " '文字目からaまで': 759,\n",
       " 'bでなければアペンド': 760,\n",
       " 'bであってさらにリストがからでなければpop': 761,\n",
       " 'listをくっつけた文字にできる': 762,\n",
       " 'enter': 763,\n",
       " 'tan': 764,\n",
       " 'asin': 765,\n",
       " 'acos': 766,\n",
       " 'atan': 767,\n",
       " 'degrees': 768,\n",
       " 'replacement': 769,\n",
       " 'バイナリハックイージー': 770,\n",
       " 'lifoqueue': 771,\n",
       " 'cmp': 772,\n",
       " \"'yneos'\": 773,\n",
       " 'mulinputs': 774,\n",
       " 'lineinputs': 775,\n",
       " 'truediv': 776,\n",
       " 'zerodivisionerror': 777,\n",
       " 'sの逆数は求まりません。': 778,\n",
       " 'put': 779,\n",
       " '出力': 780,\n",
       " 'eof': 781,\n",
       " '解けた': 782,\n",
       " '入力の文字列': 783,\n",
       " '文字ずつ見てゆく': 784,\n",
       " 'もし': 785,\n",
       " 'か': 786,\n",
       " 'の場合、それを追加': 787,\n",
       " 'もしバックスペースの場合、': 788,\n",
       " '文字削除': 789,\n",
       " 'initialization': 790,\n",
       " 'traverse': 791,\n",
       " 'sl': 792,\n",
       " 'thu': 793,\n",
       " 'sd': 794,\n",
       " 'stringlist': 795,\n",
       " 'intlistlist': 796,\n",
       " 'touppermultiple': 797,\n",
       " 'tolowermultiple': 798,\n",
       " 'nearpow': 799,\n",
       " 'duplicate': 800,\n",
       " 'maxn': 801,\n",
       " 'ones': 802,\n",
       " 'dtype': 803,\n",
       " 'inpur': 804,\n",
       " '最大値のindexを探し、最大値が複数あるか確認': 805,\n",
       " 'maxi': 806,\n",
       " 'signal': 807,\n",
       " '入力全て': 808,\n",
       " 'eofまで': 809,\n",
       " 'を': 810,\n",
       " 'iter': 811,\n",
       " 'に変換': 812,\n",
       " 'rli': 813,\n",
       " 'assert': 814,\n",
       " 'findex': 815,\n",
       " 'iiou': 816,\n",
       " 'nlargest': 817,\n",
       " '横にラインを引く': 818,\n",
       " 'pirnt': 819,\n",
       " '\\u3000or': 820,\n",
       " '長方形を分割するためには長方形の中心を通る必要がある': 821,\n",
       " 'ただし、指定された座標が中心の場合は無限に直線が弾ける': 822,\n",
       " '長方形内であれば、中心からの対象点を選ぶ事により、': 823,\n",
       " '分割できる': 824,\n",
       " '長方形の中心を通れば、': 825,\n",
       " '分割できる。そうじゃないとだめ': 826,\n",
       " 'quit': 827,\n",
       " '真ん中を通れば必ず二分できてこれが最大値だが、自分自身が真ん中の座標の場合は自分自信を通るどんな直線も二分できる': 828,\n",
       " '対角線': 829,\n",
       " '点': 830,\n",
       " 'でつoo': 831,\n",
       " 'you': 832,\n",
       " 'play': 833,\n",
       " 'cards': 834,\n",
       " \"you're\": 835,\n",
       " 'dealt': 836,\n",
       " 'nn': 837,\n",
       " 'tue': 838,\n",
       " 'may': 839,\n",
       " 'shinba': 840,\n",
       " '直線上に存在すれば三角形の面積': 841,\n",
       " '直線上にない時': 842,\n",
       " 'sqs': 843,\n",
       " 'ignore': 844,\n",
       " 'trailing': 845,\n",
       " 'spaces': 846,\n",
       " 'dprint': 847,\n",
       " 'readints': 848,\n",
       " 'ncr': 849,\n",
       " 'duplicates': 850,\n",
       " 'coordinates': 851,\n",
       " 'any': 852,\n",
       " 'equal': 853,\n",
       " '再帰関数の上限': 854,\n",
       " '以上の場合python': 855,\n",
       " 'dcp': 856,\n",
       " '分探索': 857,\n",
       " 'aはソート済みである必要あり。aの中からx未満の要素数を返す。rightだと以下': 858,\n",
       " 'appendleft': 859,\n",
       " 'rotate': 860,\n",
       " 'で': 861,\n",
       " '→': 862,\n",
       " 'にn回ローテート': 863,\n",
       " '文字列を個数カウント辞書に、': 864,\n",
       " '累積和': 865,\n",
       " 'としないこと、返り値はnone': 866,\n",
       " '古いatcoderコンテストの場合gcdなどはここからimportする': 867,\n",
       " 'pypyでもうごく': 868,\n",
       " 'maxsizeは保存するデータ数の最大値、': 869,\n",
       " 'nが最も高効率': 870,\n",
       " 'printe': 871,\n",
       " 'file': 872,\n",
       " 'stderr': 873,\n",
       " 'printl': 874,\n",
       " 'matmat': 875,\n",
       " 'matvec': 876,\n",
       " '二個目の要素で降順並び替え': 877,\n",
       " '行ベクトル': 878,\n",
       " '改行ベクトル': 879,\n",
       " '改行行列': 880,\n",
       " '問題：https': 881,\n",
       " 'contests': 882,\n",
       " 'tasks': 883,\n",
       " '長方形の面積': 884,\n",
       " \"'wの面積'\": 885,\n",
       " \"'hの面積'\": 886,\n",
       " \"'wの長方形'\": 887,\n",
       " \"'hの長方形'\": 888,\n",
       " \"'w\": 889,\n",
       " \"h'\": 890,\n",
       " 'maxが複数あるか': 891,\n",
       " 'inside': 892,\n",
       " '初項s': 893,\n",
       " '交差dのn個の数列の和': 894,\n",
       " 'arithmetic': 895,\n",
       " 'progression': 896,\n",
       " 'aとbの最大公約数': 897,\n",
       " 'aとbの最小公倍数': 898,\n",
       " 'xyplus': 899,\n",
       " '重心に来る時は複数、線上と内部は': 900,\n",
       " 'こ': 901,\n",
       " '線上にある時': 902,\n",
       " '等分可能': 903,\n",
       " '縦横で分割する': 904,\n",
       " '縦': 905,\n",
       " '横': 906,\n",
       " 'xの方が大きくなくてはならない': 907,\n",
       " 'future': 908,\n",
       " 'function': 909,\n",
       " 'io': 910,\n",
       " 'start': 911,\n",
       " 'clock': 912,\n",
       " 'stdio': 913,\n",
       " 'printf': 914,\n",
       " '計算量問題あり': 915,\n",
       " 'version': 916,\n",
       " 'minor': 917,\n",
       " '小林勇希': 918,\n",
       " 'mb': 919,\n",
       " 'alds': 920,\n",
       " 'java': 921,\n",
       " 'util': 922,\n",
       " 'close': 923,\n",
       " 'println': 924,\n",
       " 'はswap': 925,\n",
       " '値の交換': 926,\n",
       " 'を表す': 927,\n",
       " 'ユークリッドの互除法により最大公約数': 928,\n",
       " 'greatestcommondivisorを求める': 929,\n",
       " '以下のようなアルゴリズムはn回の割り算を行う必要があるため、大きな数に対しては時間内に出力を得ることはできない': 930,\n",
       " 'downto': 931,\n",
       " 'がともに': 932,\n",
       " 'で割り切れる': 933,\n",
       " 'ユークリッドの互除法による最大公約数の判定': 934,\n",
       " 'ユークリッドの互除法': 935,\n",
       " 'bq': 936,\n",
       " '一行に複数入力': 937,\n",
       " '標準入力を行う': 938,\n",
       " 'printarrline': 939,\n",
       " 'todo': 940,\n",
       " '最初に確認のswap': 941,\n",
       " '毎回入れ替えるawap': 942,\n",
       " '§': 943,\n",
       " '°': 944,\n",
       " 'xの約数': 945,\n",
       " 'yの約数': 946,\n",
       " 'ｘとyの公約数': 947,\n",
       " 'ｘとyの最大公約数': 948,\n",
       " '２つの整数': 949,\n",
       " 'について、x': 950,\n",
       " 'の余りがともに': 951,\n",
       " 'となる': 952,\n",
       " 'のうち最大のものを、x': 953,\n",
       " 'の最大公約数（greatest': 954,\n",
       " 'divisor）と言います。': 955,\n",
       " '例えば、': 956,\n",
       " 'の最大公約数': 957,\n",
       " 'は': 958,\n",
       " 'となります。これは、': 959,\n",
       " '、': 960,\n",
       " 'の公約数': 961,\n",
       " 'の最大値となります。': 962,\n",
       " 'froat': 963,\n",
       " '円周率': 964,\n",
       " '浮動小数点数で読む': 965,\n",
       " 'warnings': 966,\n",
       " 'urllib': 967,\n",
       " '面積': 968,\n",
       " '円周': 969,\n",
       " \"lf'\": 970,\n",
       " 'sprit': 971,\n",
       " 'inport': 972,\n",
       " 'iostream': 973,\n",
       " 'iomanip': 974,\n",
       " 'using': 975,\n",
       " 'namespace': 976,\n",
       " 'std': 977,\n",
       " 'const': 978,\n",
       " 'cout': 979,\n",
       " 'fixed': 980,\n",
       " 'setprecision': 981,\n",
       " 'endl': 982,\n",
       " 'dev': 983,\n",
       " 'null': 984,\n",
       " 'itp': 985,\n",
       " 'sute': 986,\n",
       " 'iterable': 987,\n",
       " \"print'\": 988,\n",
       " 'が': 989,\n",
       " 'より大きいとき、つまり最初の要素ではないとき空白を出力': 990,\n",
       " '改行を出力': 991,\n",
       " \"'format\": 992,\n",
       " 'reveres': 993,\n",
       " 'はリストを引数に展開': 994,\n",
       " 'xrange': 995,\n",
       " 'cording': 996,\n",
       " '数列の反転': 997,\n",
       " 'スペースがない数字リストを読み込み': 998,\n",
       " '優先度付きキュー': 999,\n",
       " '最小値取り出し': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a566d4dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130369, 10)\n",
      "(130369,)\n",
      "623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 21:36:55.510751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "print(dataX.shape)\n",
    "print(dataY.shape)\n",
    "print(len(np.unique(dataY)))\n",
    "# Lets build model\n",
    "vocab_size_to_predict = len(tokenizer.word_index)+1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size_to_predict, output_dim=32, mask_zero=True))\n",
    "model.add(LSTM(256, input_shape=(dataX.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = vocab_size_to_predict, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84345733",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9. Train and save the best model on Data here\n",
    "#filepath = \"LSTM_model.hdf5\"\n",
    "#checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    " #                            save_best_only=True, mode='min')\n",
    "#history = model.fit(dataX, dataY, epochs=20, batch_size=64, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08ecc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = dataX\n",
    "y = dataY\n",
    "dataX, X_test, dataY, y_test = train_test_split(X, y, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0eeb8f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0 37]\n",
      " [ 0  0  0 ...  0 37  5]\n",
      " ...\n",
      " [ 3 60  4 ...  3 60  4]\n",
      " [60  4  2 ... 60  4  4]\n",
      " [ 4  2 17 ...  4  4  1]]\n",
      "[37  5 31 ...  4  1  1]\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00bcae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 1,\n",
       " '\\n': 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '=': 5,\n",
       " ':': 6,\n",
       " ',': 7,\n",
       " '.': 8,\n",
       " '1': 9,\n",
       " '_': 10,\n",
       " 'a': 11,\n",
       " '\"': 12,\n",
       " '[': 13,\n",
       " ']': 14,\n",
       " 'x': 15,\n",
       " '0': 16,\n",
       " 'i': 17,\n",
       " 'print': 18,\n",
       " 'input': 19,\n",
       " '*': 20,\n",
       " 'y': 21,\n",
       " 'n': 22,\n",
       " 'int': 23,\n",
       " '2': 24,\n",
       " \"'\": 25,\n",
       " 'if': 26,\n",
       " 's': 27,\n",
       " 'in': 28,\n",
       " 'for': 29,\n",
       " '-': 30,\n",
       " 'b': 31,\n",
       " 'w': 32,\n",
       " 'h': 33,\n",
       " 'ans': 34,\n",
       " '+': 35,\n",
       " 'split': 36,\n",
       " '/': 37,\n",
       " 'r': 38,\n",
       " 'else': 39,\n",
       " 'range': 40,\n",
       " 'max': 41,\n",
       " 'map': 42,\n",
       " '#': 43,\n",
       " '9': 44,\n",
       " 'list': 45,\n",
       " '%': 46,\n",
       " '\\t': 47,\n",
       " 'import': 48,\n",
       " 'math': 49,\n",
       " 'len': 50,\n",
       " 'l': 51,\n",
       " 'str': 52,\n",
       " 'pi': 53,\n",
       " 'def': 54,\n",
       " 'append': 55,\n",
       " 'and': 56,\n",
       " 'return': 57,\n",
       " 'elif': 58,\n",
       " 'sys': 59,\n",
       " '>': 60,\n",
       " 'c': 61,\n",
       " '3': 62,\n",
       " '5': 63,\n",
       " '{': 64,\n",
       " '}': 65,\n",
       " 'f': 66,\n",
       " '!': 67,\n",
       " 'm': 68,\n",
       " 'gcd': 69,\n",
       " 'while': 70,\n",
       " '<': 71,\n",
       " '6': 72,\n",
       " 'min': 73,\n",
       " 'float': 74,\n",
       " 'num': 75,\n",
       " 'main': 76,\n",
       " 'join': 77,\n",
       " \"''\": 78,\n",
       " '8': 79,\n",
       " 'stdin': 80,\n",
       " 'd': 81,\n",
       " 'format': 82,\n",
       " 't': 83,\n",
       " 'true': 84,\n",
       " '?': 85,\n",
       " 'or': 86,\n",
       " 'from': 87,\n",
       " 'replace': 88,\n",
       " '4': 89,\n",
       " 'reverse': 90,\n",
       " 'self': 91,\n",
       " 'raw': 92,\n",
       " 'j': 93,\n",
       " 'tmp': 94,\n",
       " 'break': 95,\n",
       " 'line': 96,\n",
       " 'area': 97,\n",
       " 'k': 98,\n",
       " \"'b'\": 99,\n",
       " 'end': 100,\n",
       " 'sorted': 101,\n",
       " 'pop': 102,\n",
       " 'readline': 103,\n",
       " 'v': 104,\n",
       " 'sum': 105,\n",
       " 'except': 106,\n",
       " 'try': 107,\n",
       " 'result': 108,\n",
       " 'name': 109,\n",
       " 'p': 110,\n",
       " 'utf': 111,\n",
       " 'coding': 112,\n",
       " 'lst': 113,\n",
       " 'data': 114,\n",
       " 'count': 115,\n",
       " 'copy': 116,\n",
       " 'z': 117,\n",
       " 'res': 118,\n",
       " '7': 119,\n",
       " 'li': 120,\n",
       " 'lambda': 121,\n",
       " 'second': 122,\n",
       " 'nums': 123,\n",
       " 'sort': 124,\n",
       " 'mod': 125,\n",
       " 'index': 126,\n",
       " 'log': 127,\n",
       " ';': 128,\n",
       " 'flag': 129,\n",
       " 'temp': 130,\n",
       " 'collections': 131,\n",
       " 'new': 132,\n",
       " 'eoferror': 133,\n",
       " 'divide': 134,\n",
       " 'rstrip': 135,\n",
       " 'string': 136,\n",
       " 'out': 137,\n",
       " 'key': 138,\n",
       " 'divisor': 139,\n",
       " 'np': 140,\n",
       " 'reversed': 141,\n",
       " 'ma': 142,\n",
       " \"f'\": 143,\n",
       " 'other': 144,\n",
       " 'idx': 145,\n",
       " 'ret': 146,\n",
       " 'circle': 147,\n",
       " 'length': 148,\n",
       " 'not': 149,\n",
       " 'answer': 150,\n",
       " 'continue': 151,\n",
       " 'set': 152,\n",
       " 'false': 153,\n",
       " '\\\\': 154,\n",
       " 'val': 155,\n",
       " 'odd': 156,\n",
       " 'even': 157,\n",
       " 'deque': 158,\n",
       " 'as': 159,\n",
       " 'del': 160,\n",
       " 'cnt': 161,\n",
       " 'strip': 162,\n",
       " 'round': 163,\n",
       " 'is': 164,\n",
       " 'q': 165,\n",
       " 'digit': 166,\n",
       " 'u': 167,\n",
       " 'text': 168,\n",
       " 'word': 169,\n",
       " 'aa': 170,\n",
       " 'bin': 171,\n",
       " 'counter': 172,\n",
       " 'bisect': 173,\n",
       " 'python': 174,\n",
       " 'first': 175,\n",
       " 'menseki': 176,\n",
       " 'arr': 177,\n",
       " 'usr': 178,\n",
       " 'value': 179,\n",
       " 'pass': 180,\n",
       " 'class': 181,\n",
       " 'e': 182,\n",
       " 'exit': 183,\n",
       " 'alist': 184,\n",
       " 'amax': 185,\n",
       " 'common': 186,\n",
       " 'distance': 187,\n",
       " 'abs': 188,\n",
       " 'env': 189,\n",
       " 'dy': 190,\n",
       " 'sep': 191,\n",
       " 'ns': 192,\n",
       " 'ss': 193,\n",
       " 'itertools': 194,\n",
       " 'seq': 195,\n",
       " 'tate': 196,\n",
       " 'vlist': 197,\n",
       " 'remove': 198,\n",
       " 'find': 199,\n",
       " 'inp': 200,\n",
       " 'number': 201,\n",
       " 'big': 202,\n",
       " 'check': 203,\n",
       " 'table': 204,\n",
       " '&': 205,\n",
       " 'solve': 206,\n",
       " 'yoko': 207,\n",
       " 'o': 208,\n",
       " 'type': 209,\n",
       " 'inputs': 210,\n",
       " 'enumerate': 211,\n",
       " 'zzz': 212,\n",
       " 're': 213,\n",
       " 'display': 214,\n",
       " 'fractions': 215,\n",
       " 'nl': 216,\n",
       " 'array': 217,\n",
       " 'numbers': 218,\n",
       " 'nmax': 219,\n",
       " 'dx': 220,\n",
       " 'xy': 221,\n",
       " 'setrecursionlimit': 222,\n",
       " 'heapq': 223,\n",
       " 'none': 224,\n",
       " 'stack': 225,\n",
       " 'tuple': 226,\n",
       " 'all': 227,\n",
       " 'multi': 228,\n",
       " 'flg': 229,\n",
       " 'inf': 230,\n",
       " 'deepcopy': 231,\n",
       " 'ind': 232,\n",
       " 'mi': 233,\n",
       " 'linklist': 234,\n",
       " 'numpy': 235,\n",
       " 'size': 236,\n",
       " 'add': 237,\n",
       " 'dic': 238,\n",
       " 'test': 239,\n",
       " 'moji': 240,\n",
       " 'insert': 241,\n",
       " 'maxa': 242,\n",
       " 'hoge': 243,\n",
       " 'buff': 244,\n",
       " 'cent': 245,\n",
       " 'cog': 246,\n",
       " 'greatest': 247,\n",
       " 'circumference': 248,\n",
       " 'parents': 249,\n",
       " 'sub': 250,\n",
       " 'functools': 251,\n",
       " 'yes': 252,\n",
       " 'no': 253,\n",
       " 'floor': 254,\n",
       " 'ceil': 255,\n",
       " 'modint': 256,\n",
       " 'sec': 257,\n",
       " 'calc': 258,\n",
       " 'item': 259,\n",
       " 'pat': 260,\n",
       " 'men': 261,\n",
       " 'ev': 262,\n",
       " 'od': 263,\n",
       " 'readlines': 264,\n",
       " 'innums': 265,\n",
       " 'na': 266,\n",
       " \"b'\": 267,\n",
       " 'defaultdict': 268,\n",
       " 'time': 269,\n",
       " 'get': 270,\n",
       " 'exception': 271,\n",
       " 'on': 272,\n",
       " 'al': 273,\n",
       " \"n'\": 274,\n",
       " \"'''\": 275,\n",
       " 'your': 276,\n",
       " 'code': 277,\n",
       " 'here': 278,\n",
       " 'ls': 279,\n",
       " 'pow': 280,\n",
       " 'ascii': 281,\n",
       " 'left': 282,\n",
       " 'maxindex': 283,\n",
       " 'apple': 284,\n",
       " 'sq': 285,\n",
       " 'double': 286,\n",
       " 'whxy': 287,\n",
       " 'sm': 288,\n",
       " 'output': 289,\n",
       " 'full': 290,\n",
       " 'small': 291,\n",
       " 'radius': 292,\n",
       " 'stdout': 293,\n",
       " 'write': 294,\n",
       " 'sc': 295,\n",
       " 'heappop': 296,\n",
       " 'next': 297,\n",
       " 'total': 298,\n",
       " 'sqrt': 299,\n",
       " 'combinations': 300,\n",
       " 'itemgetter': 301,\n",
       " 'to': 302,\n",
       " 'raise': 303,\n",
       " 'st': 304,\n",
       " 'sa': 305,\n",
       " 'lists': 306,\n",
       " 'two': 307,\n",
       " 'forward': 308,\n",
       " 'backward': 309,\n",
       " 'fst': 310,\n",
       " 'huga': 311,\n",
       " 'ruiseki': 312,\n",
       " 'ai': 313,\n",
       " 'tatemax': 314,\n",
       " 'yokomax': 315,\n",
       " 'abc': 316,\n",
       " 'mem': 317,\n",
       " 'center': 318,\n",
       " 'bb': 319,\n",
       " 'translate': 320,\n",
       " \"'x'\": 321,\n",
       " 'inl': 322,\n",
       " 'sentence': 323,\n",
       " 'bool': 324,\n",
       " 'lru': 325,\n",
       " 'cache': 326,\n",
       " 'ch': 327,\n",
       " 'reduce': 328,\n",
       " 'plist': 329,\n",
       " 'row': 330,\n",
       " 'accumulate': 331,\n",
       " 'permutations': 332,\n",
       " 'operator': 333,\n",
       " 'mul': 334,\n",
       " 'digits': 335,\n",
       " 'lcm': 336,\n",
       " 'lis': 337,\n",
       " 'info': 338,\n",
       " 'snd': 339,\n",
       " 'rtol': 340,\n",
       " 'rectangle': 341,\n",
       " 'values': 342,\n",
       " 'lines': 343,\n",
       " 'div': 344,\n",
       " 'scanner': 345,\n",
       " 'divisors': 346,\n",
       " 'ln': 347,\n",
       " 'heappush': 348,\n",
       " 'os': 349,\n",
       " 'queue': 350,\n",
       " '@': 351,\n",
       " 'rank': 352,\n",
       " 'cin': 353,\n",
       " 'popleft': 354,\n",
       " 'buffer': 355,\n",
       " \"'inf'\": 356,\n",
       " '^': 357,\n",
       " 'factorial': 358,\n",
       " 'product': 359,\n",
       " 'stk': 360,\n",
       " 'of': 361,\n",
       " 'read': 362,\n",
       " 'ltor': 363,\n",
       " 'maxvalue': 364,\n",
       " 'ansx': 365,\n",
       " 'right': 366,\n",
       " 'buf': 367,\n",
       " 'candidate': 368,\n",
       " 'wk': 369,\n",
       " 'sv': 370,\n",
       " 'cd': 371,\n",
       " 'converter': 372,\n",
       " 'lg': 373,\n",
       " 'yy': 374,\n",
       " 'maketrans': 375,\n",
       " 'sam': 376,\n",
       " 'counts': 377,\n",
       " 'part': 378,\n",
       " 'fileinput': 379,\n",
       " 'dn': 380,\n",
       " 'up': 381,\n",
       " '|': 382,\n",
       " 'init': 383,\n",
       " 'inpl': 384,\n",
       " 'ri': 385,\n",
       " 'reslut': 386,\n",
       " 'keys': 387,\n",
       " 'lowercase': 388,\n",
       " 'func': 389,\n",
       " 'bs': 390,\n",
       " 'll': 391,\n",
       " 'vmax': 392,\n",
       " 'maxx': 393,\n",
       " \"'a'\": 394,\n",
       " 'areas': 395,\n",
       " 'decimal': 396,\n",
       " 'hs': 397,\n",
       " 'only': 398,\n",
       " 'ver': 399,\n",
       " 'harf': 400,\n",
       " 'half': 401,\n",
       " 'most': 402,\n",
       " 'wy': 403,\n",
       " 'xh': 404,\n",
       " 'g': 405,\n",
       " 'と': 406,\n",
       " 'an': 407,\n",
       " 'correct': 408,\n",
       " 'sumnum': 409,\n",
       " 'sz': 410,\n",
       " 'order': 411,\n",
       " \"'yes'\": 412,\n",
       " \"'no'\": 413,\n",
       " 'yn': 414,\n",
       " 'now': 415,\n",
       " 'fac': 416,\n",
       " 'dict': 417,\n",
       " 'lx': 418,\n",
       " 'resolve': 419,\n",
       " 'rl': 420,\n",
       " 'uppercase': 421,\n",
       " 'firsta': 422,\n",
       " 'indexed': 423,\n",
       " 'argmax': 424,\n",
       " 'kou': 425,\n",
       " 'lmax': 426,\n",
       " 'rmax': 427,\n",
       " 'id': 428,\n",
       " 'pr': 429,\n",
       " 'heapify': 430,\n",
       " 'newlist': 431,\n",
       " 'mx': 432,\n",
       " 'maxim': 433,\n",
       " 'maxcnt': 434,\n",
       " 'maxidx': 435,\n",
       " 'pos': 436,\n",
       " 'kk': 437,\n",
       " 'xx': 438,\n",
       " 'ap': 439,\n",
       " 'ac': 440,\n",
       " 'slove': 441,\n",
       " 'anoa': 442,\n",
       " 'anslist': 443,\n",
       " 'ii': 444,\n",
       " 'arg': 445,\n",
       " 'ansy': 446,\n",
       " 'dup': 447,\n",
       " 'cutting': 448,\n",
       " 'atcoder': 449,\n",
       " 'hori': 450,\n",
       " 'co': 451,\n",
       " 'has': 452,\n",
       " 'seen': 453,\n",
       " 'inds': 454,\n",
       " 'cap': 455,\n",
       " 'ord': 456,\n",
       " 'extend': 457,\n",
       " 'rectanglecutting': 458,\n",
       " 'sh': 459,\n",
       " 'dbg': 460,\n",
       " 'yset': 461,\n",
       " 'matched': 462,\n",
       " 'intersection': 463,\n",
       " 'herf': 464,\n",
       " 'smaller': 465,\n",
       " 'gcm': 466,\n",
       " 'long': 467,\n",
       " 'gc': 468,\n",
       " 'maximum': 469,\n",
       " 'natural': 470,\n",
       " 'slist': 471,\n",
       " '¨': 472,\n",
       " 'sen': 473,\n",
       " 'py': 474,\n",
       " 'around': 475,\n",
       " 'cn': 476,\n",
       " 'elem': 477,\n",
       " 'ilinei': 478,\n",
       " 'members': 479,\n",
       " 'roots': 480,\n",
       " 'retlist': 481,\n",
       " 'cf': 482,\n",
       " 'inpsum': 483,\n",
       " 'plus': 484,\n",
       " 'absum': 485,\n",
       " 'chr': 486,\n",
       " 'ni': 487,\n",
       " 'union': 488,\n",
       " 'comb': 489,\n",
       " 'si': 490,\n",
       " 'hypot': 491,\n",
       " 'sin': 492,\n",
       " 'cos': 493,\n",
       " 'radians': 494,\n",
       " 'datas': 495,\n",
       " 'modinv': 496,\n",
       " 'jun': 497,\n",
       " 'author': 498,\n",
       " 'seconda': 499,\n",
       " 'bk': 500,\n",
       " 'cp': 501,\n",
       " 'fi': 502,\n",
       " 'itr': 503,\n",
       " 'open': 504,\n",
       " 'instas': 505,\n",
       " 'saidai': 506,\n",
       " 'seca': 507,\n",
       " 'delete': 508,\n",
       " 'la': 509,\n",
       " 'orange': 510,\n",
       " 'idxs': 511,\n",
       " 'argsort': 512,\n",
       " 'compare': 513,\n",
       " 'tempmax': 514,\n",
       " 'nlist': 515,\n",
       " 'fastinput': 516,\n",
       " 'others': 517,\n",
       " 'target': 518,\n",
       " 'dou': 519,\n",
       " 'nearx': 520,\n",
       " 'neary': 521,\n",
       " 'pprint': 522,\n",
       " 'cond': 523,\n",
       " 'width': 524,\n",
       " 'ws': 525,\n",
       " 'bunkatsu': 526,\n",
       " 'beginner': 527,\n",
       " 'contest': 528,\n",
       " 'dif': 529,\n",
       " \"d'\": 530,\n",
       " 'horizon': 531,\n",
       " 'vertical': 532,\n",
       " 'another': 533,\n",
       " 'args': 534,\n",
       " 'nr': 535,\n",
       " 'point': 536,\n",
       " 'way': 537,\n",
       " 'case': 538,\n",
       " 'hantei': 539,\n",
       " 'ld': 540,\n",
       " 'lu': 541,\n",
       " 'rd': 542,\n",
       " 'ru': 543,\n",
       " 'points': 544,\n",
       " 'sp': 545,\n",
       " 'fraction': 546,\n",
       " 'kb': 547,\n",
       " 'bytes': 548,\n",
       " 'ext': 549,\n",
       " 'include': 550,\n",
       " 'larger': 551,\n",
       " 'euclidean': 552,\n",
       " 'mygcd': 553,\n",
       " 'public': 554,\n",
       " 'helper': 555,\n",
       " 'swap': 556,\n",
       " 'ar': 557,\n",
       " 'euclideanalg': 558,\n",
       " 'my': 559,\n",
       " 'π': 560,\n",
       " 'en': 561,\n",
       " 'pai': 562,\n",
       " 'numlist': 563,\n",
       " 'date': 564,\n",
       " 'iline': 565,\n",
       " 'it': 566,\n",
       " 'sr': 567,\n",
       " 'longest': 568,\n",
       " 'previous': 569,\n",
       " 'char': 570,\n",
       " 'stock': 571,\n",
       " 'nstr': 572,\n",
       " 'ij': 573,\n",
       " 'seeds': 574,\n",
       " 'valueerror': 575,\n",
       " 'ds': 576,\n",
       " 'countlist': 577,\n",
       " 'inputnum': 578,\n",
       " 'splited': 579,\n",
       " 'remaining': 580,\n",
       " 'questions': 581,\n",
       " 'numl': 582,\n",
       " 'tokens': 583,\n",
       " 'returnlist': 584,\n",
       " '文字列の入力': 585,\n",
       " 'typing': 586,\n",
       " 'indexerror': 587,\n",
       " 'orders': 588,\n",
       " 'yesno': 589,\n",
       " 'random': 590,\n",
       " 'maxsize': 591,\n",
       " 'same': 592,\n",
       " 'link': 593,\n",
       " 'power': 594,\n",
       " 'cino': 595,\n",
       " 'intl': 596,\n",
       " 'rr': 597,\n",
       " 'rs': 598,\n",
       " '入力': 599,\n",
       " 'cha': 600,\n",
       " 'honmono': 601,\n",
       " 'with': 602,\n",
       " 'groupby': 603,\n",
       " 'zip': 604,\n",
       " 'judge': 605,\n",
       " 'intinput': 606,\n",
       " 'empty': 607,\n",
       " 'convert': 608,\n",
       " 'the': 609,\n",
       " 'created': 610,\n",
       " 'mapint': 611,\n",
       " 'intcolslist': 612,\n",
       " 'roundup': 613,\n",
       " 'cont': 614,\n",
       " 'submit': 615,\n",
       " 'dash': 616,\n",
       " 'se': 617,\n",
       " 'nd': 618,\n",
       " 'seta': 619,\n",
       " 'tempa': 620,\n",
       " 'nf': 621,\n",
       " '`': 622,\n",
       " 'argfst': 623,\n",
       " 'one': 624,\n",
       " 'nibanme': 625,\n",
       " 'filter': 626,\n",
       " 'mm': 627,\n",
       " 'escape': 628,\n",
       " 'rema': 629,\n",
       " 'nextvalue': 630,\n",
       " 'largest': 631,\n",
       " 'lf': 632,\n",
       " 'halfw': 633,\n",
       " 'halfh': 634,\n",
       " 'height': 635,\n",
       " 'makey': 636,\n",
       " 'xeven': 637,\n",
       " 'yeven': 638,\n",
       " 'square': 639,\n",
       " 'debug': 640,\n",
       " 'kwargs': 641,\n",
       " 'cenx': 642,\n",
       " 'ceny': 643,\n",
       " 'axis': 644,\n",
       " 'lsq': 645,\n",
       " 'rsq': 646,\n",
       " 'usq': 647,\n",
       " 'dsq': 648,\n",
       " 'items': 649,\n",
       " 'alp': 650,\n",
       " 'mins': 651,\n",
       " 'jp': 652,\n",
       " '三角形': 653,\n",
       " 'の面積': 654,\n",
       " 'separate': 655,\n",
       " 'straight': 656,\n",
       " 'の傾き': 657,\n",
       " 'について': 658,\n",
       " 'siki': 659,\n",
       " 'void': 660,\n",
       " 'scanf': 661,\n",
       " 'stdinput': 662,\n",
       " 'xset': 663,\n",
       " 'getgcm': 664,\n",
       " 'divider': 665,\n",
       " 'euclid': 666,\n",
       " 'ex': 667,\n",
       " 'getresult': 668,\n",
       " 'greatestdivisor': 669,\n",
       " 'twoset': 670,\n",
       " 'static': 671,\n",
       " 'system': 672,\n",
       " 'nextint': 673,\n",
       " 'inputarrline': 674,\n",
       " 'xcd': 675,\n",
       " 'ycd': 676,\n",
       " '÷': 677,\n",
       " 'の約数': 678,\n",
       " 'shu': 679,\n",
       " 'nag': 680,\n",
       " 'cir': 681,\n",
       " '¢': 682,\n",
       " 'pie': 683,\n",
       " 'circum': 684,\n",
       " 'perimeter': 685,\n",
       " 'ensyu': 686,\n",
       " 'mennseki': 687,\n",
       " 'ennsyuu': 688,\n",
       " 'squre': 689,\n",
       " 'are': 690,\n",
       " 'calculate': 691,\n",
       " '‘': 692,\n",
       " 'rx': 693,\n",
       " 'gyaku': 694,\n",
       " 'dst': 695,\n",
       " 'ir': 696,\n",
       " 'root': 697,\n",
       " 'group': 698,\n",
       " 'make': 699,\n",
       " 'inums': 700,\n",
       " 'intin': 701,\n",
       " 'global': 702,\n",
       " 'xylist': 703,\n",
       " 'setdefault': 704,\n",
       " 'nodecount': 705,\n",
       " 'repl': 706,\n",
       " \"'o'\": 707,\n",
       " 'bi': 708,\n",
       " 'mojiretu': 709,\n",
       " 'rev': 710,\n",
       " 'nine': 711,\n",
       " \"'q'\": 712,\n",
       " 'do': 713,\n",
       " 'head': 714,\n",
       " 'foll': 715,\n",
       " 'nint': 716,\n",
       " 'digitnumber': 717,\n",
       " 'eval': 718,\n",
       " 'po': 719,\n",
       " 'xp': 720,\n",
       " 'yp': 721,\n",
       " 'strs': 722,\n",
       " 'jk': 723,\n",
       " 'digitstr': 724,\n",
       " 'wa': 725,\n",
       " 'keta': 726,\n",
       " 'numplusnum': 727,\n",
       " 'bが存在する間実行': 728,\n",
       " '最初のbのインデックスを取得': 729,\n",
       " 'bが先頭でないとき': 730,\n",
       " 'bの': 731,\n",
       " '個前とbを消す': 732,\n",
       " 'bが先頭の時': 733,\n",
       " 'この区切り文字でリストを連結': 734,\n",
       " '答え用意': 735,\n",
       " '末尾に要素追加': 736,\n",
       " '長さ': 737,\n",
       " '指定した位置': 738,\n",
       " '今回は末尾': 739,\n",
       " 'の要素を削除': 740,\n",
       " 'datetime': 741,\n",
       " 'alphabet': 742,\n",
       " 'abcdefghijklmnopqrstuvwxyz': 743,\n",
       " 'unite': 744,\n",
       " 'aはbの累乗数か': 745,\n",
       " 'facs': 746,\n",
       " '~': 747,\n",
       " 'primes': 748,\n",
       " 'prime': 749,\n",
       " 'ifelse': 750,\n",
       " 'factorize': 751,\n",
       " 'hani': 752,\n",
       " 'seifu': 753,\n",
       " 'cina': 754,\n",
       " 'inpln': 755,\n",
       " 'rm': 756,\n",
       " '空のリスト': 757,\n",
       " 'cが': 758,\n",
       " '文字目からaまで': 759,\n",
       " 'bでなければアペンド': 760,\n",
       " 'bであってさらにリストがからでなければpop': 761,\n",
       " 'listをくっつけた文字にできる': 762,\n",
       " 'enter': 763,\n",
       " 'tan': 764,\n",
       " 'asin': 765,\n",
       " 'acos': 766,\n",
       " 'atan': 767,\n",
       " 'degrees': 768,\n",
       " 'replacement': 769,\n",
       " 'バイナリハックイージー': 770,\n",
       " 'lifoqueue': 771,\n",
       " 'cmp': 772,\n",
       " \"'yneos'\": 773,\n",
       " 'mulinputs': 774,\n",
       " 'lineinputs': 775,\n",
       " 'truediv': 776,\n",
       " 'zerodivisionerror': 777,\n",
       " 'sの逆数は求まりません。': 778,\n",
       " 'put': 779,\n",
       " '出力': 780,\n",
       " 'eof': 781,\n",
       " '解けた': 782,\n",
       " '入力の文字列': 783,\n",
       " '文字ずつ見てゆく': 784,\n",
       " 'もし': 785,\n",
       " 'か': 786,\n",
       " 'の場合、それを追加': 787,\n",
       " 'もしバックスペースの場合、': 788,\n",
       " '文字削除': 789,\n",
       " 'initialization': 790,\n",
       " 'traverse': 791,\n",
       " 'sl': 792,\n",
       " 'thu': 793,\n",
       " 'sd': 794,\n",
       " 'stringlist': 795,\n",
       " 'intlistlist': 796,\n",
       " 'touppermultiple': 797,\n",
       " 'tolowermultiple': 798,\n",
       " 'nearpow': 799,\n",
       " 'duplicate': 800,\n",
       " 'maxn': 801,\n",
       " 'ones': 802,\n",
       " 'dtype': 803,\n",
       " 'inpur': 804,\n",
       " '最大値のindexを探し、最大値が複数あるか確認': 805,\n",
       " 'maxi': 806,\n",
       " 'signal': 807,\n",
       " '入力全て': 808,\n",
       " 'eofまで': 809,\n",
       " 'を': 810,\n",
       " 'iter': 811,\n",
       " 'に変換': 812,\n",
       " 'rli': 813,\n",
       " 'assert': 814,\n",
       " 'findex': 815,\n",
       " 'iiou': 816,\n",
       " 'nlargest': 817,\n",
       " '横にラインを引く': 818,\n",
       " 'pirnt': 819,\n",
       " '\\u3000or': 820,\n",
       " '長方形を分割するためには長方形の中心を通る必要がある': 821,\n",
       " 'ただし、指定された座標が中心の場合は無限に直線が弾ける': 822,\n",
       " '長方形内であれば、中心からの対象点を選ぶ事により、': 823,\n",
       " '分割できる': 824,\n",
       " '長方形の中心を通れば、': 825,\n",
       " '分割できる。そうじゃないとだめ': 826,\n",
       " 'quit': 827,\n",
       " '真ん中を通れば必ず二分できてこれが最大値だが、自分自身が真ん中の座標の場合は自分自信を通るどんな直線も二分できる': 828,\n",
       " '対角線': 829,\n",
       " '点': 830,\n",
       " 'でつoo': 831,\n",
       " 'you': 832,\n",
       " 'play': 833,\n",
       " 'cards': 834,\n",
       " \"you're\": 835,\n",
       " 'dealt': 836,\n",
       " 'nn': 837,\n",
       " 'tue': 838,\n",
       " 'may': 839,\n",
       " 'shinba': 840,\n",
       " '直線上に存在すれば三角形の面積': 841,\n",
       " '直線上にない時': 842,\n",
       " 'sqs': 843,\n",
       " 'ignore': 844,\n",
       " 'trailing': 845,\n",
       " 'spaces': 846,\n",
       " 'dprint': 847,\n",
       " 'readints': 848,\n",
       " 'ncr': 849,\n",
       " 'duplicates': 850,\n",
       " 'coordinates': 851,\n",
       " 'any': 852,\n",
       " 'equal': 853,\n",
       " '再帰関数の上限': 854,\n",
       " '以上の場合python': 855,\n",
       " 'dcp': 856,\n",
       " '分探索': 857,\n",
       " 'aはソート済みである必要あり。aの中からx未満の要素数を返す。rightだと以下': 858,\n",
       " 'appendleft': 859,\n",
       " 'rotate': 860,\n",
       " 'で': 861,\n",
       " '→': 862,\n",
       " 'にn回ローテート': 863,\n",
       " '文字列を個数カウント辞書に、': 864,\n",
       " '累積和': 865,\n",
       " 'としないこと、返り値はnone': 866,\n",
       " '古いatcoderコンテストの場合gcdなどはここからimportする': 867,\n",
       " 'pypyでもうごく': 868,\n",
       " 'maxsizeは保存するデータ数の最大値、': 869,\n",
       " 'nが最も高効率': 870,\n",
       " 'printe': 871,\n",
       " 'file': 872,\n",
       " 'stderr': 873,\n",
       " 'printl': 874,\n",
       " 'matmat': 875,\n",
       " 'matvec': 876,\n",
       " '二個目の要素で降順並び替え': 877,\n",
       " '行ベクトル': 878,\n",
       " '改行ベクトル': 879,\n",
       " '改行行列': 880,\n",
       " '問題：https': 881,\n",
       " 'contests': 882,\n",
       " 'tasks': 883,\n",
       " '長方形の面積': 884,\n",
       " \"'wの面積'\": 885,\n",
       " \"'hの面積'\": 886,\n",
       " \"'wの長方形'\": 887,\n",
       " \"'hの長方形'\": 888,\n",
       " \"'w\": 889,\n",
       " \"h'\": 890,\n",
       " 'maxが複数あるか': 891,\n",
       " 'inside': 892,\n",
       " '初項s': 893,\n",
       " '交差dのn個の数列の和': 894,\n",
       " 'arithmetic': 895,\n",
       " 'progression': 896,\n",
       " 'aとbの最大公約数': 897,\n",
       " 'aとbの最小公倍数': 898,\n",
       " 'xyplus': 899,\n",
       " '重心に来る時は複数、線上と内部は': 900,\n",
       " 'こ': 901,\n",
       " '線上にある時': 902,\n",
       " '等分可能': 903,\n",
       " '縦横で分割する': 904,\n",
       " '縦': 905,\n",
       " '横': 906,\n",
       " 'xの方が大きくなくてはならない': 907,\n",
       " 'future': 908,\n",
       " 'function': 909,\n",
       " 'io': 910,\n",
       " 'start': 911,\n",
       " 'clock': 912,\n",
       " 'stdio': 913,\n",
       " 'printf': 914,\n",
       " '計算量問題あり': 915,\n",
       " 'version': 916,\n",
       " 'minor': 917,\n",
       " '小林勇希': 918,\n",
       " 'mb': 919,\n",
       " 'alds': 920,\n",
       " 'java': 921,\n",
       " 'util': 922,\n",
       " 'close': 923,\n",
       " 'println': 924,\n",
       " 'はswap': 925,\n",
       " '値の交換': 926,\n",
       " 'を表す': 927,\n",
       " 'ユークリッドの互除法により最大公約数': 928,\n",
       " 'greatestcommondivisorを求める': 929,\n",
       " '以下のようなアルゴリズムはn回の割り算を行う必要があるため、大きな数に対しては時間内に出力を得ることはできない': 930,\n",
       " 'downto': 931,\n",
       " 'がともに': 932,\n",
       " 'で割り切れる': 933,\n",
       " 'ユークリッドの互除法による最大公約数の判定': 934,\n",
       " 'ユークリッドの互除法': 935,\n",
       " 'bq': 936,\n",
       " '一行に複数入力': 937,\n",
       " '標準入力を行う': 938,\n",
       " 'printarrline': 939,\n",
       " 'todo': 940,\n",
       " '最初に確認のswap': 941,\n",
       " '毎回入れ替えるawap': 942,\n",
       " '§': 943,\n",
       " '°': 944,\n",
       " 'xの約数': 945,\n",
       " 'yの約数': 946,\n",
       " 'ｘとyの公約数': 947,\n",
       " 'ｘとyの最大公約数': 948,\n",
       " '２つの整数': 949,\n",
       " 'について、x': 950,\n",
       " 'の余りがともに': 951,\n",
       " 'となる': 952,\n",
       " 'のうち最大のものを、x': 953,\n",
       " 'の最大公約数（greatest': 954,\n",
       " 'divisor）と言います。': 955,\n",
       " '例えば、': 956,\n",
       " 'の最大公約数': 957,\n",
       " 'は': 958,\n",
       " 'となります。これは、': 959,\n",
       " '、': 960,\n",
       " 'の公約数': 961,\n",
       " 'の最大値となります。': 962,\n",
       " 'froat': 963,\n",
       " '円周率': 964,\n",
       " '浮動小数点数で読む': 965,\n",
       " 'warnings': 966,\n",
       " 'urllib': 967,\n",
       " '面積': 968,\n",
       " '円周': 969,\n",
       " \"lf'\": 970,\n",
       " 'sprit': 971,\n",
       " 'inport': 972,\n",
       " 'iostream': 973,\n",
       " 'iomanip': 974,\n",
       " 'using': 975,\n",
       " 'namespace': 976,\n",
       " 'std': 977,\n",
       " 'const': 978,\n",
       " 'cout': 979,\n",
       " 'fixed': 980,\n",
       " 'setprecision': 981,\n",
       " 'endl': 982,\n",
       " 'dev': 983,\n",
       " 'null': 984,\n",
       " 'itp': 985,\n",
       " 'sute': 986,\n",
       " 'iterable': 987,\n",
       " \"print'\": 988,\n",
       " 'が': 989,\n",
       " 'より大きいとき、つまり最初の要素ではないとき空白を出力': 990,\n",
       " '改行を出力': 991,\n",
       " \"'format\": 992,\n",
       " 'reveres': 993,\n",
       " 'はリストを引数に展開': 994,\n",
       " 'xrange': 995,\n",
       " 'cording': 996,\n",
       " '数列の反転': 997,\n",
       " 'スペースがない数字リストを読み込み': 998,\n",
       " '優先度付きキュー': 999,\n",
       " '最小値取り出し': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tokenizer.json') as f:\n",
    "  json_string = json.load(f)\n",
    "\n",
    "tokenizer= keras.preprocessing.text.tokenizer_from_json(\n",
    "    json_string\n",
    ")\n",
    "\n",
    "print(len(tokenizer.word_index))\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cda142a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'s': 1299,\n",
       " '=': 9746,\n",
       " '[': 2443,\n",
       " ']': 2443,\n",
       " '\\n': 15645,\n",
       " 'a': 2465,\n",
       " 'int': 1683,\n",
       " '(': 12350,\n",
       " 'input': 2055,\n",
       " ')': 12346,\n",
       " 'if': 1506,\n",
       " '0': 2238,\n",
       " ':': 4840,\n",
       " '.': 3315,\n",
       " 'append': 360,\n",
       " 'elif': 343,\n",
       " '1': 3004,\n",
       " 'else': 768,\n",
       " 'len': 429,\n",
       " '>': 309,\n",
       " 'pop': 100,\n",
       " 'for': 1222,\n",
       " 'l': 413,\n",
       " 'in': 1259,\n",
       " 'print': 2160,\n",
       " 'list': 581,\n",
       " 'map': 679,\n",
       " 'str': 388,\n",
       " ',': 3835,\n",
       " 'split': 880,\n",
       " 'ans': 1011,\n",
       " 'i': 2213,\n",
       " '\"': 2448,\n",
       " 'b': 1183,\n",
       " 'del': 30,\n",
       " '-': 1206,\n",
       " 'join': 210,\n",
       " 'range': 720,\n",
       " \"'\": 1585,\n",
       " \"'b'\": 105,\n",
       " '!': 275,\n",
       " 'remove': 18,\n",
       " \"''\": 210,\n",
       " 'is': 28,\n",
       " '+': 923,\n",
       " 'and': 358,\n",
       " 'while': 267,\n",
       " '#': 678,\n",
       " 'bが存在する間実行': 1,\n",
       " 'index': 54,\n",
       " '最初のbのインデックスを取得': 1,\n",
       " 'bが先頭でないとき': 1,\n",
       " 'bの': 1,\n",
       " '個前とbを消す': 1,\n",
       " 'bが先頭の時': 1,\n",
       " 'この区切り文字でリストを連結': 1,\n",
       " '*': 1962,\n",
       " 'q': 28,\n",
       " 'chr': 3,\n",
       " 'x': 2298,\n",
       " '8': 178,\n",
       " 'sep': 19,\n",
       " 'result': 83,\n",
       " 'raw': 129,\n",
       " '_': 2509,\n",
       " 'out': 38,\n",
       " 'import': 546,\n",
       " 'sys': 320,\n",
       " 'stdin': 173,\n",
       " 'ni': 3,\n",
       " 'lambda': 59,\n",
       " 'ns': 19,\n",
       " 'na': 10,\n",
       " 'readline': 97,\n",
       " 'or': 145,\n",
       " 'sentence': 7,\n",
       " 'continue': 32,\n",
       " 't': 158,\n",
       " 'coding': 74,\n",
       " 'utf': 77,\n",
       " '文字列の入力': 2,\n",
       " '\\t': 549,\n",
       " 'from': 145,\n",
       " 'collections': 44,\n",
       " 'deque': 31,\n",
       " 'd': 169,\n",
       " '&': 17,\n",
       " 'n': 1731,\n",
       " 'c': 306,\n",
       " 'ln': 6,\n",
       " '答え用意': 1,\n",
       " '末尾に要素追加': 1,\n",
       " '長さ': 1,\n",
       " '指定した位置': 1,\n",
       " '今回は末尾': 1,\n",
       " 'の要素を削除': 1,\n",
       " 'p': 81,\n",
       " '<': 254,\n",
       " 'arr': 24,\n",
       " 'new': 44,\n",
       " 'def': 361,\n",
       " 'typing': 2,\n",
       " 're': 15,\n",
       " 'text': 26,\n",
       " 'length': 33,\n",
       " 'sub': 11,\n",
       " '|': 5,\n",
       " '?': 155,\n",
       " \"b'\": 10,\n",
       " 'try': 84,\n",
       " 'except': 86,\n",
       " 'indexerror': 2,\n",
       " 'li': 60,\n",
       " 'sz': 4,\n",
       " 'end': 105,\n",
       " 'ss': 19,\n",
       " 'flg': 13,\n",
       " 'reverse': 137,\n",
       " 'r': 860,\n",
       " 'e': 22,\n",
       " 'orders': 2,\n",
       " 'display': 15,\n",
       " 'order': 4,\n",
       " 'pass': 23,\n",
       " 'setrecursionlimit': 14,\n",
       " '9': 596,\n",
       " 'inf': 13,\n",
       " 'mod': 57,\n",
       " '7': 67,\n",
       " 'rstrip': 40,\n",
       " 'yesno': 2,\n",
       " 'bool': 7,\n",
       " \"'yes'\": 4,\n",
       " \"'no'\": 4,\n",
       " 'main': 214,\n",
       " 'name': 82,\n",
       " 'math': 504,\n",
       " 'defaultdict': 10,\n",
       " 'itertools': 19,\n",
       " 'copy': 68,\n",
       " 'deepcopy': 13,\n",
       " 'random': 2,\n",
       " 'heapq': 14,\n",
       " 'heappop': 8,\n",
       " 'heappush': 6,\n",
       " 'time': 10,\n",
       " 'os': 6,\n",
       " 'queue': 6,\n",
       " 'datetime': 1,\n",
       " 'functools': 11,\n",
       " 'lru': 7,\n",
       " 'cache': 7,\n",
       " '@': 6,\n",
       " 'maxsize': 2,\n",
       " 'none': 14,\n",
       " '2': 1587,\n",
       " 'numpy': 12,\n",
       " 'as': 31,\n",
       " 'np': 36,\n",
       " 'alphabet': 1,\n",
       " 'abcdefghijklmnopqrstuvwxyz': 1,\n",
       " 'yn': 4,\n",
       " 'yes': 11,\n",
       " 'no': 11,\n",
       " 'class': 23,\n",
       " 'union': 3,\n",
       " 'find': 18,\n",
       " 'init': 5,\n",
       " 'self': 135,\n",
       " 'rank': 6,\n",
       " 'return': 351,\n",
       " 'same': 2,\n",
       " 'y': 1904,\n",
       " 'link': 2,\n",
       " 'unite': 1,\n",
       " 'size': 12,\n",
       " 'set': 32,\n",
       " 'add': 12,\n",
       " 'power': 2,\n",
       " 'aはbの累乗数か': 1,\n",
       " 'now': 4,\n",
       " 'true': 158,\n",
       " 'false': 32,\n",
       " 'bin': 25,\n",
       " 'num': 215,\n",
       " 'get': 10,\n",
       " 'facs': 1,\n",
       " '%': 565,\n",
       " 'comb': 3,\n",
       " 'fac': 4,\n",
       " 'pow': 9,\n",
       " 'next': 8,\n",
       " 'z': 68,\n",
       " '~': 1,\n",
       " '/': 872,\n",
       " 'primes': 1,\n",
       " 'type': 16,\n",
       " 'prime': 1,\n",
       " 'ifelse': 1,\n",
       " 'factorize': 1,\n",
       " 'dict': 4,\n",
       " 'dic': 12,\n",
       " '{': 299,\n",
       " '}': 299,\n",
       " 'floor': 11,\n",
       " 'ceil': 11,\n",
       " 'hani': 1,\n",
       " 'min': 226,\n",
       " 'max': 702,\n",
       " 'ret': 34,\n",
       " 'seifu': 1,\n",
       " 'abs': 20,\n",
       " 'cin': 6,\n",
       " 'cino': 2,\n",
       " 'test': 12,\n",
       " 'not': 33,\n",
       " 'cina': 1,\n",
       " 'f': 292,\n",
       " 'ch': 7,\n",
       " 'k': 111,\n",
       " 'm': 275,\n",
       " 'string': 40,\n",
       " 'counter': 25,\n",
       " 'bisect': 25,\n",
       " 'inp': 18,\n",
       " 'inpl': 5,\n",
       " 'inpln': 1,\n",
       " 'popleft': 6,\n",
       " 'stack': 14,\n",
       " 'replace': 142,\n",
       " '\\\\': 32,\n",
       " 'intl': 2,\n",
       " 'lx': 4,\n",
       " 'tuple': 14,\n",
       " 'moji': 12,\n",
       " 'word': 26,\n",
       " 'data': 69,\n",
       " 'buffer': 6,\n",
       " 'float': 224,\n",
       " \"'inf'\": 6,\n",
       " 'resolve': 4,\n",
       " 'res': 68,\n",
       " 'reduce': 7,\n",
       " 'rr': 2,\n",
       " 'rs': 2,\n",
       " 'ri': 5,\n",
       " 'rm': 1,\n",
       " 'rl': 4,\n",
       " 'si': 3,\n",
       " 'j': 128,\n",
       " 'key': 37,\n",
       " 'usr': 24,\n",
       " 'env': 20,\n",
       " 'python': 25,\n",
       " '3': 306,\n",
       " 'reslut': 5,\n",
       " '入力': 2,\n",
       " '空のリスト': 1,\n",
       " 'cが': 1,\n",
       " '文字目からaまで': 1,\n",
       " 'bでなければアペンド': 1,\n",
       " 'bであってさらにリストがからでなければpop': 1,\n",
       " 'listをくっつけた文字にできる': 1,\n",
       " 'solve': 17,\n",
       " 'cha': 2,\n",
       " 'plist': 7,\n",
       " 'format': 164,\n",
       " 'answer': 33,\n",
       " 'honmono': 2,\n",
       " 'keys': 5,\n",
       " 'inputs': 16,\n",
       " 'enter': 1,\n",
       " 'cnt': 29,\n",
       " '5': 302,\n",
       " 'w': 1050,\n",
       " 'first': 25,\n",
       " 'total': 8,\n",
       " 'row': 7,\n",
       " '^': 6,\n",
       " 'sqrt': 8,\n",
       " 'hypot': 3,\n",
       " 'factorial': 6,\n",
       " 'pi': 381,\n",
       " 'sin': 3,\n",
       " 'cos': 3,\n",
       " 'tan': 1,\n",
       " 'asin': 1,\n",
       " 'acos': 1,\n",
       " 'atan': 1,\n",
       " 'radians': 3,\n",
       " 'degrees': 1,\n",
       " 'log': 53,\n",
       " 'accumulate': 7,\n",
       " 'permutations': 7,\n",
       " 'combinations': 8,\n",
       " 'with': 2,\n",
       " 'replacement': 1,\n",
       " 'product': 6,\n",
       " 'groupby': 2,\n",
       " 'operator': 7,\n",
       " 'itemgetter': 8,\n",
       " 'mul': 7,\n",
       " 'ascii': 9,\n",
       " 'lowercase': 5,\n",
       " 'uppercase': 4,\n",
       " 'digits': 7,\n",
       " 'left': 9,\n",
       " 'fractions': 15,\n",
       " 'gcd': 271,\n",
       " 'strip': 29,\n",
       " 'zip': 2,\n",
       " 'exit': 22,\n",
       " 'バイナリハックイージー': 1,\n",
       " 'lst': 70,\n",
       " 'lifoqueue': 1,\n",
       " 'cmp': 1,\n",
       " 'to': 8,\n",
       " \"'yneos'\": 1,\n",
       " 'judge': 2,\n",
       " 'lcm': 7,\n",
       " 'intinput': 2,\n",
       " 'mulinputs': 1,\n",
       " 'lineinputs': 1,\n",
       " 'func': 5,\n",
       " 'datas': 3,\n",
       " 'eoferror': 44,\n",
       " 'break': 124,\n",
       " 'modint': 11,\n",
       " 'other': 35,\n",
       " 'raise': 8,\n",
       " 'exception': 10,\n",
       " 'truediv': 1,\n",
       " 'modinv': 3,\n",
       " 'zerodivisionerror': 1,\n",
       " 'sの逆数は求まりません。': 1,\n",
       " 'u': 27,\n",
       " 'v': 89,\n",
       " ';': 53,\n",
       " 'stk': 6,\n",
       " 'empty': 2,\n",
       " 'put': 1,\n",
       " '出力': 1,\n",
       " 'eof': 1,\n",
       " '解けた': 1,\n",
       " '入力の文字列': 1,\n",
       " '文字ずつ見てゆく': 1,\n",
       " 'もし': 1,\n",
       " 'か': 1,\n",
       " 'の場合、それを追加': 1,\n",
       " 'もしバックスペースの場合、': 1,\n",
       " '文字削除': 1,\n",
       " 'enumerate': 16,\n",
       " 'convert': 2,\n",
       " 'initialization': 1,\n",
       " 'of': 6,\n",
       " 'traverse': 1,\n",
       " 'the': 2,\n",
       " 'nl': 15,\n",
       " 'sl': 1,\n",
       " 'created': 2,\n",
       " 'on': 10,\n",
       " 'thu': 1,\n",
       " 'jun': 3,\n",
       " '4': 140,\n",
       " 'author': 3,\n",
       " 'sd': 1,\n",
       " '6': 240,\n",
       " 'count': 69,\n",
       " 'st': 8,\n",
       " 'value': 24,\n",
       " 'second': 59,\n",
       " 'sort': 58,\n",
       " 'mapint': 2,\n",
       " 'stringlist': 1,\n",
       " 'intlistlist': 1,\n",
       " 'intcolslist': 2,\n",
       " 'roundup': 2,\n",
       " 'touppermultiple': 1,\n",
       " 'tolowermultiple': 1,\n",
       " 'nearpow': 1,\n",
       " 'sorted': 103,\n",
       " 'tmp': 126,\n",
       " 'sa': 8,\n",
       " 'firsta': 4,\n",
       " 'seconda': 3,\n",
       " 'cont': 2,\n",
       " 'insert': 12,\n",
       " 'duplicate': 1,\n",
       " 'sec': 11,\n",
       " 'idx': 35,\n",
       " 'indexed': 4,\n",
       " 'reversed': 36,\n",
       " 'maxn': 1,\n",
       " 'array': 15,\n",
       " 'all': 14,\n",
       " 'ind': 13,\n",
       " 'argmax': 4,\n",
       " 'ones': 1,\n",
       " 'dtype': 1,\n",
       " 'bk': 3,\n",
       " 'cp': 3,\n",
       " 'alist': 21,\n",
       " 'nums': 59,\n",
       " 'kou': 4,\n",
       " 'val': 32,\n",
       " 'seq': 19,\n",
       " 'submit': 2,\n",
       " 'read': 6,\n",
       " 'inpur': 1,\n",
       " 'lis': 7,\n",
       " 'info': 7,\n",
       " 'maxindex': 9,\n",
       " 'lmax': 4,\n",
       " 'rmax': 4,\n",
       " 'aa': 26,\n",
       " '最大値のindexを探し、最大値が複数あるか確認': 1,\n",
       " 'id': 4,\n",
       " 'dash': 2,\n",
       " 'pr': 4,\n",
       " 'fi': 3,\n",
       " 'se': 2,\n",
       " 'heapify': 4,\n",
       " 'lists': 8,\n",
       " 'newlist': 4,\n",
       " 'mx': 4,\n",
       " 'itr': 3,\n",
       " 'number': 18,\n",
       " 'flag': 50,\n",
       " 'maxim': 4,\n",
       " 'two': 8,\n",
       " 'forward': 8,\n",
       " 'backward': 8,\n",
       " 'maxi': 1,\n",
       " 'nd': 2,\n",
       " 'maxa': 12,\n",
       " 'maxcnt': 4,\n",
       " 'maxidx': 4,\n",
       " 'seta': 2,\n",
       " 'tempa': 2,\n",
       " 'signal': 1,\n",
       " 'nf': 2,\n",
       " '入力全て': 1,\n",
       " 'eofまで': 1,\n",
       " 'を': 1,\n",
       " '`': 2,\n",
       " 'iter': 1,\n",
       " 'に変換': 1,\n",
       " 'open': 3,\n",
       " 'al': 10,\n",
       " 'ma': 36,\n",
       " 'fst': 8,\n",
       " 'snd': 7,\n",
       " 'argfst': 2,\n",
       " 'instas': 3,\n",
       " 'one': 2,\n",
       " 'ltor': 6,\n",
       " 'rtol': 7,\n",
       " 'pos': 4,\n",
       " 'saidai': 3,\n",
       " 'nibanme': 2,\n",
       " 'amax': 21,\n",
       " 'bs': 5,\n",
       " 'seca': 3,\n",
       " 'delete': 3,\n",
       " 'rli': 1,\n",
       " 'la': 3,\n",
       " 'kk': 4,\n",
       " 'filter': 2,\n",
       " 'apple': 9,\n",
       " 'big': 18,\n",
       " 'orange': 3,\n",
       " 'xx': 4,\n",
       " 'hoge': 12,\n",
       " 'huga': 8,\n",
       " 'buff': 12,\n",
       " 'sum': 87,\n",
       " 'mm': 2,\n",
       " 'ap': 4,\n",
       " 'idxs': 3,\n",
       " 'argsort': 3,\n",
       " 'ac': 4,\n",
       " 'll': 5,\n",
       " 'slove': 4,\n",
       " \"n'\": 10,\n",
       " 'escape': 2,\n",
       " 'compare': 3,\n",
       " 'calc': 11,\n",
       " 'anoa': 4,\n",
       " 'tempmax': 3,\n",
       " 'vmax': 5,\n",
       " 'maxx': 5,\n",
       " 'ruiseki': 8,\n",
       " 'assert': 1,\n",
       " 'rema': 2,\n",
       " 'maxvalue': 6,\n",
       " 'nextvalue': 2,\n",
       " 'item': 11,\n",
       " 'temp': 48,\n",
       " 'numbers': 15,\n",
       " 'nlist': 3,\n",
       " 'anslist': 4,\n",
       " 'fastinput': 3,\n",
       " 'findex': 1,\n",
       " 'ai': 8,\n",
       " 'largest': 2,\n",
       " \"'''\": 10,\n",
       " 'iiou': 1,\n",
       " 'ii': 4,\n",
       " 'nmax': 15,\n",
       " 'arg': 4,\n",
       " 'others': 3,\n",
       " 'your': 10,\n",
       " 'code': 10,\n",
       " 'here': 10,\n",
       " 'target': 3,\n",
       " 'nlargest': 1,\n",
       " 'h': 1048,\n",
       " 'dou': 3,\n",
       " 'sq': 9,\n",
       " 'nearx': 3,\n",
       " 'neary': 3,\n",
       " \"'a'\": 5,\n",
       " 'area': 117,\n",
       " '横にラインを引く': 1,\n",
       " 'tatemax': 8,\n",
       " 'yokomax': 8,\n",
       " 'ansx': 6,\n",
       " 'ansy': 4,\n",
       " 'cent': 12,\n",
       " 'abc': 8,\n",
       " 'pirnt': 1,\n",
       " 'double': 9,\n",
       " 'mem': 8,\n",
       " 'whxy': 9,\n",
       " 'dup': 4,\n",
       " 'pprint': 3,\n",
       " 'lf': 2,\n",
       " 'ls': 10,\n",
       " 'mi': 13,\n",
       " 'halfw': 2,\n",
       " 'halfh': 2,\n",
       " 'areas': 5,\n",
       " 'cond': 3,\n",
       " '\\u3000or': 1,\n",
       " 'decimal': 5,\n",
       " 'center': 8,\n",
       " 'width': 3,\n",
       " 'height': 2,\n",
       " 'menseki': 25,\n",
       " 'ws': 3,\n",
       " 'hs': 5,\n",
       " 'rectangle': 7,\n",
       " 'cutting': 4,\n",
       " '長方形を分割するためには長方形の中心を通る必要がある': 1,\n",
       " 'ただし、指定された座標が中心の場合は無限に直線が弾ける': 1,\n",
       " 'makey': 2,\n",
       " 'only': 5,\n",
       " 'tate': 19,\n",
       " 'yoko': 17,\n",
       " '長方形内であれば、中心からの対象点を選ぶ事により、': 1,\n",
       " '分割できる': 1,\n",
       " '長方形の中心を通れば、': 1,\n",
       " '分割できる。そうじゃないとだめ': 1,\n",
       " 'pat': 11,\n",
       " 'quit': 1,\n",
       " 'dx': 15,\n",
       " 'dy': 20,\n",
       " 'cog': 12,\n",
       " '真ん中を通れば必ず二分できてこれが最大値だが、自分自身が真ん中の座標の場合は自分自信を通るどんな直線も二分できる': 1,\n",
       " 'multi': 14,\n",
       " '対角線': 1,\n",
       " '点': 1,\n",
       " 'bunkatsu': 3,\n",
       " 'atcoder': 4,\n",
       " 'beginner': 3,\n",
       " 'contest': 3,\n",
       " 'men': 11,\n",
       " 'dif': 3,\n",
       " 'でつoo': 1,\n",
       " 'you': 1,\n",
       " 'play': 1,\n",
       " 'cards': 1,\n",
       " \"you're\": 1,\n",
       " 'dealt': 1,\n",
       " \"d'\": 3,\n",
       " 'check': 18,\n",
       " 'nn': 1,\n",
       " 'xeven': 2,\n",
       " 'yeven': 2,\n",
       " 'tue': 1,\n",
       " 'may': 1,\n",
       " 'shinba': 1,\n",
       " 'horizon': 3,\n",
       " 'vertical': 3,\n",
       " '直線上に存在すれば三角形の面積': 1,\n",
       " '直線上にない時': 1,\n",
       " 'right': 6,\n",
       " 'sqs': 1,\n",
       " 'another': 3,\n",
       " 'ignore': 1,\n",
       " 'trailing': 1,\n",
       " 'spaces': 1,\n",
       " 'buf': 6,\n",
       " 'ver': 5,\n",
       " 'hori': 4,\n",
       " 'square': 2,\n",
       " 'sm': 9,\n",
       " 'co': 4,\n",
       " 'debug': 2,\n",
       " 'dprint': 1,\n",
       " 'args': 3,\n",
       " 'kwargs': 2,\n",
       " 'nr': 3,\n",
       " 'point': 3,\n",
       " 'candidate': 6,\n",
       " 'way': 3,\n",
       " 'cenx': 2,\n",
       " 'ceny': 2,\n",
       " 'readints': 1,\n",
       " 'ncr': 1,\n",
       " 'has': 4,\n",
       " 'duplicates': 1,\n",
       " 'seen': 4,\n",
       " 'divisor': 37,\n",
       " 'coordinates': 1,\n",
       " 'harf': 5,\n",
       " 'axis': 2,\n",
       " 'any': 1,\n",
       " 'case': 3,\n",
       " 'lsq': 2,\n",
       " 'rsq': 2,\n",
       " 'wk': 6,\n",
       " 'usq': 2,\n",
       " 'dsq': 2,\n",
       " 'equal': 1,\n",
       " 'output': 9,\n",
       " 'half': 5,\n",
       " '再帰関数の上限': 1,\n",
       " '以上の場合python': 1,\n",
       " 'dcp': 1,\n",
       " '分探索': 1,\n",
       " 'aはソート済みである必要あり。aの中からx未満の要素数を返す。rightだと以下': 1,\n",
       " 'appendleft': 1,\n",
       " 'rotate': 1,\n",
       " 'で': 1,\n",
       " '→': 1,\n",
       " 'にn回ローテート': 1,\n",
       " '文字列を個数カウント辞書に、': 1,\n",
       " 'most': 5,\n",
       " 'common': 21,\n",
       " 'values': 7,\n",
       " 'items': 2,\n",
       " '累積和': 1,\n",
       " 'としないこと、返り値はnone': 1,\n",
       " '古いatcoderコンテストの場合gcdなどはここからimportする': 1,\n",
       " 'pypyでもうごく': 1,\n",
       " 'maxsizeは保存するデータ数の最大値、': 1,\n",
       " 'nが最も高効率': 1,\n",
       " 'printe': 1,\n",
       " 'file': 1,\n",
       " 'stderr': 1,\n",
       " 'printl': 1,\n",
       " 'inds': 4,\n",
       " 'alp': 2,\n",
       " 'cap': 4,\n",
       " 'ord': 4,\n",
       " 'matmat': 1,\n",
       " 'matvec': 1,\n",
       " '二個目の要素で降順並び替え': 1,\n",
       " '行ベクトル': 1,\n",
       " '改行ベクトル': 1,\n",
       " '改行行列': 1,\n",
       " 'full': 9,\n",
       " 'hantei': 3,\n",
       " 'ld': 3,\n",
       " 'lu': 3,\n",
       " 'rd': 3,\n",
       " 'ru': 3,\n",
       " 'mins': 2,\n",
       " '問題：https': 1,\n",
       " 'jp': 2,\n",
       " 'contests': 1,\n",
       " 'tasks': 1,\n",
       " 'points': 3,\n",
       " '三角形': 2,\n",
       " 'の面積': 2,\n",
       " 'wy': 5,\n",
       " 'xh': 5,\n",
       " '長方形の面積': 1,\n",
       " 'extend': 4,\n",
       " \"'wの面積'\": 1,\n",
       " \"'hの面積'\": 1,\n",
       " \"'wの長方形'\": 1,\n",
       " \"'hの長方形'\": 1,\n",
       " \"'w\": 1,\n",
       " \"h'\": 1,\n",
       " 'maxが複数あるか': 1,\n",
       " 'inside': 1,\n",
       " '初項s': 1,\n",
       " '交差dのn個の数列の和': 1,\n",
       " 'arithmetic': 1,\n",
       " 'progression': 1,\n",
       " 'aとbの最大公約数': 1,\n",
       " 'aとbの最小公倍数': 1,\n",
       " 'g': 5,\n",
       " 'xyplus': 1,\n",
       " '重心に来る時は複数、線上と内部は': 1,\n",
       " 'こ': 1,\n",
       " 'sp': 3,\n",
       " 'fraction': 3,\n",
       " 'rectanglecutting': 4,\n",
       " 'lines': 7,\n",
       " 'sv': 6,\n",
       " 'separate': 2,\n",
       " 'straight': 2,\n",
       " 'line': 123,\n",
       " 'の傾き': 2,\n",
       " '線上にある時': 1,\n",
       " '等分可能': 1,\n",
       " 'について': 2,\n",
       " '縦横で分割する': 1,\n",
       " '縦': 1,\n",
       " '横': 1,\n",
       " 'sh': 4,\n",
       " 'dbg': 4,\n",
       " 'kb': 3,\n",
       " 'bytes': 3,\n",
       " 'divide': 41,\n",
       " 'ext': 3,\n",
       " 'xの方が大きくなくてはならない': 1,\n",
       " 'future': 1,\n",
       " 'function': 1,\n",
       " 'io': 1,\n",
       " 'start': 1,\n",
       " 'clock': 1,\n",
       " 'siki': 2,\n",
       " 'include': 3,\n",
       " 'stdio': 1,\n",
       " 'void': 2,\n",
       " 'scanf': 2,\n",
       " 'printf': 1,\n",
       " 'stdinput': 2,\n",
       " 'xset': 2,\n",
       " 'yset': 4,\n",
       " 'matched': 4,\n",
       " 'cd': 6,\n",
       " 'intersection': 4,\n",
       " 'small': 9,\n",
       " 'herf': 4,\n",
       " 'round': 29,\n",
       " 'converter': 6,\n",
       " 'larger': 3,\n",
       " 'smaller': 4,\n",
       " 'euclidean': 3,\n",
       " 'gcm': 4,\n",
       " 'getgcm': 2,\n",
       " 'xy': 15,\n",
       " 'greatest': 12,\n",
       " 'divider': 2,\n",
       " 'long': 4,\n",
       " 'euclid': 2,\n",
       " '計算量問題あり': 1,\n",
       " 'lg': 6,\n",
       " 'div': 7,\n",
       " 'mygcd': 3,\n",
       " 'version': 1,\n",
       " 'minor': 1,\n",
       " 'bb': 8,\n",
       " '小林勇希': 1,\n",
       " 'mb': 1,\n",
       " 'ex': 2,\n",
       " 'getresult': 2,\n",
       " 'gc': 4,\n",
       " 'greatestdivisor': 2,\n",
       " 'twoset': 2,\n",
       " 'alds': 1,\n",
       " 'java': 1,\n",
       " 'util': 1,\n",
       " 'scanner': 7,\n",
       " 'public': 3,\n",
       " 'static': 2,\n",
       " 'system': 2,\n",
       " 'nextint': 2,\n",
       " 'close': 1,\n",
       " 'println': 1,\n",
       " 'helper': 3,\n",
       " 'zzz': 16,\n",
       " 'maximum': 4,\n",
       " 'はswap': 1,\n",
       " '値の交換': 1,\n",
       " 'を表す': 1,\n",
       " 'ユークリッドの互除法により最大公約数': 1,\n",
       " 'greatestcommondivisorを求める': 1,\n",
       " '以下のようなアルゴリズムはn回の割り算を行う必要があるため、大きな数に対しては時間内に出力を得ることはできない': 1,\n",
       " 'downto': 1,\n",
       " 'と': 5,\n",
       " 'がともに': 1,\n",
       " 'で割り切れる': 1,\n",
       " 'ユークリッドの互除法による最大公約数の判定': 1,\n",
       " 'swap': 3,\n",
       " 'ユークリッドの互除法': 1,\n",
       " 'bq': 1,\n",
       " '一行に複数入力': 1,\n",
       " '標準入力を行う': 1,\n",
       " 'printarrline': 1,\n",
       " 'inputarrline': 2,\n",
       " 'ar': 3,\n",
       " 'natural': 4,\n",
       " 'slist': 4,\n",
       " 'euclideanalg': 3,\n",
       " 'yy': 6,\n",
       " 'todo': 1,\n",
       " '最初に確認のswap': 1,\n",
       " '毎回入れ替えるawap': 1,\n",
       " '¨': 4,\n",
       " '§': 1,\n",
       " '°': 1,\n",
       " 'xcd': 2,\n",
       " 'xの約数': 1,\n",
       " 'ycd': 2,\n",
       " 'yの約数': 1,\n",
       " 'ｘとyの公約数': 1,\n",
       " 'ｘとyの最大公約数': 1,\n",
       " '２つの整数': 1,\n",
       " 'について、x': 1,\n",
       " '÷': 2,\n",
       " 'の余りがともに': 1,\n",
       " 'となる': 1,\n",
       " 'のうち最大のものを、x': 1,\n",
       " 'の最大公約数（greatest': 1,\n",
       " 'divisor）と言います。': 1,\n",
       " '例えば、': 1,\n",
       " 'の最大公約数': 1,\n",
       " 'は': 1,\n",
       " 'となります。これは、': 1,\n",
       " 'の約数': 2,\n",
       " '、': 1,\n",
       " 'の公約数': 1,\n",
       " 'の最大値となります。': 1,\n",
       " \"f'\": 36,\n",
       " 'shu': 2,\n",
       " 'my': 3,\n",
       " 'froat': 1,\n",
       " 'nag': 2,\n",
       " '円周率': 1,\n",
       " '浮動小数点数で読む': 1,\n",
       " 'cir': 2,\n",
       " '¢': 2,\n",
       " 'pie': 2,\n",
       " 'circumference': 12,\n",
       " 'circle': 34,\n",
       " 'circum': 2,\n",
       " 'sen': 4,\n",
       " 'warnings': 1,\n",
       " 'urllib': 1,\n",
       " 'radius': 9,\n",
       " 'perimeter': 2,\n",
       " 'π': 3,\n",
       " 'ensyu': 2,\n",
       " 'en': 3,\n",
       " 'py': 4,\n",
       " '面積': 1,\n",
       " '円周': 1,\n",
       " 'mennseki': 2,\n",
       " 'ennsyuu': 2,\n",
       " 'squre': 2,\n",
       " \"lf'\": 1,\n",
       " 'are': 2,\n",
       " 'sprit': 1,\n",
       " 'calculate': 2,\n",
       " 'around': 4,\n",
       " 'inport': 1,\n",
       " 'pai': 3,\n",
       " 'iostream': 1,\n",
       " 'iomanip': 1,\n",
       " 'using': 1,\n",
       " 'namespace': 1,\n",
       " 'std': 1,\n",
       " 'const': 1,\n",
       " 'cout': 1,\n",
       " 'fixed': 1,\n",
       " 'setprecision': 1,\n",
       " 'endl': 1,\n",
       " 'dev': 1,\n",
       " 'null': 1,\n",
       " 'stdout': 9,\n",
       " 'write': 9,\n",
       " 'itp': 1,\n",
       " 'o': 17,\n",
       " 'table': 18,\n",
       " 'sute': 1,\n",
       " '‘': 2,\n",
       " 'iterable': 1,\n",
       " 'numlist': 3,\n",
       " 'cn': 4,\n",
       " \"print'\": 1,\n",
       " 'elem': 4,\n",
       " 'が': 1,\n",
       " 'より大きいとき、つまり最初の要素ではないとき空白を出力': 1,\n",
       " '改行を出力': 1,\n",
       " \"'format\": 1,\n",
       " 'reveres': 1,\n",
       " 'an': 5,\n",
       " 'はリストを引数に展開': 1,\n",
       " 'xrange': 1,\n",
       " 'cording': 1,\n",
       " 'date': 3,\n",
       " 'rx': 2,\n",
       " '数列の反転': 1,\n",
       " 'iline': 3,\n",
       " 'ilinei': 4,\n",
       " 'it': 3,\n",
       " 'gyaku': 2,\n",
       " 'translate': 8,\n",
       " 'maketrans': 6,\n",
       " 'dst': 2,\n",
       " 'sam': 6,\n",
       " 'counts': 6,\n",
       " 'スペースがない数字リストを読み込み': 1,\n",
       " '優先度付きキュー': 1,\n",
       " '最小値取り出し': 1,\n",
       " 'sr': 3,\n",
       " 'ir': 2,\n",
       " 'lr': 1,\n",
       " 'nを素因数分解': 1,\n",
       " '以上の整数n': 1,\n",
       " '素因数': 1,\n",
       " '指数': 1,\n",
       " 'の': 1,\n",
       " '次元リスト': 1,\n",
       " 'factorization': 1,\n",
       " 'kaijo': 1,\n",
       " 'unionfind': 1,\n",
       " 'parents': 12,\n",
       " 'members': 4,\n",
       " 'root': 2,\n",
       " 'roots': 4,\n",
       " 'group': 2,\n",
       " '約数生成': 1,\n",
       " 'make': 2,\n",
       " 'divisors': 7,\n",
       " 'inums': 2,\n",
       " 'digit': 28,\n",
       " 'binl': 1,\n",
       " 'intin': 2,\n",
       " 'intina': 1,\n",
       " 'intinl': 1,\n",
       " 'modadd': 1,\n",
       " 'global': 2,\n",
       " 'modmlt': 1,\n",
       " 'retlist': 4,\n",
       " 'linklist': 13,\n",
       " 'xylist': 2,\n",
       " 'setdefault': 2,\n",
       " 'longest': 3,\n",
       " 'distance': 21,\n",
       " 'vlist': 19,\n",
       " 'previous': 3,\n",
       " 'nodecount': 2,\n",
       " 'tree': 1,\n",
       " 'diameter': 1,\n",
       " \"'if\": 1,\n",
       " \"'else\": 1,\n",
       " \"'for\": 1,\n",
       " 'odd': 32,\n",
       " 'even': 32,\n",
       " \"'x'\": 8,\n",
       " 'repl': 2,\n",
       " 'char': 3,\n",
       " \"'o'\": 2,\n",
       " 'jst': 1,\n",
       " 'bi': 2,\n",
       " 'op': 1,\n",
       " 'scipy': 1,\n",
       " 'misc': 1,\n",
       " 'stock': 3,\n",
       " 'sc': 9,\n",
       " 'mojiretu': 2,\n",
       " 'rev': 2,\n",
       " 'nine': 2,\n",
       " \"'q'\": 2,\n",
       " 'nstr': 3,\n",
       " '–': 1,\n",
       " 'intm': 1,\n",
       " 'strm': 1,\n",
       " 'do': 2,\n",
       " 'head': 2,\n",
       " 'foll': 2,\n",
       " 'cf': 4,\n",
       " 'inpsum': 4,\n",
       " 'ij': 3,\n",
       " 'seeds': 3,\n",
       " \"'r'\": 1,\n",
       " \"'l'\": 1,\n",
       " \"'u'\": 1,\n",
       " \"'d'\": 1,\n",
       " 'vin': 1,\n",
       " 'nlis': 1,\n",
       " 'nint': 2,\n",
       " 'part': 6,\n",
       " ...}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bd64cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130364</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130365</th>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130366</th>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130367</th>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130368</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>105</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130369 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1   X2   X3   X4  X5  X6   X7   X8   X9  X10    Y\n",
       "0         0    0    0    0   0   0    0    0    0    0   27\n",
       "1         0    0    0    0   0   0    0    0    0   27    5\n",
       "2         0    0    0    0   0   0    0    0   27    5   45\n",
       "3         0    0    0    0   0   0    0   27    5   45    3\n",
       "4         0    0    0    0   0   0   27    5   45    3    1\n",
       "...     ...  ...  ...  ...  ..  ..  ...  ...  ...  ...  ...\n",
       "130364    5   52    3  105   4   2   18    3   50    3  105\n",
       "130365   52    3  105    4   2  18    3   50    3  105    4\n",
       "130366    3  105    4    2  18   3   50    3  105    4    4\n",
       "130367  105    4    2   18   3  50    3  105    4    4    1\n",
       "130368    4    2   18    3  50   3  105    4    4    1    1\n",
       "\n",
       "[130369 rows x 11 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0   0  27]\n",
      " [  0   0   0 ...   0  27   5]\n",
      " ...\n",
      " [  3 105   4 ...   3 105   4]\n",
      " [105   4   2 ... 105   4   4]\n",
      " [  4   2  18 ...   4   4   1]]\n",
      "[27  5 45 ...  4  1  1]\n"
     ]
    }
   ],
   "source": [
    "# Above can help with large scale traing but lets focus on one file for now\n",
    "dataxy = pd.read_csv('dataxy_10_1_1.csv', header = 0)\n",
    "display(dataxy)\n",
    "dataX = dataxy.iloc[:, 0:10].values\n",
    "dataY = dataxy.iloc[:, 10].values\n",
    "\n",
    "print(dataX)\n",
    "print(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "121dd743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras-metrics\n",
      "  Downloading keras_metrics-1.1.0-py2.py3-none-any.whl (5.6 kB)\n",
      "Requirement already satisfied: Keras>=2.1.5 in /Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages (from keras-metrics) (2.9.0)\n",
      "Installing collected packages: keras-metrics\n",
      "Successfully installed keras-metrics-1.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "30c5d4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataX, dataY, test_size=0.25, random_state= seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8067432e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_metrics as km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f87311d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130369, 10)\n",
      "(130369,)\n",
      "623\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "# Rerun with mini dataset\n",
    "print(dataX.shape)\n",
    "print(dataY.shape)\n",
    "print(len(np.unique(dataY)))\n",
    "# Lets build model\n",
    "vocab_size_to_predict = len(tokenizer.word_index)+1\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(input_dim=vocab_size_to_predict, output_dim=64, mask_zero=True))\n",
    "model1.add(LSTM(256, input_shape=(dataX.shape[1], 1)))\n",
    "model1.add(Dropout(0.01))\n",
    "model1.add(Dense(1028))\n",
    "model1.add(Dropout(0.01))\n",
    "model1.add(Dense(units = vocab_size_to_predict, activation='softmax'))\n",
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "tf.keras.utils.plot_model(model1, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0e6e016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 64)          67136     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               328704    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1028)              264196    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1028)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1049)              1079421   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,739,457\n",
      "Trainable params: 1,739,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaa132e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19bfbee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 2.7674 - accuracy: 0.3729\n",
      "Epoch 1: loss improved from inf to 2.76713, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 45s 83ms/step - loss: 2.7671 - accuracy: 0.3730\n",
      "Epoch 2/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 1.6319 - accuracy: 0.5980\n",
      "Epoch 2: loss improved from 2.76713 to 1.63198, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 47s 91ms/step - loss: 1.6320 - accuracy: 0.5979\n",
      "Epoch 3/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 1.3815 - accuracy: 0.6476\n",
      "Epoch 3: loss improved from 1.63198 to 1.38151, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 48s 95ms/step - loss: 1.3815 - accuracy: 0.6476\n",
      "Epoch 4/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 1.2420 - accuracy: 0.6758\n",
      "Epoch 4: loss improved from 1.38151 to 1.24187, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 50s 97ms/step - loss: 1.2419 - accuracy: 0.6759\n",
      "Epoch 5/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 1.1438 - accuracy: 0.6950\n",
      "Epoch 5: loss improved from 1.24187 to 1.14395, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 51s 100ms/step - loss: 1.1440 - accuracy: 0.6950\n",
      "Epoch 6/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 1.0706 - accuracy: 0.7101\n",
      "Epoch 6: loss improved from 1.14395 to 1.07060, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 51s 100ms/step - loss: 1.0706 - accuracy: 0.7101\n",
      "Epoch 7/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 1.0077 - accuracy: 0.7219\n",
      "Epoch 7: loss improved from 1.07060 to 1.00779, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 52s 102ms/step - loss: 1.0078 - accuracy: 0.7218\n",
      "Epoch 8/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.9535 - accuracy: 0.7323\n",
      "Epoch 8: loss improved from 1.00779 to 0.95338, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 53s 103ms/step - loss: 0.9534 - accuracy: 0.7323\n",
      "Epoch 9/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.9060 - accuracy: 0.7419\n",
      "Epoch 9: loss improved from 0.95338 to 0.90602, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 55s 107ms/step - loss: 0.9060 - accuracy: 0.7419\n",
      "Epoch 10/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.8590 - accuracy: 0.7520\n",
      "Epoch 10: loss improved from 0.90602 to 0.85905, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 55s 108ms/step - loss: 0.8590 - accuracy: 0.7520\n",
      "Epoch 11/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.8144 - accuracy: 0.7634\n",
      "Epoch 11: loss improved from 0.85905 to 0.81437, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 52s 103ms/step - loss: 0.8144 - accuracy: 0.7634\n",
      "Epoch 12/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.7772 - accuracy: 0.7704\n",
      "Epoch 12: loss improved from 0.81437 to 0.77709, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 53s 104ms/step - loss: 0.7771 - accuracy: 0.7705\n",
      "Epoch 13/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.7350 - accuracy: 0.7798\n",
      "Epoch 13: loss improved from 0.77709 to 0.73498, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 53s 104ms/step - loss: 0.7350 - accuracy: 0.7798\n",
      "Epoch 14/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6993 - accuracy: 0.7891\n",
      "Epoch 14: loss improved from 0.73498 to 0.69944, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 105ms/step - loss: 0.6994 - accuracy: 0.7891\n",
      "Epoch 15/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6650 - accuracy: 0.7969\n",
      "Epoch 15: loss improved from 0.69944 to 0.66512, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 60s 117ms/step - loss: 0.6651 - accuracy: 0.7969\n",
      "Epoch 16/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6313 - accuracy: 0.8064\n",
      "Epoch 16: loss improved from 0.66512 to 0.63135, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 58s 114ms/step - loss: 0.6314 - accuracy: 0.8064\n",
      "Epoch 17/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6023 - accuracy: 0.8139\n",
      "Epoch 17: loss improved from 0.63135 to 0.60221, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 57s 112ms/step - loss: 0.6022 - accuracy: 0.8139\n",
      "Epoch 18/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.5742 - accuracy: 0.8215\n",
      "Epoch 18: loss improved from 0.60221 to 0.57432, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 59s 115ms/step - loss: 0.5743 - accuracy: 0.8214\n",
      "Epoch 19/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.5492 - accuracy: 0.8291\n",
      "Epoch 19: loss improved from 0.57432 to 0.54916, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 56s 109ms/step - loss: 0.5492 - accuracy: 0.8291\n",
      "Epoch 20/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.5274 - accuracy: 0.8343\n",
      "Epoch 20: loss improved from 0.54916 to 0.52747, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 106ms/step - loss: 0.5275 - accuracy: 0.8343\n",
      "Epoch 21/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.5055 - accuracy: 0.8410\n",
      "Epoch 21: loss improved from 0.52747 to 0.50565, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 107ms/step - loss: 0.5056 - accuracy: 0.8410\n",
      "Epoch 22/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.4876 - accuracy: 0.8465\n",
      "Epoch 22: loss improved from 0.50565 to 0.48772, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 106ms/step - loss: 0.4877 - accuracy: 0.8465\n",
      "Epoch 23/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.4686 - accuracy: 0.8529\n",
      "Epoch 23: loss improved from 0.48772 to 0.46866, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 106ms/step - loss: 0.4687 - accuracy: 0.8529\n",
      "Epoch 24/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.4523 - accuracy: 0.8575\n",
      "Epoch 24: loss improved from 0.46866 to 0.45235, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 106ms/step - loss: 0.4524 - accuracy: 0.8575\n",
      "Epoch 25/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.8608\n",
      "Epoch 25: loss improved from 0.45235 to 0.44049, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 106ms/step - loss: 0.4405 - accuracy: 0.8608\n",
      "Epoch 26/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.4296 - accuracy: 0.8647\n",
      "Epoch 26: loss improved from 0.44049 to 0.42976, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 106ms/step - loss: 0.4298 - accuracy: 0.8646\n",
      "Epoch 27/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.4213 - accuracy: 0.8667\n",
      "Epoch 27: loss improved from 0.42976 to 0.42125, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 53s 105ms/step - loss: 0.4212 - accuracy: 0.8666\n",
      "Epoch 28/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.4101 - accuracy: 0.8710\n",
      "Epoch 28: loss improved from 0.42125 to 0.40999, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 53s 104ms/step - loss: 0.4100 - accuracy: 0.8710\n",
      "Epoch 29/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.4019 - accuracy: 0.8730\n",
      "Epoch 29: loss improved from 0.40999 to 0.40188, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 53s 104ms/step - loss: 0.4019 - accuracy: 0.8730\n",
      "Epoch 30/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3944 - accuracy: 0.8758\n",
      "Epoch 30: loss improved from 0.40188 to 0.39448, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 53s 104ms/step - loss: 0.3945 - accuracy: 0.8758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3875 - accuracy: 0.8777\n",
      "Epoch 31: loss improved from 0.39448 to 0.38759, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 52s 103ms/step - loss: 0.3876 - accuracy: 0.8777\n",
      "Epoch 32/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3809 - accuracy: 0.8801\n",
      "Epoch 32: loss improved from 0.38759 to 0.38112, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 105ms/step - loss: 0.3811 - accuracy: 0.8800\n",
      "Epoch 33/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3773 - accuracy: 0.8809\n",
      "Epoch 33: loss improved from 0.38112 to 0.37725, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 105ms/step - loss: 0.3773 - accuracy: 0.8809\n",
      "Epoch 34/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3726 - accuracy: 0.8831\n",
      "Epoch 34: loss improved from 0.37725 to 0.37257, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 53s 104ms/step - loss: 0.3726 - accuracy: 0.8831\n",
      "Epoch 35/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3662 - accuracy: 0.8851\n",
      "Epoch 35: loss improved from 0.37257 to 0.36626, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 56s 109ms/step - loss: 0.3663 - accuracy: 0.8850\n",
      "Epoch 36/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3651 - accuracy: 0.8854\n",
      "Epoch 36: loss improved from 0.36626 to 0.36498, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 59s 116ms/step - loss: 0.3650 - accuracy: 0.8854\n",
      "Epoch 37/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3604 - accuracy: 0.8859\n",
      "Epoch 37: loss improved from 0.36498 to 0.36045, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 55s 108ms/step - loss: 0.3604 - accuracy: 0.8859\n",
      "Epoch 38/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3588 - accuracy: 0.8874\n",
      "Epoch 38: loss improved from 0.36045 to 0.35900, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 107ms/step - loss: 0.3590 - accuracy: 0.8873\n",
      "Epoch 39/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3567 - accuracy: 0.8883\n",
      "Epoch 39: loss improved from 0.35900 to 0.35657, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 105ms/step - loss: 0.3566 - accuracy: 0.8883\n",
      "Epoch 40/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.3484 - accuracy: 0.8898\n",
      "Epoch 40: loss improved from 0.35657 to 0.34855, saving model to LSTM_model_mini.hdf5\n",
      "510/510 [==============================] - 54s 105ms/step - loss: 0.3485 - accuracy: 0.8897\n"
     ]
    }
   ],
   "source": [
    "## 9. Train and save the best model on Mini Data here\n",
    "filepath = \"LSTM_model_mini.hdf5\"\n",
    "checkpoint1 = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    "                             save_best_only=True, mode='min')\n",
    "history1 = model1.fit(dataX, dataY, epochs=40, batch_size=256, callbacks=[checkpoint1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "822858ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(history1.history['accuracy'])\n",
    "# plt.plot(history1.history['loss'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b2a3198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=vocab_size_to_predict, output_dim=64, mask_zero=True))\n",
    "model2.add(SimpleRNN(256, input_shape=(dataX.shape[1], 1)))\n",
    "model1.add(Dropout(0.01))\n",
    "model1.add(Dense(1028))\n",
    "model1.add(Dropout(0.01))\n",
    "model2.add(Dense(units = vocab_size_to_predict, activation='softmax'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "tf.keras.utils.plot_model(model2, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "35ce209a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_21 (Embedding)    (None, None, 64)          29568     \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 256)               82176     \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 462)               118734    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 230,478\n",
      "Trainable params: 230,478\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6639323c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 2.7328 - accuracy: 0.3899\n",
      "Epoch 1: loss improved from inf to 2.73276, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 10s 19ms/step - loss: 2.7328 - accuracy: 0.3899\n",
      "Epoch 2/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 1.6588 - accuracy: 0.5994\n",
      "Epoch 2: loss improved from 2.73276 to 1.65874, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 1.6587 - accuracy: 0.5994\n",
      "Epoch 3/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 1.4197 - accuracy: 0.6442\n",
      "Epoch 3: loss improved from 1.65874 to 1.42005, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 1.4200 - accuracy: 0.6441\n",
      "Epoch 4/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 1.3008 - accuracy: 0.6673\n",
      "Epoch 4: loss improved from 1.42005 to 1.30053, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 1.3005 - accuracy: 0.6674\n",
      "Epoch 5/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 1.2188 - accuracy: 0.6831\n",
      "Epoch 5: loss improved from 1.30053 to 1.21899, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 1.2190 - accuracy: 0.6832\n",
      "Epoch 6/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 1.1597 - accuracy: 0.6954\n",
      "Epoch 6: loss improved from 1.21899 to 1.15976, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 11s 21ms/step - loss: 1.1598 - accuracy: 0.6953\n",
      "Epoch 7/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 1.1089 - accuracy: 0.7053\n",
      "Epoch 7: loss improved from 1.15976 to 1.10890, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 1.1089 - accuracy: 0.7053\n",
      "Epoch 8/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 1.0664 - accuracy: 0.7148\n",
      "Epoch 8: loss improved from 1.10890 to 1.06641, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 1.0664 - accuracy: 0.7148\n",
      "Epoch 9/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 1.0322 - accuracy: 0.7211\n",
      "Epoch 9: loss improved from 1.06641 to 1.03226, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 1.0323 - accuracy: 0.7211\n",
      "Epoch 10/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.9986 - accuracy: 0.7283\n",
      "Epoch 10: loss improved from 1.03226 to 0.99863, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.9986 - accuracy: 0.7282\n",
      "Epoch 11/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.9705 - accuracy: 0.7331\n",
      "Epoch 11: loss improved from 0.99863 to 0.97057, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.9706 - accuracy: 0.7331\n",
      "Epoch 12/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.9439 - accuracy: 0.7383\n",
      "Epoch 12: loss improved from 0.97057 to 0.94418, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.9442 - accuracy: 0.7382\n",
      "Epoch 13/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.9187 - accuracy: 0.7435\n",
      "Epoch 13: loss improved from 0.94418 to 0.91871, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.9187 - accuracy: 0.7435\n",
      "Epoch 14/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.8958 - accuracy: 0.7496\n",
      "Epoch 14: loss improved from 0.91871 to 0.89560, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.8956 - accuracy: 0.7497\n",
      "Epoch 15/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.8729 - accuracy: 0.7539\n",
      "Epoch 15: loss improved from 0.89560 to 0.87291, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.8729 - accuracy: 0.7539\n",
      "Epoch 16/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.8530 - accuracy: 0.7584\n",
      "Epoch 16: loss improved from 0.87291 to 0.85315, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.8531 - accuracy: 0.7583\n",
      "Epoch 17/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.8368 - accuracy: 0.7615\n",
      "Epoch 17: loss improved from 0.85315 to 0.83677, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.8368 - accuracy: 0.7615\n",
      "Epoch 18/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.8177 - accuracy: 0.7661\n",
      "Epoch 18: loss improved from 0.83677 to 0.81788, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.8179 - accuracy: 0.7660\n",
      "Epoch 19/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.8032 - accuracy: 0.7684\n",
      "Epoch 19: loss improved from 0.81788 to 0.80298, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.8030 - accuracy: 0.7684\n",
      "Epoch 20/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.7858 - accuracy: 0.7727\n",
      "Epoch 20: loss improved from 0.80298 to 0.78581, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 11s 22ms/step - loss: 0.7858 - accuracy: 0.7727\n",
      "Epoch 21/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.7716 - accuracy: 0.7757\n",
      "Epoch 21: loss improved from 0.78581 to 0.77164, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.7716 - accuracy: 0.7757\n",
      "Epoch 22/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.7580 - accuracy: 0.7789\n",
      "Epoch 22: loss improved from 0.77164 to 0.75774, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.7577 - accuracy: 0.7791\n",
      "Epoch 23/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.7437 - accuracy: 0.7816\n",
      "Epoch 23: loss improved from 0.75774 to 0.74397, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.7440 - accuracy: 0.7816\n",
      "Epoch 24/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.7323 - accuracy: 0.7840\n",
      "Epoch 24: loss improved from 0.74397 to 0.73243, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.7324 - accuracy: 0.7840\n",
      "Epoch 25/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.7221 - accuracy: 0.7851\n",
      "Epoch 25: loss improved from 0.73243 to 0.72216, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.7222 - accuracy: 0.7850\n",
      "Epoch 26/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.7092 - accuracy: 0.7901\n",
      "Epoch 26: loss improved from 0.72216 to 0.70916, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 23ms/step - loss: 0.7092 - accuracy: 0.7901\n",
      "Epoch 27/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6992 - accuracy: 0.7920\n",
      "Epoch 27: loss improved from 0.70916 to 0.69934, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 26ms/step - loss: 0.6993 - accuracy: 0.7920\n",
      "Epoch 28/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6903 - accuracy: 0.7938\n",
      "Epoch 28: loss improved from 0.69934 to 0.69038, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 15s 29ms/step - loss: 0.6904 - accuracy: 0.7937\n",
      "Epoch 29/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6824 - accuracy: 0.7950\n",
      "Epoch 29: loss improved from 0.69038 to 0.68252, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 16s 31ms/step - loss: 0.6825 - accuracy: 0.7950\n",
      "Epoch 30/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6737 - accuracy: 0.7971\n",
      "Epoch 30: loss improved from 0.68252 to 0.67371, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 14s 28ms/step - loss: 0.6737 - accuracy: 0.7971\n",
      "Epoch 31/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.6655 - accuracy: 0.7998\n",
      "Epoch 31: loss improved from 0.67371 to 0.66555, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 26ms/step - loss: 0.6655 - accuracy: 0.7998\n",
      "Epoch 32/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.6573 - accuracy: 0.8016\n",
      "Epoch 32: loss improved from 0.66555 to 0.65721, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.6572 - accuracy: 0.8017\n",
      "Epoch 33/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.6499 - accuracy: 0.8021\n",
      "Epoch 33: loss improved from 0.65721 to 0.64990, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.6499 - accuracy: 0.8021\n",
      "Epoch 34/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6429 - accuracy: 0.8041\n",
      "Epoch 34: loss improved from 0.64990 to 0.64298, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.6430 - accuracy: 0.8041\n",
      "Epoch 35/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.6362 - accuracy: 0.8064\n",
      "Epoch 35: loss improved from 0.64298 to 0.63618, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.6362 - accuracy: 0.8064\n",
      "Epoch 36/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.6334 - accuracy: 0.8063\n",
      "Epoch 36: loss improved from 0.63618 to 0.63338, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.6334 - accuracy: 0.8063\n",
      "Epoch 37/40\n",
      "509/510 [============================>.] - ETA: 0s - loss: 0.6236 - accuracy: 0.8087\n",
      "Epoch 37: loss improved from 0.63338 to 0.62358, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 12s 24ms/step - loss: 0.6236 - accuracy: 0.8087\n",
      "Epoch 38/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.6187 - accuracy: 0.8095\n",
      "Epoch 38: loss improved from 0.62358 to 0.61888, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 26ms/step - loss: 0.6189 - accuracy: 0.8095\n",
      "Epoch 39/40\n",
      "510/510 [==============================] - ETA: 0s - loss: 0.6124 - accuracy: 0.8117\n",
      "Epoch 39: loss improved from 0.61888 to 0.61244, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.6124 - accuracy: 0.8117\n",
      "Epoch 40/40\n",
      "508/510 [============================>.] - ETA: 0s - loss: 0.6045 - accuracy: 0.8142\n",
      "Epoch 40: loss improved from 0.61244 to 0.60423, saving model to RNN_model_mini.hdf5\n",
      "510/510 [==============================] - 13s 25ms/step - loss: 0.6042 - accuracy: 0.8143\n"
     ]
    }
   ],
   "source": [
    "filepath = \"RNN_model_mini.hdf5\"\n",
    "checkpoint2 = ModelCheckpoint(filepath, monitor='loss', verbose=1, \n",
    "                             save_best_only=True, mode='min')\n",
    "history2 = model2.fit(dataX, dataY, epochs=40, batch_size=256, callbacks=[checkpoint2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d8e982ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1BklEQVR4nO3deXxU9bn48c+TfV8Ja4Cw74sYoO6oYNG6VlpXXGq1tNWqvfZX709ruf7aar3t1V5rr9pqXargiqW9WhSXVlwJiLIqi0ASloQsZJ0sM8/vj3MSJiGBgBlmkvO8X695zZxlZp45kO9zzvd8F1FVjDHGeFdUuAMwxhgTXpYIjDHG4ywRGGOMx1kiMMYYj7NEYIwxHmeJwBhjPM4Sgem1RCRPRFREYrqw7zUisuJYxGVMpLFEYCKCiGwXkUYR6dNu/Rq3MM8LU2jG9HqWCEwk+RK4rGVBRCYBieELJ/KJSHS4YzA9nyUCE0meBq4KWr4aeCp4BxFJF5GnRKRURHaIyJ0iEuVuixaR34jIPhHZBnyjg/c+JiK7RaRYRH7RUUEqjvtFpERE9ovIZyIysaOAReRaEdkoItUisk1Evtdu+wXuVU2ViGwVkbnu+iwR+bOI7BKRChF5xV1/UBWVe0U00n39hIj8j4i8KiK1wOki8g0R+cT9jkIRWdju/SeLyPsiUuluv0ZEpovI3uBqMxG5WETWdPQ7Te9micBEkg+BNBEZ5xbQlwB/abfPg0A6MBw4DSdxXOtuux44FzgOyAfmtXvvk0AzMNLd5yzgux3EcRZwKjAayHDjKOsk5hL3O9PcOO4XkWkAIjIDJ5H9xP2cU4Ht7vueBpKACUBf4P5OPr8jlwO/BFKBFUAtznHIwEl+3xeRC90YhgCv4Ry3HGAqsEZVV7q/aU7Q517pxmU8xhKBiTQtVwVzgE1AccuGoOTw76pararbgd8C891dvg08oKqFqloO3BP03n7A2cAtqlqrqiU4he+lHcTQhFPIjgVEVTeq6u6OglXV/1XVrer4J/A6cIq7+TrgcVV9Q1UDqlqsqptEZIAbywJVrVDVJve9XfVXVX3P/Uyfqr6jqmvd5c+ARThJEuAKYLmqLnK/p0xV17jbnsQp/BGRLODrwLNHEIfpJQ7bmsKYY+xp4F/AMNpVCwF9gDhgR9C6HcAg9/VAoLDdthZDgVhgt4i0rItqtz8AqvqWiPweeAgYIiJLgNtUtar9viJyNvBznKuHKJyz/LXu5sHAqx38xsFAuapWdLCtK9rELCIzgXuBiTjHJx54Iei7tnbyOX8BNopICk4SfbezhGd6N7siMBFFVXfg3DQ+B3i53eZ9OGfrQ4PWDeHAVcNunIIveFuLQqAB6KOqGe4jTVUndBLHf6vq8ThVN6NxqnfaEJF44CXgN0A/Vc3AKfhbMk0hMKKDjy8EskQko4NttTjJpOU7+ncUXrvlZ4GlwGBVTQce7kIMqGox8AFwEc5VlVULeZQlAhOJrgPOUNXa4JWq6geeB34pIqkiMhT4MQfuIzwP/EhEckUkE7g96L27captfisiaSISJSIjROQ02nFvpM4UkVicgtkH+DuIs+XsuxRodq8Ozgra/hhwrYic6X7fIBEZ68byGvAHEckUkVgROdV9z6fABBGZKiIJwMIuHK9UnCsMn3tf4vKgbc8As0Xk2yISIyLZIjI1aPtTwP8BJgFLuvBdpheyRGAijlvnXtDJ5ptwCudtODdKnwUed7f9EViGU5iu5uAriqtwCu8NQAXwIjCgg+9Icz+rAqd6qQznrL99nNXAj3ASUAVOAbw0aPvHuDeQgf3APzlwNTMf5+pmE84N51vc93wB3A0sBza7v/FwfgDcLSLVwF1uPC0x7MS5uvo3oBxYA0wJeu8SN6Yl7ROv8Q6xiWmM8TYR2Qp8T1WXhzsWEx52RWCMh4nIxTj3HN4KdywmfKzVkDEeJSLvAOOB+aoaCHM4JoysasgYYzzOqoaMMcbjelzVUJ8+fTQvLy/cYRhjTI+yatWqfaqa09G2HpcI8vLyKCjorGWhMcaYjojIjs62WdWQMcZ4XEgTgYjMFZHPRWSLiNzewfZMEVniDvP7cWdD/RpjjAmdkCUCd6TIh3BGWRwPXCYi49vt9n9xhsSdjNPr83ehiscYY0zHQnmPYAawRVW3AYjIYuACnO79LcbjDhXsDs+bJyL9VHXvkXxRU1MTRUVF+Hy+bgrdHAsJCQnk5uYSGxsb7lCM8bRQJoJBtB0utwiY2W6fT4FvAivcwbKGArlAm0QgIjcANwAMGTKE9oqKikhNTSUvL4+gIYZNBFNVysrKKCoqYtiwYeEOxxhPC+U9go5K5Pa91+4FMt3p8W4CPsGZQartm1QfVdV8Vc3PyTm49ZPP5yM7O9uSQA8iImRnZ9tVnDERIJRXBEW0HRs+F9gVvIM70ce14MwTizMO/ZdH82WWBHoe+zczJjKEMhGsBEaJyDCciUMupe046bgTc9SpaiPO3LH/6mgWKGOMiViBAPgboLkB/E0QaAb1O88Bv/sIWqcBUHXWa8BZr4ED+zX7nM9qbgh67T4PmQkjzuj2nxCyRKCqzSJyI8748NE4c7euF5EF7vaHgXHAUyLix7mJfF2o4jkWlixZwje/+U02btzI2LFjwx3OEdm+fTvjxo1jzJgxNDY2kp+fz2OPPUZsbCzvvPMOp59+OkuXLuW8884D4Nxzz+W2225j1qxZzJo1i5qamtaOfgUFBdx222288847YfxFpsdqbgR/IwSawN/sFqhuAetvdrY11kBDDTRWu89BywE/xMRDTIL7nHhgOTbB2d5Y6z5qoKnuwOvGWrdAbzxQsPvd5+YGt8BvbPscOKg2O3ROvrVnJQIAVX2VdnO2ugmg5fUHwKhQxnAsLVq0iJNPPpnFixezcOHCkH2P3+8nOjq62z93xIgRrFmzBr/fz5w5c3j++ee54oorAMjNzeWXv/xlayJor6SkhNdee42zzz672+MyPYAqNFRDbSnU7oOm2rZnw61nxH6nUPVVOvu17F/X8rrMKcyPVnQcRMUeWQEt0RCfAnEpEJvkJIvoOIiOdxJIQpq7HOs+xx1IMtFxBz9HRUNUjPO5UTHucsu6KHd9NIg4ryXKXY46+PODn6PjICo0t3V73BATkaqmpob33nuPt99+m/PPP781Efj9fn7605+ybNkyRITrr7+em266iZUrV3LzzTdTW1tLfHw8b775Ji+99BIFBQX8/ve/B9qedaekpPDjH/+YZcuW8dvf/pa33nqLv/3tb9TX13PiiSfyyCOPICJs2bKFBQsWUFpaSnR0NC+88AILFy5k3rx5XHDBBQBcccUVXHLJJZx//vkd/pbo6GhmzJhBcXFx67opU6bQ1NTEG2+8wZw5cw56z09+8hN+8YtfWCKIRKrO2WxTHTTVO9UMTfXOWa+/yT3zbjxwth1ocs+Ag6sl2lVRBBf6LQW6v+HI4oqKgaQ+kOw+MoZCcg4kZTmFX1SsU/hGRQe9jnGe41IgPtV9dgvxuBSIiTvw+f7gapZ69xjUO58Xl+y+J9kpYD1+v6rXJYL/+Nt6Nuzq3tsM4wem8fPzOpzjvNUrr7zC3LlzGT16NFlZWaxevZpp06bx6KOP8uWXX/LJJ58QExNDeXk5jY2NXHLJJTz33HNMnz6dqqoqEhMTD/n5tbW1TJw4kbvvvtuJafx47rrrLgDmz5/P3//+d8477zyuuOIKbr/9di666CJ8Ph+BQIDvfve73H///VxwwQXs37+f999/nyeffLLT7/L5fHz00Uf87ndt+/fdeeed3HnnnR0mghNOOIElS5bw9ttvk5qaesjfYr6CQADqK6C2BGpK3MLYfbRfbqg5UOgf1GDvaMiBM9S4FEjJcQruvuPdwtxdTu7jbG9/NtzykChIzICEjNAWwNExEO0mCnNIvS4RhMuiRYu45ZZbALj00ktZtGgR06ZNY/ny5SxYsICYGOdQZ2VlsXbtWgYMGMD06dMBSEtLO+znR0dHc/HFF7cuv/3229x3333U1dVRXl7OhAkTmDVrFsXFxVx00UWA02EL4LTTTuOHP/whJSUlvPzyy1x88cWt8QTbunUrU6dOZfPmzcybN4/Jkye32X7KKacA8O6773YY45133skvfvELfv3rXx/29xicAtq3/+BHfcWB54MK+H1OFUt7Eu0Uwik5kNwXskc6Z8yxiU51R0yCW+2R6DxaqhqiW860W6o9Yg5Ur8QmHNgvJsHZz+Nnzr1Vr0sEhztzD4WysjLeeust1q1bh4jg9/sREe677z5U9aBmkh2tA4iJiSEQODBRVHAb+4SEhNb7Aj6fjx/84AcUFBQwePBgFi5ciM/n41CTDM2fP59nnnmGxYsX8/jjj3e4T8s9gt27dzNr1iyWLl16UPXRHXfcwS9/+csOE8kZZ5zBz372Mz788MNO4/Ac334o3wZlW6H8SyjfemC5bt+h3xuTeKBgzxgCg6Y5r1P6umfgLa9znLPrENUfm96v1yWCcHjxxRe56qqreOSRR1rXnXbaaaxYsYKzzjqLhx9+mFmzZrVWDY0dO5Zdu3axcuVKpk+fTnV1NYmJieTl5fGHP/yBQCBAcXExH3/8cYff15Ig+vTpQ01NDS+++CLz5s0jLS2N3NxcXnnlFS688EIaGhrw+/0kJSVxzTXXMGPGDPr378+ECYdOlgMGDODee+/lnnvuOSgRnHXWWfzsZz9j165dHb73jjvuYMGCBQwfPvxIDmHP1FgH1buhqhiqdgU9u6/3F0FdWdv3pA6E7BEw9hyncE/MdArxhAxISG/7iE0Ix68yHmSJoBssWrSI229vO7jqxRdfzLPPPsuDDz7IF198weTJk4mNjeX666/nxhtv5LnnnuOmm26ivr6exMREli9fzkknncSwYcOYNGkSEydOZNq0aR1+X0ZGBtdffz2TJk0iLy+vtYoJ4Omnn+Z73/sed911F7GxsbzwwgsMHz6cfv36MW7cOC688MIu/aYLL7yQhQsXdlgNdMcdd7TeeG7vnHPOoaPe3z2Sbz9UbIfKQthf6D7vPLDcvpAHp2BPGwRpA2HgcZA5zCn4s4Y7r+OSjvnPMOZwetycxfn5+dp+YpqNGzcybty4MEXUM9TV1TFp0iRWr15Nenp6uMNpFRH/drVlsO9zKN0EpUHP1bvb7heTCBmDIX3wgef0XKfQTxsEqQOsoDcRS0RWqWp+R9vsisADli9fzne+8x1+/OMfR1QSCClVp636/uK2LWmCb7rWlEDlzrZ19bHJkDMahs+CPqOds/n0wU41TlK23Sw1vZIlAg+YPXs2O3fuDHcY3c/fDCXr3eobt8qmcueBR0cdk6JiDjRxTM6BfuMhZxzkjHEeabl209V4jiUC07PUlMCW5bD5Ddj6lnPW3yI+zTlzzxwKeSc7r9MHQUq/A4V/qNuuG9MDWSIwkS3gh+JVTsG/+XXYvcZZn9IPxn7DGXelz2i3BU5GOCM1pseyRGDCT9Wpty/fduBRFtTevrHa6Y2aOwPOuBNGzoH+k60Kx5huYonAHFut497Uuu3w98I9s52RH1tItHOGnzUccqfD0BNg+OnOGDTGmG5niaAb9fRhqM8991zWrVvXZv2HH37IzTffTENDAw0NDVxyySUMHTq0dRyiDRs2MGbMGKKjo5k7dy5jx47l2muvZfny5Zx55pngb2bJC4v55mXzeeGx/2be2acdGCJBopz6+uOuhCy3rX3WMCcJRNs8xsYcK5YIulFPH4a6I1dffTXPP/88U6ZMwe/38/nnnzN+/HiuvfZaAPLy8nj77bfp06cPAE88/jiTJk5g0ZN/4szJg6CpnsXPPsWU8aOdyTcSM5wmmnHu+DeVm2C6jU1kTDhZJWs3aRmG+rHHHmPx4sWt6/1+P7fddhuTJk1i8uTJPPjggwCsXLmSE088kSlTpjBjxgyqq6t54oknuPHGG1vfe+6557ZO7pKSksJdd93FzJkz+eCDD7j77ruZPn06EydO5IYbbmgdZ2jLli3Mnj2bKVOmMG3aNLZu3cr8+fP561//2vq5V1xxBUuXLu3S7yopKWHAgAGAM/Dd+PHjD96psQ5q9sK+LbC/iFOOH8/HBatoavZTI2lsKSplav7XnBY8GUMgOdsZ+Mxa7xgTEXrfFcFrt8Oetd37mf0nwdn3HnKX3jQMdbBbb72VMWPGMGvWLObOncvVV1/tjGraVO+MjulvgvItQKZzhh+fgiRkMPvr57Bs1Zfs37+f8y+4kC+/PKqpqI0xx4BdEXSTRYsWcemllwIHhqEGOhyG+vPPPz9oGOqORvMM1tEw1DNnzmTSpEm89dZbrF+/nurq6oOGoU5KSuK0005jy5YtlJSUsGjRok6Hoe7IXXfdRUFBAWeddRbPPvMMc+ecASWbnGEYavY6Z/VpudBvAvQd504qEsell13O4sWLWbx4MZdddtmRHUxjzDHV+64IDnPmHgq9ZRjqg6iCv4ER/dP5/rdmc/038smZfCZlFZVkDxrh1PdHxTiFf3Rcm7fOmDGDdevWkZiYyOjRo7v2fcaYsLArgm7QMgz1jh072L59O4WFhQwbNqzNMNTNzc78qe2HoQaorq6mubmZvLw81qxZQyAQoLCw8IiGoQbaDEMN0NDQQF1dHQDXXHMNDzzwAMChh6H2VUH1Hqf9/p61/O+zj6KVO8HfxOaSeqJj4sgYOd0ZJ/8wLXvuuecefvWrX3XpGBpjwqf3XRGEQY8dhrplwnFfJezbwueff07usFHONhHu/9VdvPT3t7n1/z1IUnIyMTExPPPMM11usWTzFxvTM9gw1B7ROgz1qlWkJ0ZDfaWTAALNTnv+uGTn0dK0M+rYnCPYv50xx4YNQ+1xy994g+9c9x1+/P3rSPcVQV2TU/jHpzkTqcSn2XANxniYJYLerMkH9eXMnjyQnR8uBcQ5228t/I9NpzRjTGTrNYmgs5Y4nuNvcqp96suhyblRTHyqM3tWQnpEFf49rVrSmN6qVySChIQEysrKyM7O9mYyCASgYT/UlTs3f1FnWsW0Qc7ZfwSO26OqlJWVOZ3TjDFh1SsSQW5uLkVFRZSWloY7lGPL3+SM2tlY64zjExUDsUnOTd9ohbIyoIMJ1iNEQkICubm54Q7DGM/rFYkgNjaWYcOGhTuMY6PJBxuXwqonYMd7EBUL486F4+Y78+xGUNWPMeboBQLK3mofheX1FJbXsbO8juOGZDBrTN9u/65ekQg8ofQLWP0krHnWqf/PzIPZC2HqlU7nLmNMj9LYHGBvlY/d+33s3l/PrkofxZV17Cyvp6i8jqKKehr9B0YaEIEfzhppicBz6sph/cvw2fNQ+JFT9TP2XDj+Ghh2mjX5NCYC+ANKSbWP4op69lY1UNfYjK/JT32Tn/rGAPVNfme50U9lfaNb8PvYV9NA+/YS6YmxDMlKYtyANOZM6MfgzCSGZCUxOCuJgRkJxMeE5orfEkGkaaqHL/7hFP6b34BAE+SMdc/+r4CU7j8bMMbLahuaKa6sp7iinl3762lqDiAiRAmICCIQ5S4HFPbs91FcWU9RRR3FlfXs2e+jyd95C7i4mCgSY6NJjI0mNSGGARmJjOufxoCMBAamJ9I/PYGBGQn0T08kJT48RXJIv1VE5gK/A6KBP6nqve22pwN/AYa4sfxGVf8cypgiUiDg1Pd/thg2LIWGKkjpDzO/B5MvcYbB9mJrKGO+AlWlqr6ZkmofJdUNznNVA7vdgry4op7iynr21zcd0eeKQL/UBAZlJnLc4EwGTU5kUEYigzIT6Z+WQEp8DAmx0STFRZMQG010VOT/7YYsEYhINPAQMAcoAlaKyFJV3RC02w+BDap6nojkAJ+LyDOq2hiquCJK1W5Y8wx88jRUbIe4FBh3Pkz+Ngw71W78Gk/zB5Symgb2VjmF+N6qBvZW+SivbaTJH6DJrzQHAjT7lUZ/gGZ/gOaAUtPQTElVA6U1DTQ2Bw763OS4aAZlOoX3tKEZDMxwXudmJjIwI5GEmGgCqig4z+oMyxVw63H6pMQTF9O7qmVDeUUwA9iiqtsARGQxcAEQnAgUSBWn8X8KUA40hzCm8PM3w5blzo3fL5Y58/fmnQKn3+HU/8clhTtCY46JQEDZXeVjR1ktO8vq2FFex86yOgor6thb5aO0uoFABzUuGUmxxEVHERsdRUy0OM9R0rqcFBfNjGFZ9E2NJyc1nr5pCeSkxNM3LZ6+qfGkxMd4s7/RIYQyEQwCCoOWi4CZ7fb5PbAU2AWkApeo6kEpXERuAG4AGDJkSEiCDbmKHc6Z/yfPQPUuSO4LJ94E066C7BHhjs6YbqWq7K9vYvd+H3uqfOx1n/e4zzvL6ygqb9sqJiZKyM1MZHBWEmP7p9IvLYG+bkHeLy2Bfmnx9EmJJza6d52NR4JQJoKOUm77/P51YA1wBjACeENE3lXVqjZvUn0UeBSc0Ue7P9QQqS2DDUtg7Yuw8wNAYORsOOc+GD03Inv8GtNekz9AWU0j+2oaKKttZF91AxV1jVT5mqmqb6LK10R162vneV9NAw3tqmVEIDs5nv7p8Yzum8qccf0Ykp3E0KxkhmYnMSA9gRgr5MMilImgCBgctJyLc+Yf7FrgXnUGndkiIl8CY4GOZ2TpCRpq4PNXYe0LsPUtZ5jnnLFwxp0w5TJIt560JvxUlcq6pjY3UVtfVzdQWt3gFPw1jYe8mZqaEENaQqzznBjLoIxExvVPJTsljn5pCQxIT6R/erx7dp/Q6+rWe4tQJoKVwCgRGQYUA5cCl7fbZydwJvCuiPQDxgDbQhhTaKjCljfh00VOEmiqg/TBcMKNMOlbzny+VidpwqixOcBnRZV8sLWMD7aVsXpnBb6mg2+kpsTH0Dc1nj6p8Yzrn0Z2Shx9UuLJTokjOzmenFTnOTMpjpSEmB7RIsYcXsgSgao2i8iNwDKc5qOPq+p6EVngbn8Y+H/AEyKyFqcq6aequi9UMXW7gB82vALv/hfsXQeJWc5Z/6RvweCZ1uHLhE2zP8D6XVW87xb8BdvLqWv0AzBuQBqXTh/C4Kwkpw4+1Tljz0mNJzlM7dhNeIX0X11VXwVebbfu4aDXu4CzQhlDSDQ3wmfPwYr7oXwr9BkNFz4MEy+GmLjDv9+YI+Rr8rOrsr61/XtRRT3ldY3U+JqpbWimuqGZGl8zNQ3Oo9rX1NrJaVTfFOYdn8sJw7OZOTybrGT7P2rasvR/JJrqYfXT8N7voKoI+k+Gbz/lNPu0Nv/mK/A1+SmudAYXK3IL+paeq0UV9ZRWN7TZP0ogKzmOlPgYkuNjSImPYWCG05kpJSGGlPhYxg9M42vDs+ibakN9m0OzRNAVtfuc0T4/ehhqS2Hw1+C8B5wWQFb3b7qo2R9gR3kdX+yp5ou9NWwtraGwoq7Dgj42WhjodnI6fUwOuZlJrb1Xc90erNbCxnQXSwSHsvtT+OhRpwWQvwFGnAmn/BvknRTuyEwEa/IH2Flex7bSWr7YW+0+athaUtPabl4EBmUkMiQriTPG9CU3M5HcrERyM5MYnOnU3UfZjVhzjFgiaM/fBJv+Dh894rT9j02G466EGTdA37Hhjs5EkLKaBraU1LBtXy3bSmv4cl8t20pr2VleR3NQl9iB6QmM7p/KKaP6MLpfKmP6pTKybwqJcVadaCKDJYIWdeVQ8LjzqCp2xvv/+q+cET8TM8IdnQkzX5Of9buqWFNYySc7K1hTWElRRX3r9riYKIZlJzOmfypzJ/ZneE4Kw3OSGdk3hbQE6zhoIpslgsqd8MFDsPopp/3/8NPhG/8Fo+bYDWAPamwOsHv/gZY5G3ZX8cnOCjbsrmpthTMwPYGpQzK46oShjOmfxvA+yQzMSLQ29abH8m4i2Lveaf2z9kWnwnbyJc7YP33HhTsyE2KqSlFFPat2VPD53mqKKuopdlvolFS3nSwkKS6aybnpXHfycKYOzuC4IRn0S7NWOKZ38VYiUIUd78N7D8Dm1536/5kL4IQf2NAPvVhjc4ANu6so2F7O6p0VFGyvoMRtpRMbLQxId4YhPmVUDrmZB8aWz81wZoWy1jmmt/NOIihcCcv+HYpWQlI2nH4nTL8OkrLCHZnpRi03cDeX1LClpIYNu6v4rKiydTiF3MxEThyRzfFDMzl+aBZj+qdalY7xPO8kAhRq9sI5v3FuANu4/z1aQ7Nz8/bTwsrWQn9LSQ3ltQfmNEqKi2Z0v1QunzGU/LxMjh+aadU6xnTAO4lg8Az40Rq7AdxDlVT5WL2zglU7Kli9s5K1xftbZ59KT4xlVN8Uvj6hHyNyUhjlNs8ckJZgbfGN6QLvJAKwJNBDtDTV/LSwkjWFlazeWdHaVDMuJopJg9K5+oShHD80k+OGZNI3Nd5mnDLmK/BWIjARJxBQtu2rbS301xRWsnF3VWuHrAHpCRw3JINrTsxj2tBMJgxMIz7GErox3ckSgTnmfE1+Vmzexxsb9rJ8417K3Hr95LhoJudmcP2pTlPNqYOtqaYxx4IlAnNMVNQ28tamEl7fsId/fbGP+iY/qfExnD62LyeP7MPUIRmMyEmxFjzGhIElAhMSqsqmPdWs2LyPNzftZeX2CvwBpX9aAvOOz2XO+H58bXi2TV1oTASwRGC6zZ79PlZs2ceKzaWs2FLGvhqn09bofil8/7QRzBnfj0mD0q0ljzERxhKBOWpVviY+3lbOe1v3sWLzPjaX1ADQJyWOk0b24eSRfTh5VB8GpCeGOVJjzKFYIjBdVtfYTMH2itZ5cNcWVRJQiI+JYsawLL6Vn8vJI3MY2z/VzvqN6UEsEZhO+Zr8rN5ZwUfbyvlgaxmfFFbQ5FdiooTjhmRw4xmjOGF4NscNySAh1pp0GtNTWSIwreobnYL/w21lfLStnDWFlTT6A0QJTBrkjMB5wohspudlkhRn/3WM6S3sr9njNu+t5m+f7uK9rWV8VlRJk1+JjhImDkzjmpPy+NrwLPLzsmxyFWN6MUsEHlRS7WPpml0s+aSY9buqiI6S1jH3Zw7PIn9oJqlW8BvjGZYIPKKusZnX1+/l5U+KWbG5lIDC5Nx0fn7eeM6dPJCc1Phwh2iMCRNLBL1YXWMz//qilH+s28PrG/ZS1+hnUEYiP5g1kguPG8TIvinhDtEYEwEsEfQylXWNLN9YwrL1e3h3cym+pgAZSbFcMHUgFx2XS/7QTGvaaYxpwxJBL7CvpoFX1+5m2fo9fLitHH9AGZCewKXTh3DWhH7MyMuy6RaNMZ2yRNCDFVfW88g/t/LcykIamgOMyEnme6cO5+sT+jM5N93G6DfGdIklgh5oW2kN//POVpZ8UowIfPO4XK47ZRij+6WGOzRjTA9kiaAH2bi7iofe3sKra3cTGx3FlV8byg2nDmdgho3lY4w5eiFNBCIyF/gdEA38SVXvbbf9J8AVQbGMA3JUtTyUcfU0nxVV8t9vbmb5xhJS4mO44dQRXHfyMGvyaYzpFiFLBCISDTwEzAGKgJUislRVN7Tso6r/Cfynu/95wK2WBA5YV7yfB5Z/wfKNJWQkxXLr7NFcc2Ie6UnW2csY031CeUUwA9iiqtsARGQxcAGwoZP9LwMWhTCeHmPDrioeWP4Fr2/YS1pCDLedNZqrT8yz3r7GmJAIZSIYBBQGLRcBMzvaUUSSgLnAjZ1svwG4AWDIkCHdG2UE+XxPNQ8s/4LX1u0hNSGGW2eP5tqT82ycH2NMSIUyEXTUdlE72fc84L3OqoVU9VHgUYD8/PzOPqPHKiyv49f/2MT/rt1NclwMPzpjJNedPNyqgIwxx0QoE0ERMDhoORfY1cm+l+LBaqHG5gB/fHcbD761mSgRfjBrBN89eTiZyXHhDs0Y4yGhTAQrgVEiMgwoxinsL2+/k4ikA6cBV4Ywlojz0bYy7nhlHVtKapg7oT93nTfemoEaY8KiS4lARF4CHgdeU9VAV96jqs0iciOwDKf56OOqul5EFrjbH3Z3vQh4XVVrjzj6Hqi8tpF7Xt3IC6uKGJSRyGNX53PmuH7hDssY42GievgqdxGZDVwLfA14AXhCVTeFOLYO5efna0FBQTi++isJBJQXVhVyz2ubqPE1c/2pw/nRGaNIjLMpHo0xoSciq1Q1v6NtXboiUNXlwHK3Gucy4A0RKQT+CPxFVZu6LdpeaFdlPbcsXsPH28uZkZfFLy6aaMNBGGMiRpfvEYhINk49/nzgE+AZ4GTgamBWKILrDT7cVsYPn1lNQ3OA++ZN5lvH59pgcMaYiNLVewQvA2OBp4HzVHW3u+k5Eel59TTHgKry5Pvb+cX/bmRodhKPzM+3iWCMMRGpq1cEv1fVtzra0Fmdk5f5mvz83yVreXl1MbPH9eP+S6ZYr2BjTMTq6mwl40Qko2VBRDJF5AehCaln21VZz7ce/oCXVxdz6+zRPDr/eEsCxpiI1tVEcL2qVrYsqGoFcH1IIurBPthaxnkPrmD7vlr+dFU+N88eZdNCGmMiXlerhqJERNRta+qOLGrdX4M8/cF2Fv5tA3nZSTx6VT4jcux+gDGmZ+hqIlgGPC8iD+OMF7QA+EfIouphlnxSxM/+up7Z4/py/yVTrSrIGNOjdDUR/BT4HvB9nMHkXgf+FKqgepKV28v56Ytr+drwLP5wxfHExdgk8caYnqWrHcoCwP+4D+PaUVbLDU8VkJuZyMNXWhIwxvRMXe1HMAq4BxgPJLSsV9XhIYor4u2va+LaJ1aiwOPXTCcjyW6ZGGN6pq6ewv4Z52qgGTgdeAqnc5knNfkDfP+ZVRSW1/HIlceT1yc53CEZY8xR62oiSFTVN3EGqduhqguBM0IXVuRSVX72yjre31rGvd+czMzh2eEOyRhjvpKu3iz2iUgUsNkdWroY6Bu6sCLXH9/dxuKVhdx0xkguPj433OEYY8xX1tUrgluAJOBHwPE4g89dHaKYItay9Xu457VNfGPyAG6dPTrc4RhjTLc47BWB23ns26r6E6AGZ14Cz1lbtJ9bFq9hSm4Gv/3WFOsxbIzpNQ57RaCqfuB48fDYyarKT178lKzkOP54VT4JsTaZjDGm9+jqPYJPgL+KyAtA65SSqvpySKKKMO9vLWPTnmrumzeZnNT4cIdjjDHdqquJIAsoo21LIQU8kQgeW/ElfVLiOH/KwHCHYowx3a6rPYs9eV8AYGtpDW9tKuGW2aOsSsgY0yt1tWfxn3GuANpQ1e90e0QR5s/vfUlcTBRXfm1ouEMxxpiQ6GrV0N+DXicAFwG7uj+cyFJZ18hLq4q5cOpA+qTYvQFjTO/U1aqhl4KXRWQRsDwkEUWQZz/eSX2Tn++cPCzcoRhjTMgc7XCZo4Ah3RlIpGnyB3jq/R2cNDKbsf3Twh2OMcaETFfvEVTT9h7BHpw5CnqtV9fuZk+Vj199c2K4QzHGmJDqatVQaqgDiSSqymMrvmR4TjKzRntySCVjjId0qWpIRC4SkfSg5QwRuTBkUYVZwY4KPivaz7UnDbOhJIwxvV5X7xH8XFX3tyyoaiXw85BEFAEee/dL0hNjuXjaoHCHYowxIdfVRNDRfl1tetqjFJbX8fqGPVw+cwhJcb3yJxpjTBtdTQQFIvJfIjJCRIaLyP3AqlAGFi5/fm87USJcfUJeuEMxxphjoquJ4CagEXgOeB6oB34YqqDCpdrXxPMFhXxj8gD6pycc/g3GGNMLdLXVUC1w+5F+uIjMBX4HRAN/UtV7O9hnFvAAEAvsU9XTjvR7ustzKwupaWjmOutAZozxkK62GnpDRDKCljNFZNlh3hMNPAScDYwHLhOR8e32yQD+AJyvqhOAbx1R9N3IH1CeeH870/MymZybEa4wjDHmmOtq1VAft6UQAKpaweHnLJ4BbFHVbaraCCwGLmi3z+XAy6q60/3cki7G0+3e2LCHoop6uxowxnhOVxNBQERah5QQkTw6GI20nUFAYdBykbsu2GggU0TeEZFVInJVRx8kIjeISIGIFJSWlnYx5CPz4bZykuOimTO+f0g+3xhjIlVX20feAawQkX+6y6cCNxzmPR31xGqfPGKA44EzgUTgAxH5UFW/aPMm1UeBRwHy8/MPl4COSmlNA/3SE4i2DmTGGI/p6s3if4hIPk7hvwb4K07LoUMpAgYHLedy8NDVRTg3iGuBWhH5FzAF+IJjrLS6gRwbatoY40FdvVn8XeBN4N/cx9PAwsO8bSUwSkSGiUgccCmwtN0+fwVOEZEYEUkCZgIbux5+9ymtbrD5iI0xntTVewQ3A9OBHap6OnAccMjKelVtBm4EluEU7s+r6noRWSAiC9x9NgL/AD4DPsZpYrruqH7JV2SJwBjjVV29R+BTVZ+IICLxqrpJRMYc7k2q+irwart1D7db/k/gP7sccQjUNTZT09BsicAY40ldTQRFbpv/V4A3RKSCXjRV5b7qRgC7R2CM8aSu3iy+yH25UETeBtJxqnR6hdIaHwB902xYCWOM9xzx8Jqq+s/D79WzlFY3AHZFYIzxpqOds7hXaU0Edo/AGONBlghwEkGUQFZyXLhDMcaYY84SAVBS3UB2Srz1KjbGeJIlAqxXsTHG2ywR4IwzZPcHjDFeZYkA54qgryUCY4xHeT4RBALKPrsiMMZ4mOcTwf76Jpr8aonAGONZnk8EpTXWh8AY422eTwQlVdar2BjjbZ5PBC3jDNkVgTHGqywR2PASxhiPs0RQ3UBCbBQp8Uc8/p4xxvQKlgiqG+ibmoCIDS9hjPEmSwTWh8AY43GWCGycIWOMx1kisEnrjTEe5+lE0NgcoKKuyRKBMcbTPJ0I9lmvYmOM8XYisLmKjTHGEgEAfdMsERhjvMvbicCqhowxxuOJwL0iyE62RGCM8S7PJ4LMpFjiYjx9GIwxHufpErCk2mfVQsYYz/N0IrDOZMYY4/VEUGPDSxhjTEgTgYjMFZHPRWSLiNzewfZZIrJfRNa4j7tCGU8wVbUrAmOMAUI2CL+IRAMPAXOAImCliCxV1Q3tdn1XVc8NVRydqWloxtcUoG9qwrH+amOMiSihvCKYAWxR1W2q2ggsBi4I4fcdEZuZzBhjHKFMBIOAwqDlInddeyeIyKci8pqITOjog0TkBhEpEJGC0tLSbgnOEoExxjhCmQg6mvJL2y2vBoaq6hTgQeCVjj5IVR9V1XxVzc/JyemW4EosERhjDBDaRFAEDA5azgV2Be+gqlWqWuO+fhWIFZE+IYyplQ04Z4wxjlAmgpXAKBEZJiJxwKXA0uAdRKS/uJMFi8gMN56yEMbUqrSmgdhoIT0x9lh8nTHGRKyQtRpS1WYRuRFYBkQDj6vqehFZ4G5/GJgHfF9EmoF64FJVbV99FBKl1Q30SYknKsomrTfGeFvIEgG0Vve82m7dw0Gvfw/8PpQxdKa0uoG+dn/AGGO827PYOpMZY4zDu4mgxhKBMcaARxOBP6CU2ThDxhgDeDQRlNU2EFDrQ2CMMeDRRGC9io0x5gBLBMYY43HeTgQpNvKoMcZ4MxHU2BWBMca08GYiqG4gNT6GxLjocIdijDFh59lEYFcDxhjj8GQiKKluoI8lAmOMATyaCPbZFYExxrTyZCIorbZexcYY08JziaC+0U91Q7NdERhjjMtziWCf23TUhqA2xhiH5xKBzVVsjDFteS4R2PASxhjTlvcSgfUqNsaYNryXCKp8RAlkJ1siMMYY8GIiqGkgKzmeaJu03hhjAC8mAutMZowxbXgyEVjTUWOMOcCTicCuCIwx5gBPJQJVpbTGEoExxgTzVCLYX99Ek19tnCFjjAniqURgvYqNMeZgnkoE1qvYGGMOZonAGGM8zhKBMcZ4nLcSQU0DCbFRpMbHhDsUY4yJGN5KBG4fAhEbXsIYY1qENBGIyFwR+VxEtojI7YfYb7qI+EVkXijjsSkqjTHmYCFLBCISDTwEnA2MBy4TkfGd7PdrYFmoYmlhvYqNMeZgobwimAFsUdVtqtoILAYu6GC/m4CXgJIQxgJASbXPEoExxrQTykQwCCgMWi5y17USkUHARcDDh/ogEblBRApEpKC0tPSogmlsDlBR10ROSsJRvd8YY3qrUCaCju7IarvlB4Cfqqr/UB+kqo+qar6q5ufk5BxVMGW11nTUGGM6Esp2lEXA4KDlXGBXu33ygcVuK54+wDki0qyqr3R3MC19CGwIamOMaSuUiWAlMEpEhgHFwKXA5cE7qOqwltci8gTw91AkAbDOZMYY05mQJQJVbRaRG3FaA0UDj6vqehFZ4G4/5H2B7paeGMvcCf0ZkGH3CIwxJpiotq+2j2z5+flaUFAQ7jCMMaZHEZFVqprf0TZP9Sw2xhhzMEsExhjjcZYIjDHG4ywRGGOMx1kiMMYYj7NEYIwxHmeJwBhjPM4SgTHGeFyP61AmIqXAjqN8ex9gXzeG050stqMTybFBZMdnsR2dnhrbUFXtcNTOHpcIvgoRKeisZ124WWxHJ5Jjg8iOz2I7Or0xNqsaMsYYj7NEYIwxHue1RPBouAM4BIvt6ERybBDZ8VlsR6fXxeapewTGGGMO5rUrAmOMMe1YIjDGGI/zTCIQkbki8rmIbBGR28MdTzAR2S4ia0VkjYiEddYdEXlcREpEZF3QuiwReUNENrvPmREU20IRKXaP3RoROSdMsQ0WkbdFZKOIrBeRm931YT92h4gt7MdORBJE5GMR+dSN7T/c9ZFw3DqLLezHLSjGaBH5RET+7i4f1XHzxD0CEYkGvgDmAEU48ylfpqobwhqYS0S2A/mqGvZOKiJyKlADPKWqE9119wHlqnqvm0QzVfWnERLbQqBGVX9zrONpF9sAYICqrhaRVGAVcCFwDWE+doeI7duE+diJiADJqlojIrHACuBm4JuE/7h1FttcIuD/HICI/BjIB9JU9dyj/Vv1yhXBDGCLqm5T1UZgMXBBmGOKSKr6L6C83eoLgCfd10/iFCLHXCexRQRV3a2qq93X1cBGYBARcOwOEVvYqaPGXYx1H0pkHLfOYosIIpILfAP4U9DqozpuXkkEg4DCoOUiIuQPwaXA6yKySkRuCHcwHeinqrvBKVSAvmGOp70bReQzt+ooLNVWwUQkDzgO+IgIO3btYoMIOHZu9cYaoAR4Q1Uj5rh1EhtEwHEDHgD+DxAIWndUx80riUA6WBcxmR04SVWnAWcDP3SrQEzX/A8wApgK7AZ+G85gRCQFeAm4RVWrwhlLex3EFhHHTlX9qjoVyAVmiMjEcMTRkU5iC/txE5FzgRJVXdUdn+eVRFAEDA5azgV2hSmWg6jqLve5BFiCU5UVSfa69cwt9c0lYY6nlarudf9YA8AfCeOxc+uRXwKeUdWX3dURcew6ii2Sjp0bTyXwDk4dfEQctxbBsUXIcTsJON+9v7gYOENE/sJRHjevJIKVwCgRGSYiccClwNIwxwSAiCS7N/AQkWTgLGDdod91zC0FrnZfXw38NYyxtNHyn951EWE6du6NxceAjar6X0Gbwn7sOostEo6diOSISIb7OhGYDWwiMo5bh7FFwnFT1X9X1VxVzcMpz95S1Ss52uOmqp54AOfgtBzaCtwR7niC4hoOfOo+1oc7NmARzuVuE86V1HVANvAmsNl9zoqg2J4G1gKfuX8EA8IU28k41Y2fAWvcxzmRcOwOEVvYjx0wGfjEjWEdcJe7PhKOW2exhf24tYtzFvD3r3LcPNF81BhjTOe8UjVkjDGmE5YIjDHG4ywRGGOMx1kiMMYYj7NEYIwxHmeJwJhjSERmtYwUaUyksERgjDEeZ4nAmA6IyJXuWPRrROQRd/CxGhH5rYisFpE3RSTH3XeqiHzoDkK2pGUQMhEZKSLL3fHsV4vICPfjU0TkRRHZJCLPuD1/jQkbSwTGtCMi44BLcAYDnAr4gSuAZGC1OgME/hP4ufuWp4CfqupknB6nLeufAR5S1SnAiTi9osEZ/fMWYDxOz/KTQvyTjDmkmHAHYEwEOhM4Hljpnqwn4gzeFQCec/f5C/CyiKQDGar6T3f9k8AL7vhRg1R1CYCq+gDcz/tYVYvc5TVAHs6kJ8aEhSUCYw4mwJOq+u9tVor8rN1+hxqf5VDVPQ1Br/3Y36EJM6saMuZgbwLzRKQvtM4DOxTn72Weu8/lwApV3Q9UiMgp7vr5wD/VGe+/SEQudD8jXkSSjuWPMKar7EzEmHZUdYOI3Ikza1wUzminPwRqgQkisgrYj3MfAZzhfh92C/ptwLXu+vnAIyJyt/sZ3zqGP8OYLrPRR43pIhGpUdWUcMdhTHezqiFjjPE4uyIwxhiPsysCY4zxOEsExhjjcZYIjDHG4ywRGGOMx1kiMMYYj/v/0Mb92z+GOOoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history2.history['accuracy'])\n",
    "plt.plot(history1.history['accuracy'])\n",
    "plt.title('Models accuracy')\n",
    "plt.legend(['Accuracy RNN', 'Accuracy LSTM'], loc='upper left')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c9e5b354",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_p_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/63/bhtnrx_s4qz9c77b076z4mpr0000gn/T/ipykernel_90259/2423827812.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdifferent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p_lstm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0my_p_lstm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my_p_rnn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdifferent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_p_lstm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_p_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "different = []\n",
    "for i in range(len(y_p_lstm)):\n",
    "  if y_p_lstm[i] != y_p_rnn[i]:\n",
    "    different.append(i)\n",
    "y_p_lstm[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0797a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = []\n",
    "output_lstm = []\n",
    "output_rnn = []\n",
    "\n",
    "for x in dataX:\n",
    "  input.append([tokenizer.index_word[i] if i in tokenizer.index_word else '' for i in x])\n",
    "\n",
    "for y in y_p_lstm:\n",
    "  output_lstm.append(tokenizer.index_word[y] if y in tokenizer.index_word else '')\n",
    "\n",
    "for y in y_p_rnn:\n",
    "  output_rnn.append(tokenizer.index_word[y] if y in tokenizer.index_word else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a93de8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model(\"LSTM_model_mini.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "852da584",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.load_model(\"RNN_model_mini.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47ca8114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4075/4075 [==============================] - 33s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "y_p_lstm = model1.predict(dataX)\n",
    "y_p_lstm = [np.argmax(y_softmax) for y_softmax in y_p_lstm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a9790aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4075/4075 [==============================] - 10s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "y_p_rnn = model2.predict(dataX)\n",
    "y_p_rnn = [np.argmax(y_softmax) for y_softmax in y_p_rnn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9d7c7bf3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classfication report:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.77      0.86      0.81     22987\n",
      "           2       0.84      0.89      0.87     15548\n",
      "           3       0.96      0.97      0.97     12046\n",
      "           4       0.95      0.93      0.94     12046\n",
      "           5       0.91      0.92      0.92      5580\n",
      "           6       0.94      0.96      0.95      4521\n",
      "           7       0.86      0.88      0.87      2866\n",
      "           9       0.87      0.72      0.78      1417\n",
      "          10       0.60      0.34      0.43      1170\n",
      "          12       0.83      0.82      0.83      1843\n",
      "          13       0.91      0.88      0.90      3666\n",
      "          14       0.75      0.75      0.75      2397\n",
      "          15       0.93      0.95      0.94      2397\n",
      "          16       0.91      0.28      0.43        75\n",
      "          17       0.55      0.75      0.64      2091\n",
      "          18       0.74      0.87      0.80      2114\n",
      "          19       0.75      0.84      0.80      1619\n",
      "          20       0.51      0.43      0.46      1703\n",
      "          21       0.74      0.76      0.75      1381\n",
      "          22       0.82      0.79      0.80       585\n",
      "          23       0.83      0.61      0.70        64\n",
      "          24       0.94      0.99      0.97       856\n",
      "          25       0.99      0.98      0.98      1229\n",
      "          26       0.75      0.66      0.70      1202\n",
      "          27       0.82      0.73      0.77       866\n",
      "          28       0.77      0.72      0.74       964\n",
      "          29       0.70      0.93      0.80       655\n",
      "          30       0.71      0.53      0.61      1495\n",
      "          31       0.66      0.21      0.32       417\n",
      "          32       0.00      0.00      0.00         1\n",
      "          33       0.78      0.60      0.68      2075\n",
      "          34       0.89      0.87      0.88      1570\n",
      "          35       0.83      0.58      0.68       358\n",
      "          36       0.75      0.08      0.14       116\n",
      "          37       0.69      0.25      0.37       868\n",
      "          38       0.53      0.64      0.58       808\n",
      "          39       0.71      0.50      0.58       420\n",
      "          40       0.25      0.89      0.39       529\n",
      "          41       0.58      0.22      0.32       293\n",
      "          42       0.56      0.77      0.65       317\n",
      "          43       0.85      0.97      0.91       705\n",
      "          44       0.66      0.62      0.64       763\n",
      "          45       0.91      0.81      0.86       966\n",
      "          46       0.76      0.53      0.62       484\n",
      "          47       0.59      0.31      0.41       260\n",
      "          48       0.98      0.94      0.96       208\n",
      "          49       0.71      0.66      0.69       798\n",
      "          50       0.82      0.29      0.43       261\n",
      "          51       0.99      0.72      0.83       140\n",
      "          52       0.59      0.28      0.38       123\n",
      "          53       0.90      0.83      0.86       557\n",
      "          54       0.97      0.95      0.96       355\n",
      "          55       0.89      0.94      0.91       136\n",
      "          57       0.85      0.56      0.68       110\n",
      "          58       0.84      0.71      0.77       122\n",
      "          60       0.91      0.44      0.59        71\n",
      "          61       0.71      0.84      0.77        83\n",
      "          62       0.55      0.72      0.62        83\n",
      "          63       0.79      0.88      0.83       104\n",
      "          64       0.95      0.97      0.96       171\n",
      "          65       0.61      0.52      0.56       207\n",
      "          66       0.69      0.70      0.69       129\n",
      "          67       0.52      0.64      0.58        50\n",
      "          68       0.84      0.87      0.85      1720\n",
      "          69       0.81      0.31      0.45       361\n",
      "          70       0.56      0.59      0.58       340\n",
      "          71       0.63      0.64      0.63       170\n",
      "          72       0.80      0.71      0.75       431\n",
      "          73       1.00      0.86      0.92         7\n",
      "          74       0.93      0.75      0.83       361\n",
      "          75       0.73      0.21      0.32        53\n",
      "          76       0.82      0.26      0.40       209\n",
      "          77       1.00      0.48      0.65        31\n",
      "          78       0.94      0.94      0.94        31\n",
      "          79       1.00      0.83      0.91        18\n",
      "          80       0.40      0.15      0.22        13\n",
      "          81       0.96      0.45      0.61        56\n",
      "          82       0.68      0.69      0.69       346\n",
      "          84       0.77      0.11      0.19       160\n",
      "          86       0.82      0.76      0.79       135\n",
      "          87       0.90      0.32      0.47        56\n",
      "          88       1.00      0.09      0.16        56\n",
      "          89       0.54      0.56      0.55       317\n",
      "          90       0.67      0.43      0.52        51\n",
      "          91       0.57      1.00      0.73        20\n",
      "          92       0.80      0.50      0.62         8\n",
      "          95       0.41      0.08      0.13       116\n",
      "          96       0.72      0.23      0.34        93\n",
      "         100       0.75      0.35      0.48        34\n",
      "         101       0.74      0.25      0.37       116\n",
      "         102       0.75      0.16      0.26        96\n",
      "         103       0.89      0.69      0.78       348\n",
      "         104       0.92      0.67      0.77        18\n",
      "         105       0.58      0.21      0.31        70\n",
      "         106       1.00      1.00      1.00         6\n",
      "         107       1.00      0.81      0.89        26\n",
      "         108       0.70      0.75      0.72       220\n",
      "         109       0.89      0.40      0.56       141\n",
      "         110       0.90      0.63      0.74        59\n",
      "         111       0.93      0.96      0.94       161\n",
      "         112       0.73      0.42      0.53        53\n",
      "         113       0.90      0.17      0.29        52\n",
      "         114       1.00      0.88      0.93         8\n",
      "         115       1.00      0.23      0.37        22\n",
      "         116       1.00      0.20      0.33         5\n",
      "         117       0.80      0.36      0.50        11\n",
      "         118       0.18      0.33      0.23        18\n",
      "         119       0.37      0.54      0.44        13\n",
      "         120       0.62      0.52      0.57        29\n",
      "         121       0.62      0.59      0.60        27\n",
      "         122       1.00      0.54      0.70        13\n",
      "         123       0.93      0.27      0.41        49\n",
      "         124       1.00      0.41      0.58        17\n",
      "         125       1.00      0.35      0.52        17\n",
      "         126       0.73      0.41      0.52        39\n",
      "         127       1.00      0.81      0.89        26\n",
      "         128       0.88      0.41      0.56        17\n",
      "         129       0.87      0.62      0.72        42\n",
      "         130       1.00      0.83      0.91        12\n",
      "         132       1.00      0.11      0.20         9\n",
      "         133       0.90      0.88      0.89       428\n",
      "         134       0.91      0.62      0.74        34\n",
      "         135       0.60      1.00      0.75        12\n",
      "         136       0.62      0.16      0.25        50\n",
      "         139       0.98      1.00      0.99        97\n",
      "         141       1.00      0.50      0.67         2\n",
      "         142       1.00      0.50      0.67         2\n",
      "         143       0.90      1.00      0.95         9\n",
      "         145       0.74      0.78      0.76        36\n",
      "         146       0.75      0.54      0.63        39\n",
      "         147       1.00      0.81      0.90        16\n",
      "         148       1.00      0.88      0.93         8\n",
      "         149       1.00      0.71      0.83         7\n",
      "         151       1.00      1.00      1.00         9\n",
      "         152       1.00      1.00      1.00         9\n",
      "         153       0.85      0.18      0.30        60\n",
      "         154       1.00      0.89      0.94         9\n",
      "         155       0.74      0.30      0.42       450\n",
      "         156       1.00      0.60      0.75        15\n",
      "         157       0.75      0.75      0.75         8\n",
      "         158       0.67      0.52      0.58        31\n",
      "         159       0.40      0.75      0.52         8\n",
      "         160       0.83      0.71      0.77         7\n",
      "         161       1.00      0.21      0.35        19\n",
      "         162       1.00      0.25      0.40         4\n",
      "         163       1.00      0.50      0.67         8\n",
      "         164       0.00      0.00      0.00         6\n",
      "         165       0.76      0.76      0.76        21\n",
      "         166       1.00      0.89      0.94        19\n",
      "         167       1.00      0.94      0.97        16\n",
      "         168       1.00      0.48      0.65        21\n",
      "         169       0.60      0.75      0.67         4\n",
      "         170       1.00      0.80      0.89         5\n",
      "         171       0.86      0.55      0.67        11\n",
      "         172       1.00      0.87      0.93        15\n",
      "         174       0.86      0.41      0.56        29\n",
      "         175       1.00      0.20      0.33        15\n",
      "         176       0.73      0.69      0.71        16\n",
      "         177       0.75      0.26      0.39        23\n",
      "         179       0.67      0.73      0.70        11\n",
      "         180       1.00      0.67      0.80         6\n",
      "         181       0.50      0.33      0.40         6\n",
      "         182       1.00      1.00      1.00         6\n",
      "         184       0.80      0.67      0.73         6\n",
      "         185       1.00      0.67      0.80        12\n",
      "         186       1.00      0.57      0.73         7\n",
      "         188       0.88      0.78      0.82         9\n",
      "         189       1.00      0.50      0.67         6\n",
      "         190       0.91      1.00      0.95        10\n",
      "         191       1.00      0.50      0.67         6\n",
      "         193       0.47      0.23      0.31        65\n",
      "         195       0.86      0.29      0.43        21\n",
      "         199       1.00      1.00      1.00         5\n",
      "         200       0.62      1.00      0.77         5\n",
      "         201       0.85      0.68      0.76        25\n",
      "         202       1.00      0.40      0.57         5\n",
      "         205       0.91      0.80      0.85        25\n",
      "         206       1.00      0.75      0.86         4\n",
      "         208       0.75      0.67      0.71         9\n",
      "         209       1.00      0.40      0.57        15\n",
      "         210       0.43      0.75      0.55         4\n",
      "         211       1.00      0.75      0.86         4\n",
      "         212       0.38      0.75      0.50         4\n",
      "         215       0.47      0.39      0.43        90\n",
      "         216       0.75      1.00      0.86         3\n",
      "         217       1.00      1.00      1.00         4\n",
      "         218       0.00      0.00      0.00         4\n",
      "         219       1.00      0.25      0.40        12\n",
      "         220       1.00      0.50      0.67         2\n",
      "         221       1.00      0.25      0.40         4\n",
      "         222       1.00      0.50      0.67         2\n",
      "         223       0.87      0.87      0.87        15\n",
      "         224       1.00      1.00      1.00         6\n",
      "         225       0.42      0.71      0.53         7\n",
      "         226       1.00      1.00      1.00         4\n",
      "         227       1.00      0.75      0.86         4\n",
      "         228       1.00      0.64      0.78        11\n",
      "         230       1.00      0.50      0.67         4\n",
      "         232       1.00      1.00      1.00         4\n",
      "         233       1.00      0.33      0.50         3\n",
      "         234       1.00      1.00      1.00         3\n",
      "         235       0.94      0.22      0.35        69\n",
      "         236       1.00      0.38      0.55        16\n",
      "         237       1.00      0.62      0.77         8\n",
      "         238       0.86      0.26      0.40        23\n",
      "         239       1.00      1.00      1.00         3\n",
      "         240       0.83      0.50      0.62        10\n",
      "         241       1.00      0.33      0.50         3\n",
      "         242       1.00      1.00      1.00         3\n",
      "         243       0.00      0.00      0.00         3\n",
      "         244       0.80      0.57      0.67         7\n",
      "         245       1.00      0.78      0.88        18\n",
      "         246       0.71      0.45      0.55        56\n",
      "         247       1.00      1.00      1.00         3\n",
      "         248       0.85      0.69      0.76        16\n",
      "         249       1.00      0.60      0.75        10\n",
      "         250       1.00      0.40      0.57         5\n",
      "         254       1.00      1.00      1.00         4\n",
      "         255       1.00      1.00      1.00         3\n",
      "         256       1.00      1.00      1.00         2\n",
      "         258       0.64      0.72      0.68       138\n",
      "         259       0.67      0.67      0.67         3\n",
      "         260       1.00      0.29      0.44         7\n",
      "         261       1.00      0.67      0.80        21\n",
      "         262       1.00      0.67      0.80         3\n",
      "         263       0.82      0.70      0.76        20\n",
      "         264       0.32      0.48      0.39        23\n",
      "         265       1.00      0.67      0.80         3\n",
      "         267       0.86      0.67      0.75         9\n",
      "         269       1.00      0.67      0.80         3\n",
      "         271       0.43      1.00      0.60         3\n",
      "         274       1.00      0.33      0.50         3\n",
      "         275       0.75      1.00      0.86         3\n",
      "         278       0.83      0.71      0.77         7\n",
      "         279       1.00      0.07      0.13        14\n",
      "         281       0.91      0.38      0.54        26\n",
      "         282       1.00      1.00      1.00         2\n",
      "         283       1.00      0.25      0.40         4\n",
      "         284       0.00      0.00      0.00         3\n",
      "         285       0.00      0.00      0.00         2\n",
      "         286       0.86      0.50      0.63        12\n",
      "         287       1.00      1.00      1.00         2\n",
      "         288       1.00      1.00      1.00         2\n",
      "         289       0.40      1.00      0.57         2\n",
      "         290       1.00      1.00      1.00         4\n",
      "         291       0.83      0.71      0.77        14\n",
      "         292       0.91      1.00      0.95        10\n",
      "         293       1.00      1.00      1.00         2\n",
      "         294       0.75      0.75      0.75         4\n",
      "         295       1.00      1.00      1.00         1\n",
      "         296       0.40      0.75      0.52         8\n",
      "         298       1.00      1.00      1.00         2\n",
      "         301       1.00      0.67      0.80         3\n",
      "         303       0.60      0.75      0.67         4\n",
      "         304       0.00      0.00      0.00         2\n",
      "         305       1.00      1.00      1.00         2\n",
      "         306       1.00      1.00      1.00         2\n",
      "         307       0.18      1.00      0.31         2\n",
      "         308       1.00      1.00      1.00         2\n",
      "         309       1.00      1.00      1.00         2\n",
      "         311       0.57      0.46      0.51        28\n",
      "         313       0.00      0.00      0.00         2\n",
      "         316       1.00      0.50      0.67         2\n",
      "         321       1.00      0.50      0.67         2\n",
      "         322       0.00      0.00      0.00         2\n",
      "         323       0.50      1.00      0.67         2\n",
      "         324       1.00      1.00      1.00         2\n",
      "         325       1.00      1.00      1.00         2\n",
      "         326       0.79      0.39      0.52       165\n",
      "         327       0.00      0.00      0.00         4\n",
      "         329       1.00      0.50      0.67         2\n",
      "         330       0.75      0.25      0.38        12\n",
      "         331       0.50      0.50      0.50         2\n",
      "         332       0.39      0.28      0.33        32\n",
      "         333       1.00      1.00      1.00         2\n",
      "         336       1.00      0.50      0.67         2\n",
      "         337       0.00      0.00      0.00         2\n",
      "         338       1.00      1.00      1.00         2\n",
      "         339       1.00      0.50      0.67         2\n",
      "         340       1.00      1.00      1.00         2\n",
      "         342       0.00      0.00      0.00         1\n",
      "         345       1.00      0.92      0.96        13\n",
      "         349       0.00      0.00      0.00         1\n",
      "         350       1.00      1.00      1.00         1\n",
      "         351       1.00      1.00      1.00         1\n",
      "         352       1.00      1.00      1.00         1\n",
      "         353       1.00      1.00      1.00         1\n",
      "         354       1.00      1.00      1.00         1\n",
      "         355       1.00      1.00      1.00         1\n",
      "         356       1.00      1.00      1.00         1\n",
      "         357       1.00      0.27      0.43        11\n",
      "         361       0.00      0.00      0.00         1\n",
      "         362       0.48      0.64      0.55        83\n",
      "         363       0.60      0.75      0.67         4\n",
      "         370       1.00      1.00      1.00         1\n",
      "         373       1.00      1.00      1.00         1\n",
      "         377       1.00      0.23      0.38        13\n",
      "         379       1.00      0.64      0.78        14\n",
      "         382       0.77      0.57      0.65        30\n",
      "         383       1.00      1.00      1.00         3\n",
      "         384       1.00      1.00      1.00         1\n",
      "         391       1.00      1.00      1.00         1\n",
      "         392       1.00      1.00      1.00         1\n",
      "         393       1.00      1.00      1.00         1\n",
      "         395       1.00      1.00      1.00         2\n",
      "         398       1.00      1.00      1.00         1\n",
      "         399       0.81      1.00      0.90        13\n",
      "         400       1.00      1.00      1.00         1\n",
      "         401       1.00      1.00      1.00         1\n",
      "         402       1.00      1.00      1.00         1\n",
      "         403       1.00      1.00      1.00         1\n",
      "         404       1.00      1.00      1.00         4\n",
      "         405       1.00      1.00      1.00         1\n",
      "         414       0.86      1.00      0.92         6\n",
      "         415       0.83      0.83      0.83         6\n",
      "         419       1.00      1.00      1.00         1\n",
      "         420       0.00      0.00      0.00        10\n",
      "         421       0.86      0.53      0.65        34\n",
      "         428       1.00      1.00      1.00        14\n",
      "         434       1.00      1.00      1.00         1\n",
      "         435       0.50      1.00      0.67         1\n",
      "         438       0.00      0.00      0.00         1\n",
      "         439       1.00      0.83      0.91         6\n",
      "         440       0.96      0.61      0.75        36\n",
      "         441       1.00      1.00      1.00         2\n",
      "         443       1.00      0.60      0.75         5\n",
      "         445       1.00      1.00      1.00         1\n",
      "         448       1.00      1.00      1.00         1\n",
      "         456       0.83      0.62      0.71         8\n",
      "         457       1.00      1.00      1.00         1\n",
      "         458       0.81      0.90      0.85        96\n",
      "         459       1.00      0.40      0.57         5\n",
      "\n",
      "    accuracy                           0.82    130369\n",
      "   macro avg       0.81      0.65      0.69    130369\n",
      "weighted avg       0.83      0.82      0.82    130369\n",
      "\n",
      "confusion matrix:\n",
      "[[19695   822    84 ...     0     1     0]\n",
      " [  903 13884    17 ...     0     0     0]\n",
      " [  128    12 11673 ...     0     0     0]\n",
      " ...\n",
      " [    0     0     0 ...     1     0     0]\n",
      " [    1     0     0 ...     0    86     0]\n",
      " [    2     0     0 ...     0     0     2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "print(\"classfication report:\")\n",
    "print(classification_report(dataY, y_p_rnn))\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(dataY, y_p_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f2a91df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[19695   822    84 ...     0     1     0]\n",
      " [  903 13884    17 ...     0     0     0]\n",
      " [  128    12 11673 ...     0     0     0]\n",
      " ...\n",
      " [    0     0     0 ...     1     0     0]\n",
      " [    1     0     0 ...     0    86     0]\n",
      " [    2     0     0 ...     0     0     2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(dataY, y_p_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "8c994e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  82.2  %\n",
      "f1_score :  0.816\n"
     ]
    }
   ],
   "source": [
    "MNB_f1 = round(f1_score(dataY, y_p_rnn, average='weighted'), 3)\n",
    "MNB_accuracy = round((accuracy_score(dataY, y_p_rnn)*100),2)\n",
    "\n",
    "\n",
    "print(\"Accuracy : \" , MNB_accuracy , \" %\")\n",
    "print(\"f1_score : \" , MNB_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff3382be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classfication report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.90      0.91     21583\n",
      "           2       0.92      0.93      0.92     15548\n",
      "           3       0.99      0.99      0.99     12046\n",
      "           4       0.98      0.96      0.97     12046\n",
      "           5       0.97      0.93      0.95      5580\n",
      "           6       0.96      0.99      0.98      4521\n",
      "           7       0.96      0.95      0.95      3666\n",
      "           8       0.93      0.94      0.94      2866\n",
      "           9       0.92      0.90      0.91      1417\n",
      "          10       0.96      0.36      0.52        75\n",
      "          11       0.75      0.75      0.75      1703\n",
      "          13       0.90      0.89      0.90      2397\n",
      "          14       0.92      0.99      0.96      2397\n",
      "          15       0.90      0.84      0.87      2075\n",
      "          16       0.87      0.90      0.89      1381\n",
      "          17       0.90      0.94      0.92      2114\n",
      "          18       0.67      0.85      0.75      2091\n",
      "          19       0.86      0.86      0.86      1843\n",
      "          20       0.97      0.95      0.96      1570\n",
      "          21       0.95      0.97      0.96      1720\n",
      "          22       0.63      0.65      0.64      1170\n",
      "          23       0.81      0.86      0.83      1619\n",
      "          24       0.96      0.95      0.96       966\n",
      "          26       0.77      0.81      0.79      1495\n",
      "          27       0.86      0.64      0.73       868\n",
      "          28       1.00      0.99      0.99      1229\n",
      "          29       0.94      0.81      0.87      1202\n",
      "          30       0.97      0.88      0.92       866\n",
      "          31       0.96      0.88      0.92       964\n",
      "          32       0.88      0.63      0.73       450\n",
      "          33       0.98      0.99      0.98       428\n",
      "          34       0.64      0.78      0.70       808\n",
      "          35       0.95      0.94      0.95       585\n",
      "          36       0.98      0.98      0.98       856\n",
      "          37       0.98      0.92      0.95       557\n",
      "          38       0.80      0.83      0.81       798\n",
      "          39       0.91      0.69      0.79       763\n",
      "          40       0.97      0.91      0.94       705\n",
      "          41       0.81      0.79      0.80       317\n",
      "          42       0.84      0.88      0.86       655\n",
      "          43       0.00      0.00      0.00         1\n",
      "          44       0.98      0.86      0.92        64\n",
      "          45       0.70      0.58      0.64       417\n",
      "          46       0.95      0.88      0.91       431\n",
      "          47       0.42      0.68      0.52       116\n",
      "          48       0.25      0.91      0.39       529\n",
      "          49       0.66      0.85      0.74       484\n",
      "          50       0.71      0.85      0.77       420\n",
      "          51       0.81      0.60      0.69       293\n",
      "          52       0.86      0.79      0.82       358\n",
      "          53       0.94      0.83      0.88       361\n",
      "          54       0.75      0.56      0.64       361\n",
      "          55       0.97      0.98      0.98       355\n",
      "          56       1.00      0.76      0.86       348\n",
      "          57       0.87      0.89      0.88       346\n",
      "          58       0.61      0.86      0.71       340\n",
      "          59       0.82      0.56      0.66       317\n",
      "          60       0.84      0.81      0.83       209\n",
      "          61       0.94      0.70      0.80       261\n",
      "          62       0.90      0.72      0.80        50\n",
      "          63       1.00      0.62      0.77         8\n",
      "          64       0.93      0.87      0.90        31\n",
      "          65       1.00      1.00      1.00        31\n",
      "          66       0.80      0.62      0.70        13\n",
      "          68       0.88      0.76      0.82       160\n",
      "          69       0.64      0.85      0.73       239\n",
      "          70       0.93      0.50      0.65       260\n",
      "          71       0.94      0.72      0.81       170\n",
      "          72       1.00      0.85      0.92        20\n",
      "          73       0.91      0.79      0.85       165\n",
      "          74       0.79      0.55      0.65       220\n",
      "          75       0.89      0.73      0.80       123\n",
      "          76       0.77      0.79      0.78       129\n",
      "          77       0.98      0.98      0.98       208\n",
      "          78       0.61      0.89      0.72       207\n",
      "          79       1.00      1.00      1.00         7\n",
      "          80       0.99      0.91      0.95       171\n",
      "          81       0.91      0.73      0.81        96\n",
      "          82       0.97      0.96      0.97       161\n",
      "          83       0.88      0.60      0.71       116\n",
      "          85       1.00      1.00      1.00         6\n",
      "          86       0.83      0.81      0.82       138\n",
      "          87       0.83      0.52      0.64       141\n",
      "          88       0.87      0.96      0.92       140\n",
      "          89       1.00      0.20      0.33         5\n",
      "          90       0.94      0.97      0.96       136\n",
      "          91       0.93      0.96      0.95       135\n",
      "          93       0.94      0.83      0.88       116\n",
      "          94       0.87      0.80      0.83        90\n",
      "          95       0.91      0.96      0.94       122\n",
      "          96       0.87      0.71      0.78       110\n",
      "          97       0.80      0.55      0.65        65\n",
      "          98       0.96      0.80      0.87        93\n",
      "          99       0.75      0.75      0.75         4\n",
      "         100       0.82      0.96      0.88       104\n",
      "         101       0.73      0.78      0.76        83\n",
      "         102       0.97      0.93      0.95        96\n",
      "         103       0.98      1.00      0.99        97\n",
      "         104       0.75      0.78      0.77        51\n",
      "         105       0.87      0.75      0.80        71\n",
      "         106       0.68      0.92      0.78        83\n",
      "         107       0.82      0.86      0.84        83\n",
      "         108       0.94      0.70      0.80        70\n",
      "         110       0.90      0.75      0.82        69\n",
      "         113       0.96      0.39      0.56        56\n",
      "         114       0.77      0.64      0.70        53\n",
      "         115       0.63      0.89      0.74        56\n",
      "         116       0.67      0.61      0.64        56\n",
      "         117       0.75      0.78      0.76        60\n",
      "         118       0.97      0.63      0.77        52\n",
      "         119       0.58      1.00      0.73        26\n",
      "         120       1.00      0.76      0.86        49\n",
      "         121       0.98      0.90      0.94        59\n",
      "         122       1.00      0.82      0.90        17\n",
      "         123       0.86      0.66      0.75        56\n",
      "         124       0.86      0.89      0.88        36\n",
      "         125       0.94      0.94      0.94        34\n",
      "         126       0.85      0.97      0.91        36\n",
      "         127       0.94      0.89      0.91        18\n",
      "         128       0.80      0.75      0.78        53\n",
      "         129       0.77      0.60      0.67        50\n",
      "         130       0.97      0.85      0.90        39\n",
      "         131       0.42      0.88      0.56        42\n",
      "         132       0.91      0.77      0.83        13\n",
      "         134       0.88      0.88      0.88        41\n",
      "         135       0.83      0.64      0.72        39\n",
      "         136       1.00      0.76      0.86        29\n",
      "         137       1.00      0.48      0.65        21\n",
      "         138       0.92      0.76      0.83        29\n",
      "         139       1.00      0.88      0.93         8\n",
      "         140       0.93      0.79      0.86        34\n",
      "         141       0.37      0.85      0.51        34\n",
      "         142       0.56      0.59      0.57        17\n",
      "         144       0.97      1.00      0.99        35\n",
      "         145       1.00      0.88      0.94        26\n",
      "         146       0.63      0.79      0.70        28\n",
      "         147       1.00      1.00      1.00         7\n",
      "         148       0.91      0.95      0.93        21\n",
      "         149       0.84      0.70      0.76        23\n",
      "         150       0.70      0.84      0.76        31\n",
      "         151       0.75      0.56      0.64        32\n",
      "         152       0.96      0.88      0.92        25\n",
      "         155       1.00      0.73      0.84        11\n",
      "         158       0.86      0.83      0.85        30\n",
      "         159       1.00      0.80      0.89        15\n",
      "         160       0.86      0.68      0.76        28\n",
      "         161       1.00      0.95      0.98        21\n",
      "         162       0.70      0.70      0.70        27\n",
      "         163       0.91      0.84      0.87        25\n",
      "         164       1.00      0.20      0.33         5\n",
      "         165       0.93      0.70      0.80        20\n",
      "         166       0.94      0.94      0.94        18\n",
      "         167       1.00      0.93      0.97        15\n",
      "         168       0.77      0.77      0.77        26\n",
      "         169       0.84      0.62      0.71        26\n",
      "         170       0.89      0.70      0.78        23\n",
      "         172       1.00      0.50      0.67         4\n",
      "         173       0.79      0.92      0.85        12\n",
      "         175       1.00      0.55      0.71        11\n",
      "         176       1.00      0.64      0.78        14\n",
      "         177       1.00      0.64      0.78        22\n",
      "         179       0.36      1.00      0.53         4\n",
      "         180       0.80      0.70      0.74        23\n",
      "         181       1.00      0.75      0.86         8\n",
      "         182       1.00      1.00      1.00        10\n",
      "         183       0.61      0.95      0.74        21\n",
      "         184       1.00      0.56      0.72        16\n",
      "         185       1.00      0.33      0.50         9\n",
      "         187       0.89      1.00      0.94         8\n",
      "         188       0.95      0.95      0.95        20\n",
      "         190       1.00      1.00      1.00         1\n",
      "         191       0.90      1.00      0.95        19\n",
      "         192       1.00      0.53      0.69        17\n",
      "         193       1.00      0.53      0.69        19\n",
      "         194       0.94      0.89      0.91        18\n",
      "         195       1.00      0.47      0.64        15\n",
      "         196       0.75      1.00      0.86         3\n",
      "         197       1.00      0.82      0.90        11\n",
      "         198       0.85      0.85      0.85        13\n",
      "         199       1.00      1.00      1.00        16\n",
      "         200       0.71      0.77      0.74        13\n",
      "         201       1.00      0.44      0.62         9\n",
      "         202       1.00      0.83      0.91        12\n",
      "         203       0.75      0.50      0.60         6\n",
      "         204       1.00      0.11      0.20        18\n",
      "         205       1.00      0.69      0.81        16\n",
      "         206       0.88      0.47      0.61        15\n",
      "         207       1.00      1.00      1.00         3\n",
      "         208       1.00      0.65      0.79        17\n",
      "         209       1.00      1.00      1.00        13\n",
      "         210       0.82      0.88      0.85        16\n",
      "         211       1.00      1.00      1.00        16\n",
      "         212       1.00      1.00      1.00        16\n",
      "         213       0.75      0.60      0.67        15\n",
      "         214       1.00      0.80      0.89        15\n",
      "         215       0.89      0.57      0.70        14\n",
      "         216       1.00      0.64      0.78        11\n",
      "         217       0.60      0.80      0.69        15\n",
      "         218       1.00      0.58      0.74        12\n",
      "         220       1.00      1.00      1.00         1\n",
      "         221       1.00      0.67      0.80         3\n",
      "         222       1.00      0.86      0.92        14\n",
      "         223       1.00      0.86      0.92        14\n",
      "         225       0.75      0.50      0.60        12\n",
      "         226       1.00      0.83      0.91         6\n",
      "         228       0.75      0.60      0.67         5\n",
      "         229       1.00      0.69      0.82        13\n",
      "         230       1.00      0.67      0.80         3\n",
      "         231       1.00      1.00      1.00        13\n",
      "         233       0.00      0.00      0.00         2\n",
      "         234       1.00      1.00      1.00        12\n",
      "         235       1.00      0.10      0.18        10\n",
      "         236       1.00      1.00      1.00        10\n",
      "         237       1.00      1.00      1.00         8\n",
      "         238       0.71      1.00      0.83         5\n",
      "         239       0.75      0.75      0.75         4\n",
      "         240       1.00      0.58      0.74        12\n",
      "         241       0.85      0.92      0.88        12\n",
      "         242       1.00      0.33      0.50         6\n",
      "         243       1.00      0.58      0.74        12\n",
      "         244       1.00      0.80      0.89        10\n",
      "         248       1.00      0.86      0.92         7\n",
      "         249       0.92      0.92      0.92        12\n",
      "         250       1.00      1.00      1.00         4\n",
      "         251       1.00      0.90      0.95        10\n",
      "         252       1.00      1.00      1.00         1\n",
      "         253       1.00      1.00      1.00         1\n",
      "         254       1.00      1.00      1.00        10\n",
      "         255       1.00      0.78      0.88         9\n",
      "         257       1.00      1.00      1.00         6\n",
      "         258       1.00      0.50      0.67         2\n",
      "         259       1.00      0.73      0.84        11\n",
      "         260       1.00      1.00      1.00         3\n",
      "         261       1.00      0.86      0.92         7\n",
      "         262       1.00      0.50      0.67         2\n",
      "         263       1.00      0.50      0.67         2\n",
      "         264       0.90      1.00      0.95         9\n",
      "         266       1.00      0.80      0.89        10\n",
      "         268       1.00      0.90      0.95        10\n",
      "         269       0.90      1.00      0.95         9\n",
      "         270       1.00      0.71      0.83         7\n",
      "         273       1.00      0.80      0.89        10\n",
      "         279       0.86      0.75      0.80         8\n",
      "         280       1.00      0.62      0.77         8\n",
      "         282       1.00      1.00      1.00         2\n",
      "         283       0.56      0.83      0.67         6\n",
      "         284       1.00      0.78      0.88         9\n",
      "         285       0.80      0.89      0.84         9\n",
      "         286       1.00      1.00      1.00         3\n",
      "         287       1.00      0.67      0.80         9\n",
      "         288       1.00      1.00      1.00         5\n",
      "         289       1.00      0.71      0.83         7\n",
      "         291       0.90      1.00      0.95         9\n",
      "         292       1.00      0.89      0.94         9\n",
      "         293       1.00      1.00      1.00         9\n",
      "         294       1.00      1.00      1.00         9\n",
      "         295       1.00      1.00      1.00         9\n",
      "         296       1.00      1.00      1.00         7\n",
      "         298       1.00      0.50      0.67         2\n",
      "         299       1.00      0.88      0.93         8\n",
      "         300       1.00      1.00      1.00         7\n",
      "         301       1.00      1.00      1.00         7\n",
      "         303       1.00      0.88      0.93         8\n",
      "         304       1.00      0.50      0.67         8\n",
      "         305       1.00      0.50      0.67         2\n",
      "         306       1.00      0.71      0.83         7\n",
      "         307       0.80      1.00      0.89         4\n",
      "         308       1.00      0.50      0.67         6\n",
      "         309       0.67      1.00      0.80         6\n",
      "         310       1.00      0.88      0.93         8\n",
      "         311       1.00      1.00      1.00         8\n",
      "         313       0.83      1.00      0.91         5\n",
      "         314       1.00      0.50      0.67         8\n",
      "         315       0.62      1.00      0.76         8\n",
      "         318       1.00      0.50      0.67         2\n",
      "         319       0.86      0.75      0.80         8\n",
      "         320       1.00      0.38      0.55         8\n",
      "         321       1.00      0.25      0.40         4\n",
      "         322       1.00      0.50      0.67         8\n",
      "         323       0.67      0.29      0.40         7\n",
      "         324       1.00      1.00      1.00         6\n",
      "         327       1.00      0.86      0.92         7\n",
      "         328       1.00      0.83      0.91         6\n",
      "         329       1.00      0.57      0.73         7\n",
      "         330       0.71      1.00      0.83         5\n",
      "         331       1.00      1.00      1.00         6\n",
      "         332       1.00      1.00      1.00         7\n",
      "         333       1.00      0.83      0.91         6\n",
      "         334       1.00      0.83      0.91         6\n",
      "         335       1.00      1.00      1.00         6\n",
      "         336       1.00      1.00      1.00         4\n",
      "         337       1.00      0.57      0.73         7\n",
      "         338       1.00      0.33      0.50         3\n",
      "         339       1.00      0.86      0.92         7\n",
      "         340       0.88      1.00      0.93         7\n",
      "         341       1.00      0.33      0.50         3\n",
      "         342       1.00      0.33      0.50         6\n",
      "         343       1.00      1.00      1.00         2\n",
      "         344       1.00      0.71      0.83         7\n",
      "         345       1.00      1.00      1.00         4\n",
      "         346       0.83      1.00      0.91         5\n",
      "         347       0.40      0.33      0.36         6\n",
      "         348       1.00      1.00      1.00         5\n",
      "         349       1.00      0.67      0.80         6\n",
      "         350       1.00      0.80      0.89         5\n",
      "         352       1.00      1.00      1.00         6\n",
      "         353       0.29      0.83      0.43         6\n",
      "         354       1.00      0.80      0.89         5\n",
      "         355       1.00      0.33      0.50         6\n",
      "         356       1.00      1.00      1.00         6\n",
      "         357       1.00      0.75      0.86         4\n",
      "         358       1.00      1.00      1.00         6\n",
      "         359       1.00      1.00      1.00         6\n",
      "         360       1.00      0.67      0.80         6\n",
      "         362       1.00      1.00      1.00         1\n",
      "         363       1.00      1.00      1.00         6\n",
      "         364       1.00      0.50      0.67         2\n",
      "         365       1.00      0.67      0.80         6\n",
      "         366       1.00      1.00      1.00         2\n",
      "         370       1.00      0.83      0.91         6\n",
      "         371       1.00      1.00      1.00         2\n",
      "         372       1.00      0.33      0.50         6\n",
      "         373       1.00      0.67      0.80         3\n",
      "         374       0.83      1.00      0.91         5\n",
      "         375       1.00      1.00      1.00         6\n",
      "         377       1.00      0.67      0.80         6\n",
      "         379       1.00      0.50      0.67         6\n",
      "         380       1.00      0.50      0.67         6\n",
      "         382       1.00      1.00      1.00         2\n",
      "         384       0.00      0.00      0.00         4\n",
      "         385       0.60      0.60      0.60         5\n",
      "         386       0.31      0.80      0.44         5\n",
      "         387       1.00      0.67      0.80         3\n",
      "         389       1.00      1.00      1.00         4\n",
      "         391       1.00      0.40      0.57         5\n",
      "         392       1.00      0.67      0.80         3\n",
      "         393       1.00      1.00      1.00         5\n",
      "         394       0.38      0.75      0.50         4\n",
      "         395       0.71      1.00      0.83         5\n",
      "         396       1.00      0.50      0.67         2\n",
      "         397       0.67      1.00      0.80         2\n",
      "         401       1.00      0.60      0.75         5\n",
      "         405       0.60      0.60      0.60         5\n",
      "         407       1.00      1.00      1.00         5\n",
      "         408       0.60      0.60      0.60         5\n",
      "         410       1.00      1.00      1.00         4\n",
      "         411       1.00      1.00      1.00         4\n",
      "         414       1.00      1.00      1.00         2\n",
      "         415       1.00      1.00      1.00         4\n",
      "         416       1.00      1.00      1.00         4\n",
      "         417       1.00      0.50      0.67         2\n",
      "         418       1.00      0.33      0.50         3\n",
      "         419       1.00      0.50      0.67         4\n",
      "         420       0.30      0.75      0.43         4\n",
      "         424       1.00      1.00      1.00         4\n",
      "         425       1.00      0.75      0.86         4\n",
      "         426       1.00      0.50      0.67         4\n",
      "         427       1.00      0.50      0.67         4\n",
      "         430       1.00      1.00      1.00         2\n",
      "         431       1.00      0.75      0.86         4\n",
      "         432       1.00      0.67      0.80         3\n",
      "         433       0.67      1.00      0.80         4\n",
      "         434       1.00      0.50      0.67         4\n",
      "         435       1.00      1.00      1.00         4\n",
      "         437       1.00      0.75      0.86         4\n",
      "         438       1.00      0.50      0.67         4\n",
      "         441       0.00      0.00      0.00         4\n",
      "         444       1.00      0.33      0.50         3\n",
      "         445       1.00      0.75      0.86         4\n",
      "         446       1.00      1.00      1.00         4\n",
      "         447       1.00      0.50      0.67         2\n",
      "         451       0.38      0.75      0.50         4\n",
      "         453       1.00      1.00      1.00         4\n",
      "         454       1.00      0.75      0.86         4\n",
      "         455       1.00      1.00      1.00         4\n",
      "         456       1.00      1.00      1.00         4\n",
      "         457       1.00      1.00      1.00         4\n",
      "         459       1.00      1.00      1.00         4\n",
      "         460       1.00      0.50      0.67         4\n",
      "         461       0.75      0.75      0.75         4\n",
      "         462       1.00      1.00      1.00         4\n",
      "         463       1.00      1.00      1.00         4\n",
      "         464       1.00      1.00      1.00         4\n",
      "         465       1.00      1.00      1.00         4\n",
      "         466       0.67      0.50      0.57         4\n",
      "         467       1.00      0.50      0.67         2\n",
      "         468       1.00      0.75      0.86         4\n",
      "         469       1.00      1.00      1.00         4\n",
      "         470       1.00      0.50      0.67         4\n",
      "         471       1.00      0.25      0.40         4\n",
      "         473       1.00      1.00      1.00         4\n",
      "         474       1.00      0.67      0.80         3\n",
      "         475       1.00      0.75      0.86         4\n",
      "         476       1.00      0.50      0.67         4\n",
      "         477       1.00      0.75      0.86         4\n",
      "         478       1.00      0.75      0.86         4\n",
      "         479       1.00      0.67      0.80         3\n",
      "         480       1.00      1.00      1.00         4\n",
      "         481       1.00      0.75      0.86         4\n",
      "         482       1.00      1.00      1.00         4\n",
      "         483       1.00      0.75      0.86         4\n",
      "         485       1.00      1.00      1.00         4\n",
      "         486       1.00      1.00      1.00         3\n",
      "         487       1.00      1.00      1.00         3\n",
      "         488       1.00      1.00      1.00         1\n",
      "         489       1.00      1.00      1.00         1\n",
      "         490       1.00      1.00      1.00         3\n",
      "         491       1.00      1.00      1.00         3\n",
      "         492       1.00      1.00      1.00         3\n",
      "         493       1.00      1.00      1.00         3\n",
      "         494       0.75      1.00      0.86         3\n",
      "         495       1.00      0.67      0.80         3\n",
      "         497       1.00      1.00      1.00         2\n",
      "         502       1.00      0.67      0.80         3\n",
      "         504       1.00      1.00      1.00         3\n",
      "         506       1.00      0.33      0.50         3\n",
      "         508       1.00      1.00      1.00         2\n",
      "         509       1.00      0.67      0.80         3\n",
      "         510       1.00      0.67      0.80         3\n",
      "         511       1.00      1.00      1.00         3\n",
      "         512       1.00      1.00      1.00         3\n",
      "         513       1.00      0.67      0.80         3\n",
      "         516       1.00      0.67      0.80         3\n",
      "         517       1.00      1.00      1.00         3\n",
      "         518       1.00      0.67      0.80         3\n",
      "         519       0.67      0.67      0.67         3\n",
      "         522       1.00      1.00      1.00         3\n",
      "         523       1.00      0.67      0.80         3\n",
      "         524       1.00      0.50      0.67         2\n",
      "         526       1.00      0.67      0.80         3\n",
      "         529       1.00      0.33      0.50         3\n",
      "         531       0.00      0.00      0.00         2\n",
      "         532       1.00      0.50      0.67         2\n",
      "         533       1.00      0.67      0.80         3\n",
      "         534       1.00      1.00      1.00         3\n",
      "         537       1.00      0.67      0.80         3\n",
      "         539       1.00      0.67      0.80         3\n",
      "         549       0.60      1.00      0.75         3\n",
      "         551       0.75      1.00      0.86         3\n",
      "         553       1.00      0.33      0.50         3\n",
      "         554       1.00      1.00      1.00         3\n",
      "         555       1.00      1.00      1.00         3\n",
      "         556       0.00      0.00      0.00         1\n",
      "         557       0.75      1.00      0.86         3\n",
      "         560       0.13      1.00      0.23         3\n",
      "         561       1.00      0.33      0.50         3\n",
      "         562       1.00      0.67      0.80         3\n",
      "         563       0.50      1.00      0.67         3\n",
      "         564       1.00      0.33      0.50         3\n",
      "         565       1.00      1.00      1.00         3\n",
      "         566       1.00      0.67      0.80         3\n",
      "         567       1.00      1.00      1.00         3\n",
      "         570       1.00      1.00      1.00         3\n",
      "         572       1.00      0.67      0.80         3\n",
      "         573       1.00      1.00      1.00         3\n",
      "         574       1.00      0.67      0.80         3\n",
      "         577       1.00      1.00      1.00         3\n",
      "         579       1.00      0.67      0.80         3\n",
      "         582       0.60      1.00      0.75         3\n",
      "         583       1.00      0.67      0.80         3\n",
      "         586       0.00      0.00      0.00         2\n",
      "         588       1.00      0.50      0.67         2\n",
      "         590       1.00      1.00      1.00         2\n",
      "         592       1.00      1.00      1.00         2\n",
      "         593       0.67      1.00      0.80         2\n",
      "         594       1.00      1.00      1.00         1\n",
      "         595       0.50      1.00      0.67         2\n",
      "         596       0.00      0.00      0.00         2\n",
      "         597       1.00      0.50      0.67         2\n",
      "         598       1.00      0.50      0.67         2\n",
      "         600       1.00      0.50      0.67         2\n",
      "         603       1.00      1.00      1.00         2\n",
      "         604       1.00      1.00      1.00         1\n",
      "         605       1.00      1.00      1.00         1\n",
      "         606       1.00      1.00      1.00         2\n",
      "         607       1.00      1.00      1.00         2\n",
      "         608       0.00      0.00      0.00         2\n",
      "         614       1.00      0.50      0.67         2\n",
      "         615       0.00      0.00      0.00         2\n",
      "         617       1.00      1.00      1.00         2\n",
      "         621       1.00      1.00      1.00         2\n",
      "         624       0.00      0.00      0.00         2\n",
      "         625       1.00      1.00      1.00         2\n",
      "         626       1.00      1.00      1.00         2\n",
      "         627       1.00      1.00      1.00         2\n",
      "         628       1.00      0.50      0.67         2\n",
      "         633       0.50      1.00      0.67         2\n",
      "         634       1.00      1.00      1.00         2\n",
      "         635       1.00      0.50      0.67         2\n",
      "         636       1.00      1.00      1.00         2\n",
      "         637       1.00      0.50      0.67         2\n",
      "         638       1.00      1.00      1.00         2\n",
      "         639       1.00      0.50      0.67         2\n",
      "         641       1.00      1.00      1.00         2\n",
      "         642       0.67      1.00      0.80         2\n",
      "         643       1.00      1.00      1.00         2\n",
      "         645       0.40      1.00      0.57         2\n",
      "         646       1.00      1.00      1.00         2\n",
      "         647       1.00      1.00      1.00         2\n",
      "         648       1.00      1.00      1.00         2\n",
      "         649       1.00      1.00      1.00         1\n",
      "         659       1.00      1.00      1.00         2\n",
      "         660       1.00      1.00      1.00         2\n",
      "         661       1.00      1.00      1.00         2\n",
      "         662       1.00      0.50      0.67         2\n",
      "         663       0.00      0.00      0.00         2\n",
      "         667       1.00      0.50      0.67         2\n",
      "         669       0.00      0.00      0.00         2\n",
      "         670       1.00      1.00      1.00         2\n",
      "         671       1.00      1.00      1.00         2\n",
      "         675       1.00      1.00      1.00         2\n",
      "         676       1.00      1.00      1.00         2\n",
      "         679       1.00      1.00      1.00         1\n",
      "         680       1.00      1.00      1.00         2\n",
      "         681       1.00      1.00      1.00         1\n",
      "         683       0.67      1.00      0.80         2\n",
      "         684       1.00      1.00      1.00         2\n",
      "         685       1.00      1.00      1.00         2\n",
      "         686       1.00      1.00      1.00         2\n",
      "         687       0.33      0.50      0.40         2\n",
      "         688       1.00      1.00      1.00         2\n",
      "         690       1.00      1.00      1.00         2\n",
      "         692       1.00      1.00      1.00         2\n",
      "         693       0.00      0.00      0.00         2\n",
      "         694       1.00      1.00      1.00         2\n",
      "         695       1.00      0.50      0.67         2\n",
      "         696       1.00      1.00      1.00         2\n",
      "         697       1.00      1.00      1.00         2\n",
      "         700       1.00      0.50      0.67         2\n",
      "         701       1.00      1.00      1.00         2\n",
      "         702       1.00      1.00      1.00         2\n",
      "         703       1.00      1.00      1.00         2\n",
      "         704       1.00      1.00      1.00         2\n",
      "         705       1.00      1.00      1.00         2\n",
      "         707       0.00      0.00      0.00         2\n",
      "         709       1.00      1.00      1.00         2\n",
      "         713       0.00      0.00      0.00         2\n",
      "         714       1.00      0.50      0.67         2\n",
      "         715       1.00      1.00      1.00         2\n",
      "         716       1.00      1.00      1.00         2\n",
      "         718       1.00      1.00      1.00         2\n",
      "         719       1.00      0.50      0.67         2\n",
      "         722       1.00      0.50      0.67         2\n",
      "         723       1.00      0.50      0.67         2\n",
      "         724       1.00      1.00      1.00         2\n",
      "         725       1.00      0.50      0.67         2\n",
      "         726       1.00      1.00      1.00         2\n",
      "         741       1.00      1.00      1.00         1\n",
      "         742       1.00      1.00      1.00         1\n",
      "         744       1.00      1.00      1.00         1\n",
      "         747       1.00      1.00      1.00         1\n",
      "         750       1.00      1.00      1.00         1\n",
      "         751       1.00      1.00      1.00         1\n",
      "         752       1.00      1.00      1.00         1\n",
      "         753       1.00      1.00      1.00         1\n",
      "         754       1.00      1.00      1.00         1\n",
      "         755       0.50      1.00      0.67         1\n",
      "         756       0.00      0.00      0.00         1\n",
      "         764       0.00      0.00      0.00         1\n",
      "         765       1.00      1.00      1.00         1\n",
      "         766       1.00      1.00      1.00         1\n",
      "         767       1.00      1.00      1.00         1\n",
      "         768       1.00      1.00      1.00         1\n",
      "         774       0.00      0.00      0.00         1\n",
      "         775       0.00      0.00      0.00         1\n",
      "         779       1.00      1.00      1.00         1\n",
      "         792       0.00      0.00      0.00         1\n",
      "         801       0.00      0.00      0.00         1\n",
      "         802       0.50      1.00      0.67         1\n",
      "         803       1.00      1.00      1.00         1\n",
      "         804       1.00      1.00      1.00         1\n",
      "         806       0.00      0.00      0.00         1\n",
      "         807       0.50      1.00      0.67         1\n",
      "         813       0.00      0.00      0.00         1\n",
      "         817       1.00      1.00      1.00         1\n",
      "         819       0.00      0.00      0.00         1\n",
      "         827       0.00      0.00      0.00         1\n",
      "         837       0.17      1.00      0.29         1\n",
      "         847       1.00      1.00      1.00         1\n",
      "         848       1.00      1.00      1.00         1\n",
      "         852       1.00      1.00      1.00         1\n",
      "         856       1.00      1.00      1.00         1\n",
      "         871       1.00      1.00      1.00         1\n",
      "         872       1.00      1.00      1.00         1\n",
      "         873       1.00      1.00      1.00         1\n",
      "         874       1.00      1.00      1.00         1\n",
      "         875       1.00      1.00      1.00         1\n",
      "         876       1.00      1.00      1.00         1\n",
      "         892       1.00      1.00      1.00         1\n",
      "         899       1.00      1.00      1.00         1\n",
      "         910       1.00      1.00      1.00         1\n",
      "         914       1.00      1.00      1.00         1\n",
      "         917       1.00      1.00      1.00         1\n",
      "         921       0.00      0.00      0.00         1\n",
      "         922       1.00      1.00      1.00         1\n",
      "         923       1.00      1.00      1.00         1\n",
      "         924       1.00      1.00      1.00         1\n",
      "         963       0.00      0.00      0.00         1\n",
      "         966       0.00      0.00      0.00         1\n",
      "         971       0.00      0.00      0.00         1\n",
      "         975       0.00      0.00      0.00         1\n",
      "         976       1.00      1.00      1.00         1\n",
      "         977       1.00      1.00      1.00         1\n",
      "         978       1.00      1.00      1.00         1\n",
      "         979       1.00      1.00      1.00         1\n",
      "         980       1.00      1.00      1.00         1\n",
      "         981       1.00      1.00      1.00         1\n",
      "         982       1.00      1.00      1.00         1\n",
      "         986       0.00      0.00      0.00         1\n",
      "         993       1.00      1.00      1.00         1\n",
      "         995       1.00      1.00      1.00         1\n",
      "        1001       1.00      1.00      1.00         1\n",
      "        1008       1.00      1.00      1.00         1\n",
      "        1009       1.00      1.00      1.00         1\n",
      "        1012       1.00      1.00      1.00         1\n",
      "        1013       1.00      1.00      1.00         1\n",
      "        1014       1.00      1.00      1.00         1\n",
      "        1015       1.00      1.00      1.00         1\n",
      "        1016       1.00      1.00      1.00         1\n",
      "        1034       1.00      1.00      1.00         1\n",
      "        1035       0.00      0.00      0.00         1\n",
      "        1037       0.00      0.00      0.00         1\n",
      "        1046       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.90    130369\n",
      "   macro avg       0.87      0.78      0.80    130369\n",
      "weighted avg       0.91      0.90      0.90    130369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, mean_absolute_error\n",
    "print(\"classfication report:\")\n",
    "print(classification_report(dataY, y_p_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9219b3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix:\n",
      "[[19402   361    22 ...     0     0     0]\n",
      " [  468 14402     0 ...     0     0     0]\n",
      " [   28     5 11881 ...     0     0     0]\n",
      " ...\n",
      " [    0     0     0 ...     0     0     0]\n",
      " [    0     0     0 ...     0     0     0]\n",
      " [    0     0     0 ...     0     0     1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(dataY, y_p_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "231d2b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  90.18  %\n",
      "f1_score :  0.904\n"
     ]
    }
   ],
   "source": [
    "MNB_f1 = round(f1_score(dataY, y_p_lstm, average='weighted'), 3)\n",
    "MNB_accuracy = round((accuracy_score(dataY, y_p_lstm)*100),2)\n",
    "\n",
    "\n",
    "print(\"Accuracy : \" , MNB_accuracy , \" %\")\n",
    "print(\"f1_score : \" , MNB_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc803788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Absolute Error:\", mean_absolute_error(dataY,y_p_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa8c5f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48, 5, 19, 3, 19, 3, 4, 4, 2, 11]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_p_rnn[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b64f3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['', '', '', '', '', '', '', '', '', ''],\n",
       " ['', '', '', '', '', '', '', '', '', 's'],\n",
       " ['', '', '', '', '', '', '', '', 's', '='],\n",
       " ['', '', '', '', '', '', '', 's', '=', 'list'],\n",
       " ['', '', '', '', '', '', 's', '=', 'list', '('],\n",
       " ['', '', '', '', '', 's', '=', 'list', '(', '<UNK>'],\n",
       " ['', '', '', '', 's', '=', 'list', '(', '<UNK>', '('],\n",
       " ['', '', '', 's', '=', 'list', '(', '<UNK>', '(', ')'],\n",
       " ['', '', 's', '=', 'list', '(', '<UNK>', '(', ')', ')'],\n",
       " ['', 's', '=', 'list', '(', '<UNK>', '(', ')', ')', '\\n']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a841553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import', '=', 'input', '(', 'input', '(', ')', ')', '\\n', 'a']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_rnn[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3ae5132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UNK>', '=', 'input', '(', 'input', '(', ')', ')', '\\n', 't']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lstm[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30fa9c46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['s', '=', 'list', '(', '<UNK>', '(', ')', ')', '\\n', 't'],\n",
       " ['=', 'list', '(', '<UNK>', '(', ')', ')', '\\n', 't', '='],\n",
       " ['list', '(', '<UNK>', '(', ')', ')', '\\n', 't', '=', '['],\n",
       " ['(', '<UNK>', '(', ')', ')', '\\n', 't', '=', '[', ']'],\n",
       " ['<UNK>', '(', ')', ')', '\\n', 't', '=', '[', ']', '\\n'],\n",
       " ['(', ')', ')', '\\n', 't', '=', '[', ']', '\\n', 'for'],\n",
       " [')', ')', '\\n', 't', '=', '[', ']', '\\n', 'for', 'e'],\n",
       " [')', '\\n', 't', '=', '[', ']', '\\n', 'for', 'e', 'in'],\n",
       " ['\\n', 't', '=', '[', ']', '\\n', 'for', 'e', 'in', 's'],\n",
       " ['t', '=', '[', ']', '\\n', 'for', 'e', 'in', 's', ':']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21864569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['=', '[', ']', '\\n', 'for', 'i', 'in', 'range', ':', '\\n']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_rnn[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f620e091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['=', '[', ']', '\\n', 'for', 'i', 'in', 's', ':', '\\n']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lstm[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e09c682",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/63/bhtnrx_s4qz9c77b076z4mpr0000gn/T/ipykernel_23491/2747965593.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#2nd Pie chart code for top 6 languages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlanguages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlanguage_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlanguages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mautopct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%1.2f%%'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#2nd Pie chart code for top 6 languages\n",
    "languages=df.language.value_counts().index\n",
    "language_values=df.language.value_counts().values\n",
    "plt.pie(language_values[:6],labels=languages[:6],autopct='%1.2f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a82677a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['=', '[', ']', '\\n', 'for', 'e', 'in', 's', ':', '\\n'],\n",
       " ['[', ']', '\\n', 'for', 'e', 'in', 's', ':', '\\n', '<UNK>'],\n",
       " [']', '\\n', 'for', 'e', 'in', 's', ':', '\\n', '<UNK>', 'if'],\n",
       " ['\\n', 'for', 'e', 'in', 's', ':', '\\n', '<UNK>', 'if', 'e'],\n",
       " ['for', 'e', 'in', 's', ':', '\\n', '<UNK>', 'if', 'e', 'is'],\n",
       " ['e', 'in', 's', ':', '\\n', '<UNK>', 'if', 'e', 'is', '<UNK>'],\n",
       " ['in', 's', ':', '\\n', '<UNK>', 'if', 'e', 'is', '<UNK>', ':'],\n",
       " ['s', ':', '\\n', '<UNK>', 'if', 'e', 'is', '<UNK>', ':', '\\n'],\n",
       " [':', '\\n', '<UNK>', 'if', 'e', 'is', '<UNK>', ':', '\\n', '<UNK>'],\n",
       " ['\\n', '<UNK>', 'if', 'e', 'is', '<UNK>', ':', '\\n', '<UNK>', 'if']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33f06332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UNK>', 'if', 'i', 'is', '<UNK>', ':', '\\n', '<UNK>', 'if', 't']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_rnn[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6bd00554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<UNK>', 'if', 'e', 'is', '<UNK>', ':', '\\n', '<UNK>', 'if', 't']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_lstm[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80fd52c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/63/bhtnrx_s4qz9c77b076z4mpr0000gn/T/ipykernel_5797/3086598270.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mean Absolute Error:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_p_rnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Mean Absolute Error:\", metrics.mean_absolute_error(dataY,y_p_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b1323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy: how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(dataY, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2ef9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# define dataset\n",
    "#X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# define the model\n",
    "model = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b9491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n",
      "/Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:684: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate random forest algorithm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# define dataset\n",
    "#X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=3)\n",
    "# define the model\n",
    "model = RandomForestClassifier()\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, dataX, dataY, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "# report performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f619ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
