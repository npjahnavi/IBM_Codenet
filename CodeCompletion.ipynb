{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cffa6f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jahnavinp/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout, LSTM, SimpleRNN, Embedding\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tokenize, glob\n",
    "import os, re, requests, tarfile, shutil, json, glob # For file manipualation only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd2ce810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "print(\"Tensorflow version \" + tf.__version__)\n",
    "\n",
    "try:\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "except ValueError:\n",
    "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
    "\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9f4e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    }
   ],
   "source": [
    "!wget https://dax-cdn.cdn.appdomain.cloud/dax-project-codenet/1.0.0/Project_CodeNet.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76b6e331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenInfo(type=1 (NAME), string='while', start=(1, 0), end=(1, 5), line='while True:\\n')\n",
      "TokenInfo(type=1 (NAME), string='True', start=(1, 6), end=(1, 10), line='while True:\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(1, 10), end=(1, 11), line='while True:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(1, 11), end=(1, 12), line='while True:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='    ', start=(2, 0), end=(2, 4), line='    try:\\n')\n",
      "TokenInfo(type=1 (NAME), string='try', start=(2, 4), end=(2, 7), line='    try:\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(2, 7), end=(2, 8), line='    try:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(2, 8), end=(2, 9), line='    try:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(3, 0), end=(3, 8), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='a', start=(3, 8), end=(3, 9), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=',', start=(3, 9), end=(3, 10), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='b', start=(3, 11), end=(3, 12), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='=', start=(3, 13), end=(3, 14), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='map', start=(3, 15), end=(3, 18), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='(', start=(3, 18), end=(3, 19), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='int', start=(3, 19), end=(3, 22), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=',', start=(3, 22), end=(3, 23), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='input', start=(3, 24), end=(3, 29), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='(', start=(3, 29), end=(3, 30), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=')', start=(3, 30), end=(3, 31), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='.', start=(3, 31), end=(3, 32), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=1 (NAME), string='split', start=(3, 32), end=(3, 37), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string='(', start=(3, 37), end=(3, 38), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=')', start=(3, 38), end=(3, 39), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=54 (OP), string=')', start=(3, 39), end=(3, 40), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(3, 40), end=(3, 41), line='        a, b = map(int, input().split())\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(4, 4), end=(4, 4), line='    except EOFError:\\n')\n",
      "TokenInfo(type=1 (NAME), string='except', start=(4, 4), end=(4, 10), line='    except EOFError:\\n')\n",
      "TokenInfo(type=1 (NAME), string='EOFError', start=(4, 11), end=(4, 19), line='    except EOFError:\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(4, 19), end=(4, 20), line='    except EOFError:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(4, 20), end=(4, 21), line='    except EOFError:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(5, 0), end=(5, 8), line='        break\\n')\n",
      "TokenInfo(type=1 (NAME), string='break', start=(5, 8), end=(5, 13), line='        break\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(5, 13), end=(5, 14), line='        break\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(6, 4), end=(6, 4), line='    count=1\\n')\n",
      "TokenInfo(type=1 (NAME), string='count', start=(6, 4), end=(6, 9), line='    count=1\\n')\n",
      "TokenInfo(type=54 (OP), string='=', start=(6, 9), end=(6, 10), line='    count=1\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1', start=(6, 10), end=(6, 11), line='    count=1\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(6, 11), end=(6, 12), line='    count=1\\n')\n",
      "TokenInfo(type=1 (NAME), string='k', start=(7, 4), end=(7, 5), line='    k=a+b\\n')\n",
      "TokenInfo(type=54 (OP), string='=', start=(7, 5), end=(7, 6), line='    k=a+b\\n')\n",
      "TokenInfo(type=1 (NAME), string='a', start=(7, 6), end=(7, 7), line='    k=a+b\\n')\n",
      "TokenInfo(type=54 (OP), string='+', start=(7, 7), end=(7, 8), line='    k=a+b\\n')\n",
      "TokenInfo(type=1 (NAME), string='b', start=(7, 8), end=(7, 9), line='    k=a+b\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(7, 9), end=(7, 10), line='    k=a+b\\n')\n",
      "TokenInfo(type=1 (NAME), string='while', start=(8, 4), end=(8, 9), line='    while k>=10:\\n')\n",
      "TokenInfo(type=1 (NAME), string='k', start=(8, 10), end=(8, 11), line='    while k>=10:\\n')\n",
      "TokenInfo(type=54 (OP), string='>=', start=(8, 11), end=(8, 13), line='    while k>=10:\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='10', start=(8, 13), end=(8, 15), line='    while k>=10:\\n')\n",
      "TokenInfo(type=54 (OP), string=':', start=(8, 15), end=(8, 16), line='    while k>=10:\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(8, 16), end=(8, 17), line='    while k>=10:\\n')\n",
      "TokenInfo(type=5 (INDENT), string='        ', start=(9, 0), end=(9, 8), line='        k//=10\\n')\n",
      "TokenInfo(type=1 (NAME), string='k', start=(9, 8), end=(9, 9), line='        k//=10\\n')\n",
      "TokenInfo(type=54 (OP), string='//=', start=(9, 9), end=(9, 12), line='        k//=10\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='10', start=(9, 12), end=(9, 14), line='        k//=10\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(9, 14), end=(9, 15), line='        k//=10\\n')\n",
      "TokenInfo(type=1 (NAME), string='count', start=(10, 8), end=(10, 13), line='        count+=1\\n')\n",
      "TokenInfo(type=54 (OP), string='+=', start=(10, 13), end=(10, 15), line='        count+=1\\n')\n",
      "TokenInfo(type=2 (NUMBER), string='1', start=(10, 15), end=(10, 16), line='        count+=1\\n')\n",
      "TokenInfo(type=4 (NEWLINE), string='\\n', start=(10, 16), end=(10, 17), line='        count+=1\\n')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(11, 4), end=(11, 4), line='    print(count)')\n",
      "TokenInfo(type=1 (NAME), string='print', start=(11, 4), end=(11, 9), line='    print(count)')\n",
      "TokenInfo(type=54 (OP), string='(', start=(11, 9), end=(11, 10), line='    print(count)')\n",
      "TokenInfo(type=1 (NAME), string='count', start=(11, 10), end=(11, 15), line='    print(count)')\n",
      "TokenInfo(type=54 (OP), string=')', start=(11, 15), end=(11, 16), line='    print(count)')\n",
      "TokenInfo(type=4 (NEWLINE), string='', start=(11, 16), end=(11, 17), line='')\n",
      "TokenInfo(type=6 (DEDENT), string='', start=(12, 0), end=(12, 0), line='')\n",
      "TokenInfo(type=0 (ENDMARKER), string='', start=(12, 0), end=(12, 0), line='')\n"
     ]
    }
   ],
   "source": [
    "# Experiment with tokenize library to see its use\n",
    "with tokenize.open('data/p00002/Python/s005515634.py') as f:\n",
    "    tokens = tokenize.generate_tokens(f.readline)\n",
    "    for token in tokens:\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00e2d40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: '/all_py_files.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/63/bhtnrx_s4qz9c77b076z4mpr0000gn/T/ipykernel_21398/1686773937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mall_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/*/Python/*.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/all_py_files.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'%s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[0;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[1;32m   1501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# datasource doesn't support creating a new file ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1503\u001b[0;31m         \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1504\u001b[0m         \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0mown_fh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/all_py_files.txt'"
     ]
    }
   ],
   "source": [
    "all_files = glob.glob('data/*/Python/*.py')\n",
    "print(len(all_files))\n",
    "np.savetxt('/all_py_files.txt', X = all_files, delimiter=',', comments='', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "facbd15f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files_mini = glob.glob('data/*/Python/*.py')\n",
    "np.savetxt('all_py_files.txt', X = all_files_mini, delimiter=',', comments='', fmt='%s')\n",
    "len(all_files_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ff943d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokenizer vocabulary 2091\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_save(dir_name):\n",
    "  tokenizerD = Tokenizer(\n",
    "    num_words=10000,\n",
    "    filters='',\n",
    "    lower=False, split=' ', char_level=False, oov_token=\"<UNK>\",\n",
    "    document_count=0\n",
    "  )\n",
    "  \n",
    "  # Assumes a file with all_py_files.txt name\n",
    "  file_index = 1\n",
    "  # with open(dir_name + 'all_py_files.txt') as dir_f:\n",
    "  #   for py_file in dir_f.readlines():\n",
    "  for py_file in all_files:\n",
    "    file_index = file_index + 1\n",
    "    with tokenize.open(py_file.strip()) as f:\n",
    "      try:\n",
    "        tokens = [tok.string for tok in tokenize.generate_tokens(f.readline)]\n",
    "        tokenizerD.fit_on_texts(tokens)\n",
    "      except:\n",
    "        pass \n",
    "\n",
    "      if file_index % 2000 == 0:\n",
    "        print(str(file_index) + \" python files tokenized with #tokens: \" + str(len(tokenizerD.word_index)))\n",
    "        tokenizer_json = tokenizerD.to_json()\n",
    "        with open(dir_name + 'tokenizer.json', 'w') as f:\n",
    "          json.dump(tokenizer_json, f)\n",
    "\n",
    "  # Save final version\n",
    "  tokenizer_json = tokenizerD.to_json()\n",
    "  with open(dir_name + 'tokenizer.json', 'w') as f:\n",
    "    json.dump(tokenizer_json, f)\n",
    "\n",
    "  print('Total tokenizer vocabulary {}'.format(len(tokenizerD.word_index)))\n",
    "\n",
    "\n",
    "tokenize_and_save('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4144d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tf.keras.models.load_model(\"LSTM_model_mini_18Apr2022.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8630bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.load_model(\"RNN_model_mini_18Apr2022.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7b539",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
